{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg; haskey(Pkg.installed(),\"Knet\") || Pkg.add(\"Knet\")\n",
    "using Statistics: mean\n",
    "using Base.Iterators: cycle\n",
    "using Knet: Knet, AutoGrad, Data, param, param0, mat, RNN, dropout, value, nll, adam, minibatch, progress!, converge, Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set display width, load packages, import symbols\n",
    "ENV[\"COLUMNS\"]=72\n",
    "using Pkg; for p in (\"Knet\",\"Plots\"); haskey(Pkg.installed(),p) || Pkg.add(p); end\n",
    "using Knet: Knet, dir, zeroone, progress, sgd, load, save, gc, Param, KnetArray, gpu, Data, nll, relu, training, dropout # param, param0, xavier\n",
    "using Statistics: mean\n",
    "using Base.Iterators: flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import LinearAlgebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atype = Knet.gpu()>=0 ? Knet.KnetArray : Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "load_data (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function load_data(path)\n",
    "    xtrain, ytrain = open(path) do f\n",
    "    xtrain = []\n",
    "    ytrain = []\n",
    "    sentence = []\n",
    "    arcs = []\n",
    "    count = 1\n",
    "    for i in enumerate(eachline(f))  \n",
    "      if i[2] == \"\"\n",
    "        push!(xtrain, sentence)\n",
    "        labels = zeros(count, count)\n",
    "        for j = 1:count\n",
    "            if arcs[j] != 0\n",
    "                labels[j, arcs[j]] = 1\n",
    "            end\n",
    "        end\n",
    "        push!(ytrain, labels)\n",
    "      elseif i[2][1] != '#'\n",
    "        temp = split(i[2])\n",
    "        if temp[1] == \"1\"\n",
    "            sentence = []\n",
    "            arcs = []\n",
    "            push!(sentence, temp[2])\n",
    "            push!(arcs, parse(Int64, temp[7]))\n",
    "            count = 1\n",
    "        else\n",
    "            push!(sentence, temp[2]) \n",
    "            if isnumeric(temp[7][1])\n",
    "                push!(arcs, parse(Int64, temp[7]))\n",
    "            else\n",
    "                push!(arcs, 0)\n",
    "            end\n",
    "            count += 1\n",
    "        end\n",
    "      end\n",
    "    end\n",
    "    xtrain, ytrain\n",
    "    end\n",
    "    xtrain, ytrain\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "load_data2 (generic function with 1 method)"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function load_data2(path)\n",
    "    xtrain, ytrain = open(path) do f\n",
    "    xtrain = []\n",
    "    ytrain = []\n",
    "    sentence = []\n",
    "    arcs = []\n",
    "    count = 1\n",
    "    for i in enumerate(eachline(f))  \n",
    "      if i[2] == \"\"\n",
    "        push!(xtrain, sentence)\n",
    "        labels = zeros(count, count)\n",
    "        push!(ytrain, arcs)\n",
    "      elseif i[2][1] != '#'\n",
    "        temp = split(i[2])\n",
    "        if temp[1] == \"1\"\n",
    "            sentence = []\n",
    "            arcs = []\n",
    "            push!(sentence, temp[2])\n",
    "            push!(arcs, parse(Int64, temp[7]))\n",
    "            count = 1\n",
    "        else\n",
    "            push!(sentence, temp[2]) \n",
    "            if isnumeric(temp[7][1])\n",
    "                push!(arcs, parse(Int64, temp[7]))\n",
    "            else\n",
    "                push!(arcs, 0)\n",
    "            end\n",
    "            count += 1\n",
    "        end\n",
    "      end\n",
    "    end\n",
    "    xtrain, ytrain\n",
    "    end\n",
    "    xtrain, ytrain\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "load_embed (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function load_embed(path)\n",
    "    wembed, wembedind = open(path) do f\n",
    "        wembed = Dict()\n",
    "        wembedind = []\n",
    "        for i in enumerate(eachline(f))\n",
    "            line = i[2]\n",
    "            tokens = split(line)\n",
    "            key = tokens[1]\n",
    "            temp = Array{Float32, 1}()\n",
    "            for token in tokens[2:end]\n",
    "                tmp = tryparse(Float32, token)\n",
    "                append!(temp, tmp)\n",
    "            end\n",
    "            wembed[key] = i[1]\n",
    "            push!(wembedind,temp)\n",
    "        end\n",
    "        wembed, wembedind\n",
    "    end\n",
    "    wembed, wembedind\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Any[Any[\"Al\", \"-\", \"Zaman\", \":\", \"American\", \"forces\", \"killed\", \"Shaikh\", \"Abdullah\", \"al\"  …  \"the\", \"town\", \"of\", \"Qaim\", \",\", \"near\", \"the\", \"Syrian\", \"border\", \".\"], Any[\"[\", \"This\", \"killing\", \"of\", \"a\", \"respected\", \"cleric\", \"will\", \"be\", \"causing\", \"us\", \"trouble\", \"for\", \"years\", \"to\", \"come\", \".\", \"]\"], Any[\"DPA\", \":\", \"Iraqi\", \"authorities\", \"announced\", \"that\", \"they\", \"had\", \"busted\", \"up\", \"3\", \"terrorist\", \"cells\", \"operating\", \"in\", \"Baghdad\", \".\"], Any[\"Two\", \"of\", \"them\", \"were\", \"being\", \"run\", \"by\", \"2\", \"officials\", \"of\", \"the\", \"Ministry\", \"of\", \"the\", \"Interior\", \"!\"], Any[\"The\", \"MoI\", \"in\", \"Iraq\", \"is\", \"equivalent\", \"to\", \"the\", \"US\", \"FBI\"  …  \"members\", \"of\", \"the\", \"Weathermen\", \"bombers\", \"back\", \"in\", \"the\", \"1960s\", \".\"], Any[\"The\", \"third\", \"was\", \"being\", \"run\", \"by\", \"the\", \"head\", \"of\", \"an\", \"investment\", \"firm\", \".\"], Any[\"You\", \"wonder\", \"if\", \"he\", \"was\", \"manipulating\", \"the\", \"market\", \"with\", \"his\", \"bombing\", \"targets\", \".\"], Any[\"The\", \"cells\", \"were\", \"operating\", \"in\", \"the\", \"Ghazaliyah\", \"and\", \"al\", \"-\", \"Jihad\", \"districts\", \"of\", \"the\", \"capital\", \".\"], Any[\"Although\", \"the\", \"announcement\", \"was\", \"probably\", \"made\", \"to\", \"show\", \"progress\", \"in\"  …  \"Baathists\", \"continue\", \"to\", \"penetrate\", \"the\", \"Iraqi\", \"government\", \"very\", \"hopeful\", \".\"], Any[\"It\", \"reminds\", \"me\", \"too\", \"much\", \"of\", \"the\", \"ARVN\", \"officers\", \"who\", \"were\", \"secretly\", \"working\", \"for\", \"the\", \"other\", \"side\", \"in\", \"Vietnam\", \".\"]  …  Any[\"Over\", \"two\", \"hours\", \"later\", \"(\", \"and\", \"ten\", \"minutes\", \"before\", \"they\", \"closed\", \")\", \"my\", \"car\", \"was\", \"finally\", \"finished\", \".\"], Any[\"A\", \"few\", \"minutes\", \"after\", \"I\", \"left\", \",\", \"I\", \"was\", \"called\"  …  \"which\", \"they\", \"should\", \"have\", \"left\", \"in\", \"the\", \"car\", \")\", \".\"], Any[\"Of\", \"course\", \",\", \"they\", \"would\", \"be\", \"closing\", \"in\", \"5\", \"minutes\"  …  \"to\", \"hurry\", \"up\", \"or\", \"get\", \"it\", \"the\", \"next\", \"day\", \".\"], Any[\"Of\", \"course\", \"I\", \"could\", \"n't\", \"make\", \"it\", \"back\", \"in\", \"time\"  …  \"stay\", \"5\", \"extra\", \"minutes\", \"to\", \"wait\", \"for\", \"me\", \")\", \".\"], Any[\"The\", \"next\", \"day\", \",\", \"no\", \"one\", \"could\", \"find\", \"my\", \"wheel\", \"lock\", \"and\", \"that\", \"particular\", \"technician\", \"was\", \"not\", \"in\", \".\"], Any[\"Of\", \"course\", \",\", \"they\", \"could\", \"n't\", \"call\", \"him\", \"either\", \"to\"  …  \"no\", \"wheel\", \"lock\", \"should\", \"I\", \"get\", \"a\", \"flat\", \")\", \".\"], Any[\"On\", \"Monday\", \"I\", \"called\", \"and\", \"again\", \"it\", \"was\", \"a\", \"big\"  …  \"do\", \"to\", \"find\", \"anyone\", \"who\", \"knew\", \"anything\", \"about\", \"it\", \".\"], Any[\"Supposedly\", \"they\", \"will\", \"be\", \"holding\", \"it\", \"for\", \"me\", \"this\", \"evening\"  …  \"'m\", \"sure\", \"that\", \"will\", \"also\", \"be\", \"a\", \"huge\", \"ordeal\", \".\"], Any[\"The\", \"employees\", \"at\", \"this\", \"Sear's\", \"are\", \"completely\", \"apathetic\", \"and\", \"there\"  …  \"be\", \"any\", \"sort\", \"of\", \"management\", \"that\", \"I\", \"could\", \"see\", \".\"], Any[\"I\", \"will\", \"never\", \"return\", \"there\", \"again\", \"(\", \"and\", \"now\", \"have\"  …  \"of\", \"work\", \"they\", \"actually\", \"performed\", \"on\", \"my\", \"car\", \")\", \".\"]], Any[Any[0, 1, 1, 1, 6, 7, 1, 7, 8, 8  …  21, 18, 23, 21, 21, 28, 28, 28, 21, 1], Any[10, 3, 10, 7, 7, 7, 3, 10, 10, 0, 10, 10, 14, 10, 16, 14, 10, 10], Any[0, 1, 4, 5, 1, 9, 9, 9, 5, 9, 13, 13, 9, 13, 16, 14, 1], Any[6, 3, 1, 6, 6, 0, 9, 9, 6, 12, 12, 9, 15, 15, 12, 6], Any[2, 6, 4, 2, 6, 0, 10, 10, 10, 6  …  22, 31, 31, 31, 27, 35, 35, 35, 22, 6], Any[2, 5, 5, 5, 0, 8, 8, 5, 12, 12, 12, 8, 5], Any[2, 0, 6, 6, 6, 2, 8, 6, 12, 12, 12, 6, 2], Any[2, 4, 4, 0, 12, 12, 12, 11, 11, 11, 7, 4, 15, 15, 12, 4], Any[6, 3, 6, 6, 6, 21, 8, 6, 8, 11  …  27, 23, 29, 27, 32, 32, 29, 34, 21, 21], Any[2, 0, 2, 5, 2, 9, 9, 9, 2, 13, 13, 13, 9, 17, 17, 17, 13, 19, 13, 2]  …  Any[2, 3, 4, 17, 4, 11, 8, 9, 11, 11, 4, 4, 14, 17, 17, 17, 0, 17], Any[3, 3, 6, 6, 6, 10, 10, 10, 10, 0  …  26, 26, 26, 26, 20, 29, 29, 26, 26, 10], Any[7, 1, 7, 7, 7, 7, 0, 10, 10, 7  …  17, 15, 17, 20, 17, 20, 24, 24, 20, 7], Any[6, 1, 6, 6, 6, 0, 6, 6, 10, 6  …  6, 20, 20, 17, 22, 17, 24, 22, 6, 6], Any[3, 3, 8, 8, 6, 8, 8, 0, 11, 11, 8, 18, 15, 15, 18, 18, 18, 8, 8], Any[7, 1, 7, 7, 7, 7, 0, 7, 7, 11  …  46, 46, 38, 49, 49, 38, 51, 49, 38, 7], Any[2, 4, 4, 0, 13, 13, 13, 13, 13, 13  …  4, 15, 13, 15, 18, 16, 18, 21, 19, 4], Any[5, 5, 5, 5, 0, 5, 8, 5, 10, 5  …  15, 5, 22, 22, 22, 22, 22, 22, 15, 5], Any[2, 8, 5, 5, 2, 8, 8, 0, 13, 13  …  13, 17, 15, 19, 17, 23, 23, 23, 19, 8], Any[4, 4, 4, 0, 4, 4, 4, 10, 10, 4  …  18, 16, 21, 21, 18, 24, 24, 21, 4, 4]])"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2 = load_data2(\"en-ud-train.conllu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13-element Array{Any,1}:\n",
       "  2\n",
       "  5\n",
       "  5\n",
       "  5\n",
       "  0\n",
       "  8\n",
       "  8\n",
       "  5\n",
       " 12\n",
       " 12\n",
       " 12\n",
       "  8\n",
       "  5"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2[2][6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50-element Array{Float32,1}:\n",
       "  0.013441\n",
       "  0.23682 \n",
       " -0.16899 \n",
       "  0.40951 \n",
       "  0.63812 \n",
       "  0.47709 \n",
       " -0.42852 \n",
       " -0.55641 \n",
       " -0.364   \n",
       " -0.23938 \n",
       "  0.13001 \n",
       " -0.063734\n",
       " -0.39575 \n",
       "  ⋮       \n",
       "  0.70358 \n",
       "  0.44858 \n",
       " -0.080262\n",
       "  0.63003 \n",
       "  0.32111 \n",
       " -0.46765 \n",
       "  0.22786 \n",
       "  0.36034 \n",
       " -0.37818 \n",
       " -0.56657 \n",
       "  0.044691\n",
       "  0.30392 "
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wembedind[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9-element Array{Tuple{Int64,Int64},1}:\n",
       " (1, 1)\n",
       " (1, 2)\n",
       " (1, 3)\n",
       " (2, 1)\n",
       " (2, 2)\n",
       " (2, 3)\n",
       " (3, 1)\n",
       " (3, 2)\n",
       " (3, 3)"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(x,y) for x in 1:3 for y in 1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Base.Iterators.Zip{Tuple{Base.Generator{Array{Any,1},getfield(Main, Symbol(\"##128#129\"))},Array{Any,1}}}((Base.Generator{Array{Any,1},getfield(Main, Symbol(\"##128#129\"))}(getfield(Main, Symbol(\"##128#129\"))(), Any[Any[\"Al\", \"-\", \"Zaman\", \":\", \"American\", \"forces\", \"killed\", \"Shaikh\", \"Abdullah\", \"al\"  …  \"the\", \"town\", \"of\", \"Qaim\", \",\", \"near\", \"the\", \"Syrian\", \"border\", \".\"], Any[\"[\", \"This\", \"killing\", \"of\", \"a\", \"respected\", \"cleric\", \"will\", \"be\", \"causing\", \"us\", \"trouble\", \"for\", \"years\", \"to\", \"come\", \".\", \"]\"], Any[\"DPA\", \":\", \"Iraqi\", \"authorities\", \"announced\", \"that\", \"they\", \"had\", \"busted\", \"up\", \"3\", \"terrorist\", \"cells\", \"operating\", \"in\", \"Baghdad\", \".\"], Any[\"Two\", \"of\", \"them\", \"were\", \"being\", \"run\", \"by\", \"2\", \"officials\", \"of\", \"the\", \"Ministry\", \"of\", \"the\", \"Interior\", \"!\"], Any[\"The\", \"MoI\", \"in\", \"Iraq\", \"is\", \"equivalent\", \"to\", \"the\", \"US\", \"FBI\"  …  \"members\", \"of\", \"the\", \"Weathermen\", \"bombers\", \"back\", \"in\", \"the\", \"1960s\", \".\"], Any[\"The\", \"third\", \"was\", \"being\", \"run\", \"by\", \"the\", \"head\", \"of\", \"an\", \"investment\", \"firm\", \".\"], Any[\"You\", \"wonder\", \"if\", \"he\", \"was\", \"manipulating\", \"the\", \"market\", \"with\", \"his\", \"bombing\", \"targets\", \".\"], Any[\"The\", \"cells\", \"were\", \"operating\", \"in\", \"the\", \"Ghazaliyah\", \"and\", \"al\", \"-\", \"Jihad\", \"districts\", \"of\", \"the\", \"capital\", \".\"], Any[\"Although\", \"the\", \"announcement\", \"was\", \"probably\", \"made\", \"to\", \"show\", \"progress\", \"in\"  …  \"Baathists\", \"continue\", \"to\", \"penetrate\", \"the\", \"Iraqi\", \"government\", \"very\", \"hopeful\", \".\"], Any[\"It\", \"reminds\", \"me\", \"too\", \"much\", \"of\", \"the\", \"ARVN\", \"officers\", \"who\", \"were\", \"secretly\", \"working\", \"for\", \"the\", \"other\", \"side\", \"in\", \"Vietnam\", \".\"]  …  Any[\"Over\", \"two\", \"hours\", \"later\", \"(\", \"and\", \"ten\", \"minutes\", \"before\", \"they\", \"closed\", \")\", \"my\", \"car\", \"was\", \"finally\", \"finished\", \".\"], Any[\"A\", \"few\", \"minutes\", \"after\", \"I\", \"left\", \",\", \"I\", \"was\", \"called\"  …  \"which\", \"they\", \"should\", \"have\", \"left\", \"in\", \"the\", \"car\", \")\", \".\"], Any[\"Of\", \"course\", \",\", \"they\", \"would\", \"be\", \"closing\", \"in\", \"5\", \"minutes\"  …  \"to\", \"hurry\", \"up\", \"or\", \"get\", \"it\", \"the\", \"next\", \"day\", \".\"], Any[\"Of\", \"course\", \"I\", \"could\", \"n't\", \"make\", \"it\", \"back\", \"in\", \"time\"  …  \"stay\", \"5\", \"extra\", \"minutes\", \"to\", \"wait\", \"for\", \"me\", \")\", \".\"], Any[\"The\", \"next\", \"day\", \",\", \"no\", \"one\", \"could\", \"find\", \"my\", \"wheel\", \"lock\", \"and\", \"that\", \"particular\", \"technician\", \"was\", \"not\", \"in\", \".\"], Any[\"Of\", \"course\", \",\", \"they\", \"could\", \"n't\", \"call\", \"him\", \"either\", \"to\"  …  \"no\", \"wheel\", \"lock\", \"should\", \"I\", \"get\", \"a\", \"flat\", \")\", \".\"], Any[\"On\", \"Monday\", \"I\", \"called\", \"and\", \"again\", \"it\", \"was\", \"a\", \"big\"  …  \"do\", \"to\", \"find\", \"anyone\", \"who\", \"knew\", \"anything\", \"about\", \"it\", \".\"], Any[\"Supposedly\", \"they\", \"will\", \"be\", \"holding\", \"it\", \"for\", \"me\", \"this\", \"evening\"  …  \"'m\", \"sure\", \"that\", \"will\", \"also\", \"be\", \"a\", \"huge\", \"ordeal\", \".\"], Any[\"The\", \"employees\", \"at\", \"this\", \"Sear's\", \"are\", \"completely\", \"apathetic\", \"and\", \"there\"  …  \"be\", \"any\", \"sort\", \"of\", \"management\", \"that\", \"I\", \"could\", \"see\", \".\"], Any[\"I\", \"will\", \"never\", \"return\", \"there\", \"again\", \"(\", \"and\", \"now\", \"have\"  …  \"of\", \"work\", \"they\", \"actually\", \"performed\", \"on\", \"my\", \"car\", \")\", \".\"]]), Any[Any[0, 1, 1, 1, 6, 7, 1, 7, 8, 8  …  21, 18, 23, 21, 21, 28, 28, 28, 21, 1], Any[10, 3, 10, 7, 7, 7, 3, 10, 10, 0, 10, 10, 14, 10, 16, 14, 10, 10], Any[0, 1, 4, 5, 1, 9, 9, 9, 5, 9, 13, 13, 9, 13, 16, 14, 1], Any[6, 3, 1, 6, 6, 0, 9, 9, 6, 12, 12, 9, 15, 15, 12, 6], Any[2, 6, 4, 2, 6, 0, 10, 10, 10, 6  …  22, 31, 31, 31, 27, 35, 35, 35, 22, 6], Any[2, 5, 5, 5, 0, 8, 8, 5, 12, 12, 12, 8, 5], Any[2, 0, 6, 6, 6, 2, 8, 6, 12, 12, 12, 6, 2], Any[2, 4, 4, 0, 12, 12, 12, 11, 11, 11, 7, 4, 15, 15, 12, 4], Any[6, 3, 6, 6, 6, 21, 8, 6, 8, 11  …  27, 23, 29, 27, 32, 32, 29, 34, 21, 21], Any[2, 0, 2, 5, 2, 9, 9, 9, 2, 13, 13, 13, 9, 17, 17, 17, 13, 19, 13, 2]  …  Any[2, 3, 4, 17, 4, 11, 8, 9, 11, 11, 4, 4, 14, 17, 17, 17, 0, 17], Any[3, 3, 6, 6, 6, 10, 10, 10, 10, 0  …  26, 26, 26, 26, 20, 29, 29, 26, 26, 10], Any[7, 1, 7, 7, 7, 7, 0, 10, 10, 7  …  17, 15, 17, 20, 17, 20, 24, 24, 20, 7], Any[6, 1, 6, 6, 6, 0, 6, 6, 10, 6  …  6, 20, 20, 17, 22, 17, 24, 22, 6, 6], Any[3, 3, 8, 8, 6, 8, 8, 0, 11, 11, 8, 18, 15, 15, 18, 18, 18, 8, 8], Any[7, 1, 7, 7, 7, 7, 0, 7, 7, 11  …  46, 46, 38, 49, 49, 38, 51, 49, 38, 7], Any[2, 4, 4, 0, 13, 13, 13, 13, 13, 13  …  4, 15, 13, 15, 18, 16, 18, 21, 19, 4], Any[5, 5, 5, 5, 0, 5, 8, 5, 10, 5  …  15, 5, 22, 22, 22, 22, 22, 22, 15, 5], Any[2, 8, 5, 5, 2, 8, 8, 0, 13, 13  …  13, 17, 15, 19, 17, 23, 23, 23, 19, 8], Any[4, 4, 4, 0, 4, 4, 4, 10, 10, 4  …  18, 16, 21, 21, 18, 24, 24, 21, 4, 4]]))"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data22 = zip((reshape(x,1,1,length(x)) for x in data2[1]),data2[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×1×29 Array{Any,3}:\n",
       "[:, :, 1] =\n",
       " \"Al\"\n",
       "\n",
       "[:, :, 2] =\n",
       " \"-\"\n",
       "\n",
       "[:, :, 3] =\n",
       " \"Zaman\"\n",
       "\n",
       "...\n",
       "\n",
       "[:, :, 27] =\n",
       " \"Syrian\"\n",
       "\n",
       "[:, :, 28] =\n",
       " \"border\"\n",
       "\n",
       "[:, :, 29] =\n",
       " \".\""
      ]
     },
     "execution_count": 449,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterate(data22)[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Base.Iterators.Flatten{Base.Generator{Array{Any,1},getfield(Main, Symbol(\"##103#104\"))}}(Base.Generator{Array{Any,1},getfield(Main, Symbol(\"##103#104\"))}(getfield(Main, Symbol(\"##103#104\"))(), Any[Any[\"Al\", \"-\", \"Zaman\", \":\", \"American\", \"forces\", \"killed\", \"Shaikh\", \"Abdullah\", \"al\"  …  \"the\", \"town\", \"of\", \"Qaim\", \",\", \"near\", \"the\", \"Syrian\", \"border\", \".\"], Any[\"[\", \"This\", \"killing\", \"of\", \"a\", \"respected\", \"cleric\", \"will\", \"be\", \"causing\", \"us\", \"trouble\", \"for\", \"years\", \"to\", \"come\", \".\", \"]\"], Any[\"DPA\", \":\", \"Iraqi\", \"authorities\", \"announced\", \"that\", \"they\", \"had\", \"busted\", \"up\", \"3\", \"terrorist\", \"cells\", \"operating\", \"in\", \"Baghdad\", \".\"], Any[\"Two\", \"of\", \"them\", \"were\", \"being\", \"run\", \"by\", \"2\", \"officials\", \"of\", \"the\", \"Ministry\", \"of\", \"the\", \"Interior\", \"!\"], Any[\"The\", \"MoI\", \"in\", \"Iraq\", \"is\", \"equivalent\", \"to\", \"the\", \"US\", \"FBI\"  …  \"members\", \"of\", \"the\", \"Weathermen\", \"bombers\", \"back\", \"in\", \"the\", \"1960s\", \".\"], Any[\"The\", \"third\", \"was\", \"being\", \"run\", \"by\", \"the\", \"head\", \"of\", \"an\", \"investment\", \"firm\", \".\"], Any[\"You\", \"wonder\", \"if\", \"he\", \"was\", \"manipulating\", \"the\", \"market\", \"with\", \"his\", \"bombing\", \"targets\", \".\"], Any[\"The\", \"cells\", \"were\", \"operating\", \"in\", \"the\", \"Ghazaliyah\", \"and\", \"al\", \"-\", \"Jihad\", \"districts\", \"of\", \"the\", \"capital\", \".\"], Any[\"Although\", \"the\", \"announcement\", \"was\", \"probably\", \"made\", \"to\", \"show\", \"progress\", \"in\"  …  \"Baathists\", \"continue\", \"to\", \"penetrate\", \"the\", \"Iraqi\", \"government\", \"very\", \"hopeful\", \".\"], Any[\"It\", \"reminds\", \"me\", \"too\", \"much\", \"of\", \"the\", \"ARVN\", \"officers\", \"who\", \"were\", \"secretly\", \"working\", \"for\", \"the\", \"other\", \"side\", \"in\", \"Vietnam\", \".\"]  …  Any[\"Over\", \"two\", \"hours\", \"later\", \"(\", \"and\", \"ten\", \"minutes\", \"before\", \"they\", \"closed\", \")\", \"my\", \"car\", \"was\", \"finally\", \"finished\", \".\"], Any[\"A\", \"few\", \"minutes\", \"after\", \"I\", \"left\", \",\", \"I\", \"was\", \"called\"  …  \"which\", \"they\", \"should\", \"have\", \"left\", \"in\", \"the\", \"car\", \")\", \".\"], Any[\"Of\", \"course\", \",\", \"they\", \"would\", \"be\", \"closing\", \"in\", \"5\", \"minutes\"  …  \"to\", \"hurry\", \"up\", \"or\", \"get\", \"it\", \"the\", \"next\", \"day\", \".\"], Any[\"Of\", \"course\", \"I\", \"could\", \"n't\", \"make\", \"it\", \"back\", \"in\", \"time\"  …  \"stay\", \"5\", \"extra\", \"minutes\", \"to\", \"wait\", \"for\", \"me\", \")\", \".\"], Any[\"The\", \"next\", \"day\", \",\", \"no\", \"one\", \"could\", \"find\", \"my\", \"wheel\", \"lock\", \"and\", \"that\", \"particular\", \"technician\", \"was\", \"not\", \"in\", \".\"], Any[\"Of\", \"course\", \",\", \"they\", \"could\", \"n't\", \"call\", \"him\", \"either\", \"to\"  …  \"no\", \"wheel\", \"lock\", \"should\", \"I\", \"get\", \"a\", \"flat\", \")\", \".\"], Any[\"On\", \"Monday\", \"I\", \"called\", \"and\", \"again\", \"it\", \"was\", \"a\", \"big\"  …  \"do\", \"to\", \"find\", \"anyone\", \"who\", \"knew\", \"anything\", \"about\", \"it\", \".\"], Any[\"Supposedly\", \"they\", \"will\", \"be\", \"holding\", \"it\", \"for\", \"me\", \"this\", \"evening\"  …  \"'m\", \"sure\", \"that\", \"will\", \"also\", \"be\", \"a\", \"huge\", \"ordeal\", \".\"], Any[\"The\", \"employees\", \"at\", \"this\", \"Sear's\", \"are\", \"completely\", \"apathetic\", \"and\", \"there\"  …  \"be\", \"any\", \"sort\", \"of\", \"management\", \"that\", \"I\", \"could\", \"see\", \".\"], Any[\"I\", \"will\", \"never\", \"return\", \"there\", \"again\", \"(\", \"and\", \"now\", \"have\"  …  \"of\", \"work\", \"they\", \"actually\", \"performed\", \"on\", \"my\", \"car\", \")\", \".\"]]))"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data3 = ((reshape(x,1,1,length(x)) ,y) for x in data2[1] for y in data2[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Base.Generator{Base.Iterators.Zip{Tuple{Base.Generator{Array{Any,1},getfield(Main, Symbol(\"##128#129\"))},Array{Any,1}}},getfield(Main, Symbol(\"##182#183\"))}(getfield(Main, Symbol(\"##182#183\"))(), Base.Iterators.Zip{Tuple{Base.Generator{Array{Any,1},getfield(Main, Symbol(\"##128#129\"))},Array{Any,1}}}((Base.Generator{Array{Any,1},getfield(Main, Symbol(\"##128#129\"))}(getfield(Main, Symbol(\"##128#129\"))(), Any[Any[\"Al\", \"-\", \"Zaman\", \":\", \"American\", \"forces\", \"killed\", \"Shaikh\", \"Abdullah\", \"al\"  …  \"the\", \"town\", \"of\", \"Qaim\", \",\", \"near\", \"the\", \"Syrian\", \"border\", \".\"], Any[\"[\", \"This\", \"killing\", \"of\", \"a\", \"respected\", \"cleric\", \"will\", \"be\", \"causing\", \"us\", \"trouble\", \"for\", \"years\", \"to\", \"come\", \".\", \"]\"], Any[\"DPA\", \":\", \"Iraqi\", \"authorities\", \"announced\", \"that\", \"they\", \"had\", \"busted\", \"up\", \"3\", \"terrorist\", \"cells\", \"operating\", \"in\", \"Baghdad\", \".\"], Any[\"Two\", \"of\", \"them\", \"were\", \"being\", \"run\", \"by\", \"2\", \"officials\", \"of\", \"the\", \"Ministry\", \"of\", \"the\", \"Interior\", \"!\"], Any[\"The\", \"MoI\", \"in\", \"Iraq\", \"is\", \"equivalent\", \"to\", \"the\", \"US\", \"FBI\"  …  \"members\", \"of\", \"the\", \"Weathermen\", \"bombers\", \"back\", \"in\", \"the\", \"1960s\", \".\"], Any[\"The\", \"third\", \"was\", \"being\", \"run\", \"by\", \"the\", \"head\", \"of\", \"an\", \"investment\", \"firm\", \".\"], Any[\"You\", \"wonder\", \"if\", \"he\", \"was\", \"manipulating\", \"the\", \"market\", \"with\", \"his\", \"bombing\", \"targets\", \".\"], Any[\"The\", \"cells\", \"were\", \"operating\", \"in\", \"the\", \"Ghazaliyah\", \"and\", \"al\", \"-\", \"Jihad\", \"districts\", \"of\", \"the\", \"capital\", \".\"], Any[\"Although\", \"the\", \"announcement\", \"was\", \"probably\", \"made\", \"to\", \"show\", \"progress\", \"in\"  …  \"Baathists\", \"continue\", \"to\", \"penetrate\", \"the\", \"Iraqi\", \"government\", \"very\", \"hopeful\", \".\"], Any[\"It\", \"reminds\", \"me\", \"too\", \"much\", \"of\", \"the\", \"ARVN\", \"officers\", \"who\", \"were\", \"secretly\", \"working\", \"for\", \"the\", \"other\", \"side\", \"in\", \"Vietnam\", \".\"]  …  Any[\"Over\", \"two\", \"hours\", \"later\", \"(\", \"and\", \"ten\", \"minutes\", \"before\", \"they\", \"closed\", \")\", \"my\", \"car\", \"was\", \"finally\", \"finished\", \".\"], Any[\"A\", \"few\", \"minutes\", \"after\", \"I\", \"left\", \",\", \"I\", \"was\", \"called\"  …  \"which\", \"they\", \"should\", \"have\", \"left\", \"in\", \"the\", \"car\", \")\", \".\"], Any[\"Of\", \"course\", \",\", \"they\", \"would\", \"be\", \"closing\", \"in\", \"5\", \"minutes\"  …  \"to\", \"hurry\", \"up\", \"or\", \"get\", \"it\", \"the\", \"next\", \"day\", \".\"], Any[\"Of\", \"course\", \"I\", \"could\", \"n't\", \"make\", \"it\", \"back\", \"in\", \"time\"  …  \"stay\", \"5\", \"extra\", \"minutes\", \"to\", \"wait\", \"for\", \"me\", \")\", \".\"], Any[\"The\", \"next\", \"day\", \",\", \"no\", \"one\", \"could\", \"find\", \"my\", \"wheel\", \"lock\", \"and\", \"that\", \"particular\", \"technician\", \"was\", \"not\", \"in\", \".\"], Any[\"Of\", \"course\", \",\", \"they\", \"could\", \"n't\", \"call\", \"him\", \"either\", \"to\"  …  \"no\", \"wheel\", \"lock\", \"should\", \"I\", \"get\", \"a\", \"flat\", \")\", \".\"], Any[\"On\", \"Monday\", \"I\", \"called\", \"and\", \"again\", \"it\", \"was\", \"a\", \"big\"  …  \"do\", \"to\", \"find\", \"anyone\", \"who\", \"knew\", \"anything\", \"about\", \"it\", \".\"], Any[\"Supposedly\", \"they\", \"will\", \"be\", \"holding\", \"it\", \"for\", \"me\", \"this\", \"evening\"  …  \"'m\", \"sure\", \"that\", \"will\", \"also\", \"be\", \"a\", \"huge\", \"ordeal\", \".\"], Any[\"The\", \"employees\", \"at\", \"this\", \"Sear's\", \"are\", \"completely\", \"apathetic\", \"and\", \"there\"  …  \"be\", \"any\", \"sort\", \"of\", \"management\", \"that\", \"I\", \"could\", \"see\", \".\"], Any[\"I\", \"will\", \"never\", \"return\", \"there\", \"again\", \"(\", \"and\", \"now\", \"have\"  …  \"of\", \"work\", \"they\", \"actually\", \"performed\", \"on\", \"my\", \"car\", \")\", \".\"]]), Any[Any[0, 1, 1, 1, 6, 7, 1, 7, 8, 8  …  21, 18, 23, 21, 21, 28, 28, 28, 21, 1], Any[10, 3, 10, 7, 7, 7, 3, 10, 10, 0, 10, 10, 14, 10, 16, 14, 10, 10], Any[0, 1, 4, 5, 1, 9, 9, 9, 5, 9, 13, 13, 9, 13, 16, 14, 1], Any[6, 3, 1, 6, 6, 0, 9, 9, 6, 12, 12, 9, 15, 15, 12, 6], Any[2, 6, 4, 2, 6, 0, 10, 10, 10, 6  …  22, 31, 31, 31, 27, 35, 35, 35, 22, 6], Any[2, 5, 5, 5, 0, 8, 8, 5, 12, 12, 12, 8, 5], Any[2, 0, 6, 6, 6, 2, 8, 6, 12, 12, 12, 6, 2], Any[2, 4, 4, 0, 12, 12, 12, 11, 11, 11, 7, 4, 15, 15, 12, 4], Any[6, 3, 6, 6, 6, 21, 8, 6, 8, 11  …  27, 23, 29, 27, 32, 32, 29, 34, 21, 21], Any[2, 0, 2, 5, 2, 9, 9, 9, 2, 13, 13, 13, 9, 17, 17, 17, 13, 19, 13, 2]  …  Any[2, 3, 4, 17, 4, 11, 8, 9, 11, 11, 4, 4, 14, 17, 17, 17, 0, 17], Any[3, 3, 6, 6, 6, 10, 10, 10, 10, 0  …  26, 26, 26, 26, 20, 29, 29, 26, 26, 10], Any[7, 1, 7, 7, 7, 7, 0, 10, 10, 7  …  17, 15, 17, 20, 17, 20, 24, 24, 20, 7], Any[6, 1, 6, 6, 6, 0, 6, 6, 10, 6  …  6, 20, 20, 17, 22, 17, 24, 22, 6, 6], Any[3, 3, 8, 8, 6, 8, 8, 0, 11, 11, 8, 18, 15, 15, 18, 18, 18, 8, 8], Any[7, 1, 7, 7, 7, 7, 0, 7, 7, 11  …  46, 46, 38, 49, 49, 38, 51, 49, 38, 7], Any[2, 4, 4, 0, 13, 13, 13, 13, 13, 13  …  4, 15, 13, 15, 18, 16, 18, 21, 19, 4], Any[5, 5, 5, 5, 0, 5, 8, 5, 10, 5  …  15, 5, 22, 22, 22, 22, 22, 22, 15, 5], Any[2, 8, 5, 5, 2, 8, 8, 0, 13, 13  …  13, 17, 15, 19, 17, 23, 23, 23, 19, 8], Any[4, 4, 4, 0, 4, 4, 4, 10, 10, 4  …  18, 16, 21, 21, 18, 24, 24, 21, 4, 4]])))"
      ]
     },
     "execution_count": 543,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data4 = ((reshape(cat(rootind,map(getind,x), dims=3),1,length(x)+1),y) for (x,y) in data22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "399999"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rootind = 399999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×30 Array{Int64,2}:\n",
       " 399999  319  12  34409  46  141  …  47795  2  356  1  3390  719  3"
      ]
     },
     "execution_count": 544,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterate(data4)[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×1×29 Array{Int64,3}:\n",
       "[:, :, 1] =\n",
       " 319\n",
       "\n",
       "[:, :, 2] =\n",
       " 12\n",
       "\n",
       "[:, :, 3] =\n",
       " 34409\n",
       "\n",
       "...\n",
       "\n",
       "[:, :, 27] =\n",
       " 3390\n",
       "\n",
       "[:, :, 28] =\n",
       " 719\n",
       "\n",
       "[:, :, 29] =\n",
       " 3"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map(getind,iterate(data3)[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50×1×30 Array{Float64,3}:\n",
       "[:, :, 1] =\n",
       " -0.7589799761772156  \n",
       " -0.47426000237464905 \n",
       "  0.47369998693466187 \n",
       "  0.7724999785423279  \n",
       " -0.7806400060653687  \n",
       "  0.23232999444007874 \n",
       "  0.0461140014231205  \n",
       "  0.8401399850845337  \n",
       "  0.243709996342659   \n",
       "  0.022978000342845917\n",
       "  0.5396400094032288  \n",
       " -0.36100998520851135 \n",
       "  0.9419800043106079  \n",
       "  ⋮                   \n",
       "  0.035413000732660294\n",
       "  0.5883399844169617  \n",
       "  0.4543899893760681  \n",
       " -0.8425400257110596  \n",
       "  0.10649999976158142 \n",
       " -0.059397000819444656\n",
       "  0.09044899791479111 \n",
       "  0.30581000447273254 \n",
       " -0.6142399907112122  \n",
       "  0.7895399928092957  \n",
       " -0.014116000384092331\n",
       "  0.6448000073432922  \n",
       "\n",
       "[:, :, 2] =\n",
       "  0.542140007019043  \n",
       "  1.0302000045776367 \n",
       "  0.8689600229263306 \n",
       "  0.5001400113105774 \n",
       "  0.9518200159072876 \n",
       " -1.3366999626159668 \n",
       " -0.4010699987411499 \n",
       "  0.3922699987888336 \n",
       "  0.536620020866394  \n",
       "  0.48791998624801636\n",
       " -0.8468700051307678 \n",
       " -0.6293799877166748 \n",
       " -1.3402999639511108 \n",
       "  ⋮                  \n",
       " -0.7340899705886841 \n",
       "  1.3209999799728394 \n",
       "  0.5159100294113159 \n",
       "  0.5730599761009216 \n",
       " -0.8462100028991699 \n",
       " -0.2014700025320053 \n",
       "  1.2488000392913818 \n",
       " -0.7575899958610535 \n",
       " -2.0058999061584473 \n",
       "  0.4759100079536438 \n",
       "  0.18317000567913055\n",
       "  0.43380001187324524\n",
       "\n",
       "[:, :, 3] =\n",
       " -0.16767999529838562\n",
       "  1.2151000499725342 \n",
       "  0.49514999985694885\n",
       "  0.2683599889278412 \n",
       " -0.4584999978542328 \n",
       " -0.2331099957227707 \n",
       " -0.528219997882843  \n",
       " -1.3557000160217285 \n",
       "  0.1609800010919571 \n",
       "  0.376910001039505  \n",
       " -0.9270200133323669 \n",
       " -0.43904000520706177\n",
       " -1.0634000301361084 \n",
       "  ⋮                  \n",
       "  0.6995599865913391 \n",
       "  0.1488499939441681 \n",
       "  0.02945300005376339\n",
       "  1.488800048828125  \n",
       "  0.52360999584198   \n",
       "  0.09935399889945984\n",
       "  1.2515000104904175 \n",
       "  0.09938099980354309\n",
       " -0.07926099747419357\n",
       " -0.30862000584602356\n",
       "  0.30893000960350037\n",
       "  0.11022999882698059\n",
       "\n",
       "...\n",
       "\n",
       "[:, :, 28] =\n",
       "  0.12416999787092209 \n",
       "  1.0658999681472778  \n",
       "  0.33647000789642334 \n",
       " -0.9356300234794617  \n",
       "  0.04676799848675728 \n",
       " -0.3745900094509125  \n",
       " -0.26107001304626465 \n",
       "  0.7559199929237366  \n",
       " -0.8221700191497803  \n",
       " -0.30507999658584595 \n",
       "  0.2481199949979782  \n",
       " -0.9628099799156189  \n",
       " -0.2593500018119812  \n",
       "  ⋮                   \n",
       " -1.1033999919891357  \n",
       "  1.426300048828125   \n",
       "  0.5391299724578857  \n",
       "  0.6975299715995789  \n",
       " -0.023492999374866486\n",
       " -0.8098599910736084  \n",
       "  0.13798999786376953 \n",
       " -0.7403200268745422  \n",
       " -0.9150000214576721  \n",
       "  1.7891000509262085  \n",
       " -0.4135800004005432  \n",
       " -1.4449000358581543  \n",
       "\n",
       "[:, :, 29] =\n",
       "  0.38784000277519226 \n",
       " -0.3619599938392639  \n",
       " -0.04096100106835365 \n",
       "  0.2936899960041046  \n",
       " -0.4400100111961365  \n",
       " -0.426829993724823   \n",
       " -0.32728999853134155 \n",
       "  0.3984200060367584  \n",
       "  0.5217900276184082  \n",
       " -1.8260999917984009  \n",
       " -0.2205599993467331  \n",
       " -1.1518000364303589  \n",
       "  0.047251999378204346\n",
       "  ⋮                   \n",
       " -0.971310019493103   \n",
       "  0.9911999702453613  \n",
       " -0.304639995098114   \n",
       "  0.76569002866745    \n",
       "  1.4150999784469604  \n",
       " -0.8635600209236145  \n",
       " -0.3028700053691864  \n",
       " -0.8349900245666504  \n",
       "  0.39190998673439026 \n",
       "  0.41297000646591187 \n",
       " -0.4170700013637543  \n",
       " -1.1414999961853027  \n",
       "\n",
       "[:, :, 30] =\n",
       "  0.15163999795913696 \n",
       "  0.30177000164985657 \n",
       " -0.16763000190258026 \n",
       "  0.17684000730514526 \n",
       "  0.3171899914741516  \n",
       "  0.33972999453544617 \n",
       " -0.4347800016403198  \n",
       " -0.3108600080013275  \n",
       " -0.44999000430107117 \n",
       " -0.29486000537872314 \n",
       "  0.16607999801635742 \n",
       "  0.11963000148534775 \n",
       " -0.41328001022338867 \n",
       "  ⋮                   \n",
       "  0.41705000400543213 \n",
       "  0.056763000786304474\n",
       " -6.368100002873689e-5\n",
       "  0.06898699700832367 \n",
       "  0.08793900161981583 \n",
       " -0.10284999758005142 \n",
       " -0.13931000232696533 \n",
       "  0.22314000129699707 \n",
       " -0.08080299943685532 \n",
       " -0.35651999711990356 \n",
       "  0.016412999480962753\n",
       "  0.1021599993109703  "
      ]
     },
     "execution_count": 545,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wembedmat[:,iterate(data4)[1][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getind(\":\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 4)"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterate((x^2 for x in 3:12),iterate(x^2 for x in 3:12)[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dict{Any,Any}(\"newdigate\"=>164100,\"daufuskie\"=>254784,\"single-arm\"=>192007,\"titration\"=>117879,\"qajar\"=>66399,\"pinheiro\"=>38763,\"hospitalet\"=>282158,\"kennedale\"=>223560,\"tetracyclic\"=>353804,\"moher\"=>167242…), Any[Float32[0.418, 0.24968, -0.41242, 0.1217, 0.34527, -0.044457, -0.49688, -0.17862, -0.00066023, -0.6566  …  -0.29871, -0.15749, -0.34758, -0.045637, -0.44251, 0.18785, 0.0027849, -0.18411, -0.11514, -0.78581], Float32[0.013441, 0.23682, -0.16899, 0.40951, 0.63812, 0.47709, -0.42852, -0.55641, -0.364, -0.23938  …  -0.080262, 0.63003, 0.32111, -0.46765, 0.22786, 0.36034, -0.37818, -0.56657, 0.044691, 0.30392], Float32[0.15164, 0.30177, -0.16763, 0.17684, 0.31719, 0.33973, -0.43478, -0.31086, -0.44999, -0.29486  …  -6.3681e-5, 0.068987, 0.087939, -0.10285, -0.13931, 0.22314, -0.080803, -0.35652, 0.016413, 0.10216], Float32[0.70853, 0.57088, -0.4716, 0.18048, 0.54449, 0.72603, 0.18157, -0.52393, 0.10381, -0.17566  …  -0.34727, 0.28483, 0.075693, -0.062178, -0.38988, 0.22902, -0.21617, -0.22562, -0.093918, -0.80375], Float32[0.68047, -0.039263, 0.30186, -0.17792, 0.42962, 0.032246, -0.41376, 0.13228, -0.29847, -0.085253  …  -0.094375, 0.018324, 0.21048, -0.03088, -0.19722, 0.082279, -0.09434, -0.073297, -0.064699, -0.26044], Float32[0.26818, 0.14346, -0.27877, 0.016257, 0.11384, 0.69923, -0.51332, -0.47368, -0.33075, -0.13834  …  -0.069043, 0.36885, 0.25168, -0.24517, 0.25381, 0.1367, -0.31178, -0.6321, -0.25028, -0.38097], Float32[0.33042, 0.24995, -0.60874, 0.10923, 0.036372, 0.151, -0.55083, -0.074239, -0.092307, -0.32821  …  -0.48609, -0.0080272, 0.031184, -0.36576, -0.42699, 0.42164, -0.11666, -0.50703, -0.027273, -0.53285], Float32[0.21705, 0.46515, -0.46757, 0.10082, 1.0135, 0.74845, -0.53104, -0.26256, 0.16812, 0.13182  …  0.13813, 0.36973, -0.64289, 0.024142, -0.039315, -0.26037, 0.12017, -0.043782, 0.41013, 0.1796], Float32[0.25769, 0.45629, -0.76974, -0.37679, 0.59272, -0.063527, 0.20545, -0.57385, -0.29009, -0.13662  …  0.030498, -0.39543, -0.38515, -1.0002, 0.087599, -0.31009, -0.34677, -0.31438, 0.75004, 0.97065], Float32[0.23727, 0.40478, -0.20547, 0.58805, 0.65533, 0.32867, -0.81964, -0.23236, 0.27428, 0.24265  …  -0.12342, 0.65961, -0.51802, -0.82995, -0.082739, 0.28155, -0.423, -0.27378, -0.007901, -0.030231]  …  Float32[-0.74397, 0.082164, -0.0091471, 0.4129, -0.42255, 0.10125, -0.18602, 0.21051, -0.59037, 0.66988  …  0.53364, -0.93231, -0.39097, 0.60638, 0.42173, 0.1062, -0.14878, -0.11773, -0.097637, 0.093382], Float32[-0.30016, -0.80268, -0.46637, -0.29822, -1.032, -1.0705, 0.84562, 0.70225, 0.11996, -0.7183  …  0.35178, 0.14789, -0.015559, 0.16185, 0.5095, -0.60983, 1.2486, 0.33713, -0.22136, -0.39756], Float32[-1.1167, 0.14057, 0.36302, -0.13836, -1.4797, -0.98573, 0.40487, -0.39773, -0.40102, 0.34691  …  0.30189, 0.048314, -0.079109, 0.58375, 0.14685, -0.46245, 0.44275, 0.21229, 0.16195, -0.78756], Float32[-0.24171, -0.23367, 0.10672, -1.6023, 0.1244, -0.016423, 0.1302, 0.70318, -0.14301, 0.47307  …  0.77956, -0.072954, 0.16443, -0.0061933, 0.061141, -0.28784, 0.58601, 0.47279, -0.61084, -0.72091], Float32[-0.042672, -0.088106, -0.31724, -0.25209, -0.26851, -0.06615, 0.90325, -0.13818, 0.3186, 0.30621  …  -0.38187, -0.91153, 0.11853, -0.13034, 0.48774, -0.99745, -0.069557, 0.24963, 0.75791, 0.50679], Float32[0.23204, 0.025672, -0.70699, -0.045465, 0.13989, -0.62807, 0.72625, 0.34108, 0.44614, 0.16329  …  -0.095526, -0.29605, 0.38567, 0.13684, 0.59331, -0.69486, 0.1241, -0.18069, -0.2583, -0.039673], Float32[-0.60921, -0.67218, 0.23521, -0.11195, -0.46094, -0.0074616, 0.25578, 0.85632, 0.055977, -0.23792  …  0.67205, -0.59822, -0.20259, 0.39243, 0.028873, 0.030003, -0.10617, -0.11411, -0.24901, -0.12026], Float32[-0.51181, 0.058706, 1.0913, -0.55163, -0.10249, -0.1265, 0.99503, 0.079711, -0.16246, 0.56488  …  0.024747, 0.20092, -1.0851, -0.13626, 0.35052, -0.85891, 0.067858, -0.25003, -1.125, 1.5863], Float32[-0.75898, -0.47426, 0.4737, 0.7725, -0.78064, 0.23233, 0.046114, 0.84014, 0.24371, 0.022978  …  0.45439, -0.84254, 0.1065, -0.059397, 0.090449, 0.30581, -0.61424, 0.78954, -0.014116, 0.6448], Float32[0.072617, -0.51393, 0.4728, -0.52202, -0.35534, 0.34629, 0.23211, 0.23096, 0.26694, 0.41028  …  0.6888, -0.17986, -0.066569, -0.48044, -0.55946, -0.27594, 0.056072, -0.18907, -0.59021, 0.55559]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wembed, wembedind = load_embed(\"glove.6B.50d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "BoundsError",
     "evalue": "BoundsError",
     "output_type": "error",
     "traceback": [
      "BoundsError",
      "",
      "Stacktrace:",
      " [1] getindex(::Int64, ::Int64) at .\\number.jl:78",
      " [2] top-level scope at In[9]:1"
     ]
    }
   ],
   "source": [
    "wembedind[get(wembed,\"and\",-1)[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "getind (generic function with 1 method)"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function getind(word; max=400000, root=false)\n",
    "    abc = get(wembed,lowercase(word),-1)\n",
    "    if root == true\n",
    "        return max-1\n",
    "    elseif (abc >= 0)\n",
    "        return abc\n",
    "    else\n",
    "        return max\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "399999"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rootemdind = 399999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "BoundsError",
     "evalue": "BoundsError: attempt to access 400000-element Array{Any,1} at index []",
     "output_type": "error",
     "traceback": [
      "BoundsError: attempt to access 400000-element Array{Any,1} at index []",
      "",
      "Stacktrace:",
      " [1] throw_boundserror(::Array{Any,1}, ::Tuple{}) at .\\abstractarray.jl:484",
      " [2] checkbounds at .\\abstractarray.jl:449 [inlined]",
      " [3] _getindex at .\\abstractarray.jl:949 [inlined]",
      " [4] getindex(::Array{Any,1}) at .\\abstractarray.jl:927",
      " [5] top-level scope at In[12]:1"
     ]
    }
   ],
   "source": [
    "wembedind[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wembed[\"'\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50-element Array{Float32,1}:\n",
       " -0.039369\n",
       "  1.2036  \n",
       "  0.35401 \n",
       " -0.55999 \n",
       " -0.52078 \n",
       " -0.66988 \n",
       " -0.75417 \n",
       " -0.6534  \n",
       " -0.23246 \n",
       "  0.58686 \n",
       " -0.40797 \n",
       "  1.2057  \n",
       " -1.11    \n",
       "  ⋮       \n",
       "  0.33207 \n",
       "  0.020538\n",
       " -0.60141 \n",
       "  0.50403 \n",
       " -0.083316\n",
       "  0.20239 \n",
       "  0.443   \n",
       " -0.060769\n",
       " -0.42807 \n",
       " -0.084135\n",
       "  0.49164 \n",
       "  0.085654"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wembedind[58]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000-element Array{Any,1}:\n",
       " Float32[0.418, 0.24968, -0.41242, 0.1217, 0.34527, -0.044457, -0.49688, -0.17862, -0.00066023, -0.6566  …  -0.29871, -0.15749, -0.34758, -0.045637, -0.44251, 0.18785, 0.0027849, -0.18411, -0.11514, -0.78581]   \n",
       " Float32[0.013441, 0.23682, -0.16899, 0.40951, 0.63812, 0.47709, -0.42852, -0.55641, -0.364, -0.23938  …  -0.080262, 0.63003, 0.32111, -0.46765, 0.22786, 0.36034, -0.37818, -0.56657, 0.044691, 0.30392]          \n",
       " Float32[0.15164, 0.30177, -0.16763, 0.17684, 0.31719, 0.33973, -0.43478, -0.31086, -0.44999, -0.29486  …  -6.3681e-5, 0.068987, 0.087939, -0.10285, -0.13931, 0.22314, -0.080803, -0.35652, 0.016413, 0.10216]    \n",
       " Float32[0.70853, 0.57088, -0.4716, 0.18048, 0.54449, 0.72603, 0.18157, -0.52393, 0.10381, -0.17566  …  -0.34727, 0.28483, 0.075693, -0.062178, -0.38988, 0.22902, -0.21617, -0.22562, -0.093918, -0.80375]        \n",
       " Float32[0.68047, -0.039263, 0.30186, -0.17792, 0.42962, 0.032246, -0.41376, 0.13228, -0.29847, -0.085253  …  -0.094375, 0.018324, 0.21048, -0.03088, -0.19722, 0.082279, -0.09434, -0.073297, -0.064699, -0.26044]\n",
       " Float32[0.26818, 0.14346, -0.27877, 0.016257, 0.11384, 0.69923, -0.51332, -0.47368, -0.33075, -0.13834  …  -0.069043, 0.36885, 0.25168, -0.24517, 0.25381, 0.1367, -0.31178, -0.6321, -0.25028, -0.38097]         \n",
       " Float32[0.33042, 0.24995, -0.60874, 0.10923, 0.036372, 0.151, -0.55083, -0.074239, -0.092307, -0.32821  …  -0.48609, -0.0080272, 0.031184, -0.36576, -0.42699, 0.42164, -0.11666, -0.50703, -0.027273, -0.53285]  \n",
       " Float32[0.21705, 0.46515, -0.46757, 0.10082, 1.0135, 0.74845, -0.53104, -0.26256, 0.16812, 0.13182  …  0.13813, 0.36973, -0.64289, 0.024142, -0.039315, -0.26037, 0.12017, -0.043782, 0.41013, 0.1796]            \n",
       " Float32[0.25769, 0.45629, -0.76974, -0.37679, 0.59272, -0.063527, 0.20545, -0.57385, -0.29009, -0.13662  …  0.030498, -0.39543, -0.38515, -1.0002, 0.087599, -0.31009, -0.34677, -0.31438, 0.75004, 0.97065]      \n",
       " Float32[0.23727, 0.40478, -0.20547, 0.58805, 0.65533, 0.32867, -0.81964, -0.23236, 0.27428, 0.24265  …  -0.12342, 0.65961, -0.51802, -0.82995, -0.082739, 0.28155, -0.423, -0.27378, -0.007901, -0.030231]        \n",
       " Float32[0.15272, 0.36181, -0.22168, 0.066051, 0.13029, 0.37075, -0.75874, -0.44722, 0.22563, 0.10208  …  0.020339, 0.2142, 0.044097, 0.14003, -0.20079, 0.074794, -0.36076, 0.43382, -0.084617, 0.1214]           \n",
       " Float32[-0.16768, 1.2151, 0.49515, 0.26836, -0.4585, -0.23311, -0.52822, -1.3557, 0.16098, 0.37691  …  0.029453, 1.4888, 0.52361, 0.099354, 1.2515, 0.099381, -0.079261, -0.30862, 0.30893, 0.11023]              \n",
       " Float32[0.88387, -0.14199, 0.13566, 0.098682, 0.51218, 0.49138, -0.47155, -0.30742, 0.01963, 0.12686  …  -0.35283, 0.44882, -0.16534, 0.31579, 0.14963, -0.071277, -0.53506, 0.52711, -0.20148, 0.0095952]        \n",
       " ⋮                                                                                                                                                                                                                 \n",
       " Float32[0.28677, -0.90521, -0.36906, 0.26354, -0.094759, -0.24936, 1.1724, 0.71973, 0.068801, 0.50144  …  -0.32084, -0.3243, -0.21523, -0.12142, -0.33796, 0.1333, 0.03839, 0.55117, 0.76385, -0.54141]           \n",
       " Float32[-0.28064, 0.15323, -0.30591, -0.90841, 0.32428, -0.64677, 0.6134, 0.52447, 0.2436, -0.068487  …  0.17918, -0.36046, -0.16821, 0.75879, -0.66969, -0.29004, 1.2623, 0.50678, -0.76215, 0.31148]            \n",
       " Float32[-0.74397, 0.082164, -0.0091471, 0.4129, -0.42255, 0.10125, -0.18602, 0.21051, -0.59037, 0.66988  …  0.53364, -0.93231, -0.39097, 0.60638, 0.42173, 0.1062, -0.14878, -0.11773, -0.097637, 0.093382]       \n",
       " Float32[-0.30016, -0.80268, -0.46637, -0.29822, -1.032, -1.0705, 0.84562, 0.70225, 0.11996, -0.7183  …  0.35178, 0.14789, -0.015559, 0.16185, 0.5095, -0.60983, 1.2486, 0.33713, -0.22136, -0.39756]              \n",
       " Float32[-1.1167, 0.14057, 0.36302, -0.13836, -1.4797, -0.98573, 0.40487, -0.39773, -0.40102, 0.34691  …  0.30189, 0.048314, -0.079109, 0.58375, 0.14685, -0.46245, 0.44275, 0.21229, 0.16195, -0.78756]           \n",
       " Float32[-0.24171, -0.23367, 0.10672, -1.6023, 0.1244, -0.016423, 0.1302, 0.70318, -0.14301, 0.47307  …  0.77956, -0.072954, 0.16443, -0.0061933, 0.061141, -0.28784, 0.58601, 0.47279, -0.61084, -0.72091]        \n",
       " Float32[-0.042672, -0.088106, -0.31724, -0.25209, -0.26851, -0.06615, 0.90325, -0.13818, 0.3186, 0.30621  …  -0.38187, -0.91153, 0.11853, -0.13034, 0.48774, -0.99745, -0.069557, 0.24963, 0.75791, 0.50679]      \n",
       " Float32[0.23204, 0.025672, -0.70699, -0.045465, 0.13989, -0.62807, 0.72625, 0.34108, 0.44614, 0.16329  …  -0.095526, -0.29605, 0.38567, 0.13684, 0.59331, -0.69486, 0.1241, -0.18069, -0.2583, -0.039673]         \n",
       " Float32[-0.60921, -0.67218, 0.23521, -0.11195, -0.46094, -0.0074616, 0.25578, 0.85632, 0.055977, -0.23792  …  0.67205, -0.59822, -0.20259, 0.39243, 0.028873, 0.030003, -0.10617, -0.11411, -0.24901, -0.12026]   \n",
       " Float32[-0.51181, 0.058706, 1.0913, -0.55163, -0.10249, -0.1265, 0.99503, 0.079711, -0.16246, 0.56488  …  0.024747, 0.20092, -1.0851, -0.13626, 0.35052, -0.85891, 0.067858, -0.25003, -1.125, 1.5863]            \n",
       " Float32[-0.75898, -0.47426, 0.4737, 0.7725, -0.78064, 0.23233, 0.046114, 0.84014, 0.24371, 0.022978  …  0.45439, -0.84254, 0.1065, -0.059397, 0.090449, 0.30581, -0.61424, 0.78954, -0.014116, 0.6448]            \n",
       " Float32[0.072617, -0.51393, 0.4728, -0.52202, -0.35534, 0.34629, 0.23211, 0.23096, 0.26694, 0.41028  …  0.6888, -0.17986, -0.066569, -0.48044, -0.55946, -0.27594, 0.056072, -0.18907, -0.59021, 0.55559]         "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wembedind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dict{Any,Any}(\"newdigate\"=>164100,\"daufuskie\"=>254784,\"single-arm\"=>192007,\"titration\"=>117879,\"qajar\"=>66399,\"pinheiro\"=>38763,\"hospitalet\"=>282158,\"kennedale\"=>223560,\"tetracyclic\"=>353804,\"moher\"=>167242…), Any[Float32[0.418, 0.24968, -0.41242, 0.1217, 0.34527, -0.044457, -0.49688, -0.17862, -0.00066023, -0.6566  …  -0.29871, -0.15749, -0.34758, -0.045637, -0.44251, 0.18785, 0.0027849, -0.18411, -0.11514, -0.78581], Float32[0.013441, 0.23682, -0.16899, 0.40951, 0.63812, 0.47709, -0.42852, -0.55641, -0.364, -0.23938  …  -0.080262, 0.63003, 0.32111, -0.46765, 0.22786, 0.36034, -0.37818, -0.56657, 0.044691, 0.30392], Float32[0.15164, 0.30177, -0.16763, 0.17684, 0.31719, 0.33973, -0.43478, -0.31086, -0.44999, -0.29486  …  -6.3681e-5, 0.068987, 0.087939, -0.10285, -0.13931, 0.22314, -0.080803, -0.35652, 0.016413, 0.10216], Float32[0.70853, 0.57088, -0.4716, 0.18048, 0.54449, 0.72603, 0.18157, -0.52393, 0.10381, -0.17566  …  -0.34727, 0.28483, 0.075693, -0.062178, -0.38988, 0.22902, -0.21617, -0.22562, -0.093918, -0.80375], Float32[0.68047, -0.039263, 0.30186, -0.17792, 0.42962, 0.032246, -0.41376, 0.13228, -0.29847, -0.085253  …  -0.094375, 0.018324, 0.21048, -0.03088, -0.19722, 0.082279, -0.09434, -0.073297, -0.064699, -0.26044], Float32[0.26818, 0.14346, -0.27877, 0.016257, 0.11384, 0.69923, -0.51332, -0.47368, -0.33075, -0.13834  …  -0.069043, 0.36885, 0.25168, -0.24517, 0.25381, 0.1367, -0.31178, -0.6321, -0.25028, -0.38097], Float32[0.33042, 0.24995, -0.60874, 0.10923, 0.036372, 0.151, -0.55083, -0.074239, -0.092307, -0.32821  …  -0.48609, -0.0080272, 0.031184, -0.36576, -0.42699, 0.42164, -0.11666, -0.50703, -0.027273, -0.53285], Float32[0.21705, 0.46515, -0.46757, 0.10082, 1.0135, 0.74845, -0.53104, -0.26256, 0.16812, 0.13182  …  0.13813, 0.36973, -0.64289, 0.024142, -0.039315, -0.26037, 0.12017, -0.043782, 0.41013, 0.1796], Float32[0.25769, 0.45629, -0.76974, -0.37679, 0.59272, -0.063527, 0.20545, -0.57385, -0.29009, -0.13662  …  0.030498, -0.39543, -0.38515, -1.0002, 0.087599, -0.31009, -0.34677, -0.31438, 0.75004, 0.97065], Float32[0.23727, 0.40478, -0.20547, 0.58805, 0.65533, 0.32867, -0.81964, -0.23236, 0.27428, 0.24265  …  -0.12342, 0.65961, -0.51802, -0.82995, -0.082739, 0.28155, -0.423, -0.27378, -0.007901, -0.030231]  …  Float32[-0.74397, 0.082164, -0.0091471, 0.4129, -0.42255, 0.10125, -0.18602, 0.21051, -0.59037, 0.66988  …  0.53364, -0.93231, -0.39097, 0.60638, 0.42173, 0.1062, -0.14878, -0.11773, -0.097637, 0.093382], Float32[-0.30016, -0.80268, -0.46637, -0.29822, -1.032, -1.0705, 0.84562, 0.70225, 0.11996, -0.7183  …  0.35178, 0.14789, -0.015559, 0.16185, 0.5095, -0.60983, 1.2486, 0.33713, -0.22136, -0.39756], Float32[-1.1167, 0.14057, 0.36302, -0.13836, -1.4797, -0.98573, 0.40487, -0.39773, -0.40102, 0.34691  …  0.30189, 0.048314, -0.079109, 0.58375, 0.14685, -0.46245, 0.44275, 0.21229, 0.16195, -0.78756], Float32[-0.24171, -0.23367, 0.10672, -1.6023, 0.1244, -0.016423, 0.1302, 0.70318, -0.14301, 0.47307  …  0.77956, -0.072954, 0.16443, -0.0061933, 0.061141, -0.28784, 0.58601, 0.47279, -0.61084, -0.72091], Float32[-0.042672, -0.088106, -0.31724, -0.25209, -0.26851, -0.06615, 0.90325, -0.13818, 0.3186, 0.30621  …  -0.38187, -0.91153, 0.11853, -0.13034, 0.48774, -0.99745, -0.069557, 0.24963, 0.75791, 0.50679], Float32[0.23204, 0.025672, -0.70699, -0.045465, 0.13989, -0.62807, 0.72625, 0.34108, 0.44614, 0.16329  …  -0.095526, -0.29605, 0.38567, 0.13684, 0.59331, -0.69486, 0.1241, -0.18069, -0.2583, -0.039673], Float32[-0.60921, -0.67218, 0.23521, -0.11195, -0.46094, -0.0074616, 0.25578, 0.85632, 0.055977, -0.23792  …  0.67205, -0.59822, -0.20259, 0.39243, 0.028873, 0.030003, -0.10617, -0.11411, -0.24901, -0.12026], Float32[-0.51181, 0.058706, 1.0913, -0.55163, -0.10249, -0.1265, 0.99503, 0.079711, -0.16246, 0.56488  …  0.024747, 0.20092, -1.0851, -0.13626, 0.35052, -0.85891, 0.067858, -0.25003, -1.125, 1.5863], Float32[-0.75898, -0.47426, 0.4737, 0.7725, -0.78064, 0.23233, 0.046114, 0.84014, 0.24371, 0.022978  …  0.45439, -0.84254, 0.1065, -0.059397, 0.090449, 0.30581, -0.61424, 0.78954, -0.014116, 0.6448], Float32[0.072617, -0.51393, 0.4728, -0.52202, -0.35534, 0.34629, 0.23211, 0.23096, 0.26694, 0.41028  …  0.6888, -0.17986, -0.066569, -0.48044, -0.55946, -0.27594, 0.056072, -0.18907, -0.59021, 0.55559]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wembeds5 = load_embed(\"glove.6B.50d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Any[Any[\"Al\", \"-\", \"Zaman\", \":\", \"American\", \"forces\", \"killed\", \"Shaikh\", \"Abdullah\", \"al\"  …  \"the\", \"town\", \"of\", \"Qaim\", \",\", \"near\", \"the\", \"Syrian\", \"border\", \".\"], Any[\"[\", \"This\", \"killing\", \"of\", \"a\", \"respected\", \"cleric\", \"will\", \"be\", \"causing\", \"us\", \"trouble\", \"for\", \"years\", \"to\", \"come\", \".\", \"]\"], Any[\"DPA\", \":\", \"Iraqi\", \"authorities\", \"announced\", \"that\", \"they\", \"had\", \"busted\", \"up\", \"3\", \"terrorist\", \"cells\", \"operating\", \"in\", \"Baghdad\", \".\"], Any[\"Two\", \"of\", \"them\", \"were\", \"being\", \"run\", \"by\", \"2\", \"officials\", \"of\", \"the\", \"Ministry\", \"of\", \"the\", \"Interior\", \"!\"], Any[\"The\", \"MoI\", \"in\", \"Iraq\", \"is\", \"equivalent\", \"to\", \"the\", \"US\", \"FBI\"  …  \"members\", \"of\", \"the\", \"Weathermen\", \"bombers\", \"back\", \"in\", \"the\", \"1960s\", \".\"], Any[\"The\", \"third\", \"was\", \"being\", \"run\", \"by\", \"the\", \"head\", \"of\", \"an\", \"investment\", \"firm\", \".\"], Any[\"You\", \"wonder\", \"if\", \"he\", \"was\", \"manipulating\", \"the\", \"market\", \"with\", \"his\", \"bombing\", \"targets\", \".\"], Any[\"The\", \"cells\", \"were\", \"operating\", \"in\", \"the\", \"Ghazaliyah\", \"and\", \"al\", \"-\", \"Jihad\", \"districts\", \"of\", \"the\", \"capital\", \".\"], Any[\"Although\", \"the\", \"announcement\", \"was\", \"probably\", \"made\", \"to\", \"show\", \"progress\", \"in\"  …  \"Baathists\", \"continue\", \"to\", \"penetrate\", \"the\", \"Iraqi\", \"government\", \"very\", \"hopeful\", \".\"], Any[\"It\", \"reminds\", \"me\", \"too\", \"much\", \"of\", \"the\", \"ARVN\", \"officers\", \"who\", \"were\", \"secretly\", \"working\", \"for\", \"the\", \"other\", \"side\", \"in\", \"Vietnam\", \".\"]  …  Any[\"Over\", \"two\", \"hours\", \"later\", \"(\", \"and\", \"ten\", \"minutes\", \"before\", \"they\", \"closed\", \")\", \"my\", \"car\", \"was\", \"finally\", \"finished\", \".\"], Any[\"A\", \"few\", \"minutes\", \"after\", \"I\", \"left\", \",\", \"I\", \"was\", \"called\"  …  \"which\", \"they\", \"should\", \"have\", \"left\", \"in\", \"the\", \"car\", \")\", \".\"], Any[\"Of\", \"course\", \",\", \"they\", \"would\", \"be\", \"closing\", \"in\", \"5\", \"minutes\"  …  \"to\", \"hurry\", \"up\", \"or\", \"get\", \"it\", \"the\", \"next\", \"day\", \".\"], Any[\"Of\", \"course\", \"I\", \"could\", \"n't\", \"make\", \"it\", \"back\", \"in\", \"time\"  …  \"stay\", \"5\", \"extra\", \"minutes\", \"to\", \"wait\", \"for\", \"me\", \")\", \".\"], Any[\"The\", \"next\", \"day\", \",\", \"no\", \"one\", \"could\", \"find\", \"my\", \"wheel\", \"lock\", \"and\", \"that\", \"particular\", \"technician\", \"was\", \"not\", \"in\", \".\"], Any[\"Of\", \"course\", \",\", \"they\", \"could\", \"n't\", \"call\", \"him\", \"either\", \"to\"  …  \"no\", \"wheel\", \"lock\", \"should\", \"I\", \"get\", \"a\", \"flat\", \")\", \".\"], Any[\"On\", \"Monday\", \"I\", \"called\", \"and\", \"again\", \"it\", \"was\", \"a\", \"big\"  …  \"do\", \"to\", \"find\", \"anyone\", \"who\", \"knew\", \"anything\", \"about\", \"it\", \".\"], Any[\"Supposedly\", \"they\", \"will\", \"be\", \"holding\", \"it\", \"for\", \"me\", \"this\", \"evening\"  …  \"'m\", \"sure\", \"that\", \"will\", \"also\", \"be\", \"a\", \"huge\", \"ordeal\", \".\"], Any[\"The\", \"employees\", \"at\", \"this\", \"Sear's\", \"are\", \"completely\", \"apathetic\", \"and\", \"there\"  …  \"be\", \"any\", \"sort\", \"of\", \"management\", \"that\", \"I\", \"could\", \"see\", \".\"], Any[\"I\", \"will\", \"never\", \"return\", \"there\", \"again\", \"(\", \"and\", \"now\", \"have\"  …  \"of\", \"work\", \"they\", \"actually\", \"performed\", \"on\", \"my\", \"car\", \")\", \".\"]], Any[[0.0 0.0 … 0.0 0.0; 1.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 1.0 0.0 … 0.0 0.0], [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], [0.0 0.0 … 0.0 0.0; 1.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 1.0 0.0 … 0.0 0.0], [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], [0.0 1.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], [0.0 1.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], [0.0 1.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 1.0 … 0.0 0.0], [0.0 1.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], [0.0 1.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 1.0 … 0.0 0.0]  …  [0.0 1.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 1.0 0.0], [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], [0.0 0.0 … 0.0 0.0; 1.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], [0.0 0.0 … 0.0 0.0; 1.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], [0.0 0.0 … 0.0 0.0; 1.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], [0.0 1.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], [0.0 1.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataa = load_data(\"en-ud-train.conllu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Any[Any[\"Al\", \"-\", \"Zaman\", \":\", \"American\", \"forces\", \"killed\", \"Shaikh\", \"Abdullah\", \"al\"  …  \"the\", \"town\", \"of\", \"Qaim\", \",\", \"near\", \"the\", \"Syrian\", \"border\", \".\"], Any[\"[\", \"This\", \"killing\", \"of\", \"a\", \"respected\", \"cleric\", \"will\", \"be\", \"causing\", \"us\", \"trouble\", \"for\", \"years\", \"to\", \"come\", \".\", \"]\"], Any[\"DPA\", \":\", \"Iraqi\", \"authorities\", \"announced\", \"that\", \"they\", \"had\", \"busted\", \"up\", \"3\", \"terrorist\", \"cells\", \"operating\", \"in\", \"Baghdad\", \".\"], Any[\"Two\", \"of\", \"them\", \"were\", \"being\", \"run\", \"by\", \"2\", \"officials\", \"of\", \"the\", \"Ministry\", \"of\", \"the\", \"Interior\", \"!\"], Any[\"The\", \"MoI\", \"in\", \"Iraq\", \"is\", \"equivalent\", \"to\", \"the\", \"US\", \"FBI\"  …  \"members\", \"of\", \"the\", \"Weathermen\", \"bombers\", \"back\", \"in\", \"the\", \"1960s\", \".\"], Any[\"The\", \"third\", \"was\", \"being\", \"run\", \"by\", \"the\", \"head\", \"of\", \"an\", \"investment\", \"firm\", \".\"], Any[\"You\", \"wonder\", \"if\", \"he\", \"was\", \"manipulating\", \"the\", \"market\", \"with\", \"his\", \"bombing\", \"targets\", \".\"], Any[\"The\", \"cells\", \"were\", \"operating\", \"in\", \"the\", \"Ghazaliyah\", \"and\", \"al\", \"-\", \"Jihad\", \"districts\", \"of\", \"the\", \"capital\", \".\"], Any[\"Although\", \"the\", \"announcement\", \"was\", \"probably\", \"made\", \"to\", \"show\", \"progress\", \"in\"  …  \"Baathists\", \"continue\", \"to\", \"penetrate\", \"the\", \"Iraqi\", \"government\", \"very\", \"hopeful\", \".\"], Any[\"It\", \"reminds\", \"me\", \"too\", \"much\", \"of\", \"the\", \"ARVN\", \"officers\", \"who\", \"were\", \"secretly\", \"working\", \"for\", \"the\", \"other\", \"side\", \"in\", \"Vietnam\", \".\"]  …  Any[\"Over\", \"two\", \"hours\", \"later\", \"(\", \"and\", \"ten\", \"minutes\", \"before\", \"they\", \"closed\", \")\", \"my\", \"car\", \"was\", \"finally\", \"finished\", \".\"], Any[\"A\", \"few\", \"minutes\", \"after\", \"I\", \"left\", \",\", \"I\", \"was\", \"called\"  …  \"which\", \"they\", \"should\", \"have\", \"left\", \"in\", \"the\", \"car\", \")\", \".\"], Any[\"Of\", \"course\", \",\", \"they\", \"would\", \"be\", \"closing\", \"in\", \"5\", \"minutes\"  …  \"to\", \"hurry\", \"up\", \"or\", \"get\", \"it\", \"the\", \"next\", \"day\", \".\"], Any[\"Of\", \"course\", \"I\", \"could\", \"n't\", \"make\", \"it\", \"back\", \"in\", \"time\"  …  \"stay\", \"5\", \"extra\", \"minutes\", \"to\", \"wait\", \"for\", \"me\", \")\", \".\"], Any[\"The\", \"next\", \"day\", \",\", \"no\", \"one\", \"could\", \"find\", \"my\", \"wheel\", \"lock\", \"and\", \"that\", \"particular\", \"technician\", \"was\", \"not\", \"in\", \".\"], Any[\"Of\", \"course\", \",\", \"they\", \"could\", \"n't\", \"call\", \"him\", \"either\", \"to\"  …  \"no\", \"wheel\", \"lock\", \"should\", \"I\", \"get\", \"a\", \"flat\", \")\", \".\"], Any[\"On\", \"Monday\", \"I\", \"called\", \"and\", \"again\", \"it\", \"was\", \"a\", \"big\"  …  \"do\", \"to\", \"find\", \"anyone\", \"who\", \"knew\", \"anything\", \"about\", \"it\", \".\"], Any[\"Supposedly\", \"they\", \"will\", \"be\", \"holding\", \"it\", \"for\", \"me\", \"this\", \"evening\"  …  \"'m\", \"sure\", \"that\", \"will\", \"also\", \"be\", \"a\", \"huge\", \"ordeal\", \".\"], Any[\"The\", \"employees\", \"at\", \"this\", \"Sear's\", \"are\", \"completely\", \"apathetic\", \"and\", \"there\"  …  \"be\", \"any\", \"sort\", \"of\", \"management\", \"that\", \"I\", \"could\", \"see\", \".\"], Any[\"I\", \"will\", \"never\", \"return\", \"there\", \"again\", \"(\", \"and\", \"now\", \"have\"  …  \"of\", \"work\", \"they\", \"actually\", \"performed\", \"on\", \"my\", \"car\", \")\", \".\"]], Any[[0.0 0.0 … 0.0 0.0; 1.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 1.0 0.0 … 0.0 0.0], [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], [0.0 0.0 … 0.0 0.0; 1.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 1.0 0.0 … 0.0 0.0], [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], [0.0 1.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], [0.0 1.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], [0.0 1.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 1.0 … 0.0 0.0], [0.0 1.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], [0.0 1.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 1.0 … 0.0 0.0]  …  [0.0 1.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 1.0 0.0], [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], [0.0 0.0 … 0.0 0.0; 1.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], [0.0 0.0 … 0.0 0.0; 1.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], [0.0 0.0 … 0.0 0.0; 1.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], [0.0 1.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], [0.0 1.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datax, datay = dataa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14-element Array{Float64,1}:\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 1.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshape(datay[7]', 13 * 13)[42:55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12543-element Array{Any,1}:\n",
       " Any[\"Al\", \"-\", \"Zaman\", \":\", \"American\", \"forces\", \"killed\", \"Shaikh\", \"Abdullah\", \"al\"  …  \"the\", \"town\", \"of\", \"Qaim\", \",\", \"near\", \"the\", \"Syrian\", \"border\", \".\"]                                 \n",
       " Any[\"[\", \"This\", \"killing\", \"of\", \"a\", \"respected\", \"cleric\", \"will\", \"be\", \"causing\", \"us\", \"trouble\", \"for\", \"years\", \"to\", \"come\", \".\", \"]\"]                                                       \n",
       " Any[\"DPA\", \":\", \"Iraqi\", \"authorities\", \"announced\", \"that\", \"they\", \"had\", \"busted\", \"up\", \"3\", \"terrorist\", \"cells\", \"operating\", \"in\", \"Baghdad\", \".\"]                                             \n",
       " Any[\"Two\", \"of\", \"them\", \"were\", \"being\", \"run\", \"by\", \"2\", \"officials\", \"of\", \"the\", \"Ministry\", \"of\", \"the\", \"Interior\", \"!\"]                                                                       \n",
       " Any[\"The\", \"MoI\", \"in\", \"Iraq\", \"is\", \"equivalent\", \"to\", \"the\", \"US\", \"FBI\"  …  \"members\", \"of\", \"the\", \"Weathermen\", \"bombers\", \"back\", \"in\", \"the\", \"1960s\", \".\"]                                  \n",
       " Any[\"The\", \"third\", \"was\", \"being\", \"run\", \"by\", \"the\", \"head\", \"of\", \"an\", \"investment\", \"firm\", \".\"]                                                                                                \n",
       " Any[\"You\", \"wonder\", \"if\", \"he\", \"was\", \"manipulating\", \"the\", \"market\", \"with\", \"his\", \"bombing\", \"targets\", \".\"]                                                                                    \n",
       " Any[\"The\", \"cells\", \"were\", \"operating\", \"in\", \"the\", \"Ghazaliyah\", \"and\", \"al\", \"-\", \"Jihad\", \"districts\", \"of\", \"the\", \"capital\", \".\"]                                                              \n",
       " Any[\"Although\", \"the\", \"announcement\", \"was\", \"probably\", \"made\", \"to\", \"show\", \"progress\", \"in\"  …  \"Baathists\", \"continue\", \"to\", \"penetrate\", \"the\", \"Iraqi\", \"government\", \"very\", \"hopeful\", \".\"]\n",
       " Any[\"It\", \"reminds\", \"me\", \"too\", \"much\", \"of\", \"the\", \"ARVN\", \"officers\", \"who\", \"were\", \"secretly\", \"working\", \"for\", \"the\", \"other\", \"side\", \"in\", \"Vietnam\", \".\"]                                 \n",
       " Any[\"Al\", \"-\", \"Zaman\", \":\", \"Guerrillas\", \"killed\", \"a\", \"member\", \"of\", \"the\", \"Kurdistan\", \"Democratic\", \"Party\", \"after\", \"kidnapping\", \"him\", \"in\", \"Mosul\", \".\"]                                \n",
       " Any[\"The\", \"police\", \"commander\", \"of\", \"Ninevah\", \"Province\", \"announced\", \"that\", \"bombings\", \"had\"  …  \"been\", \"a\", \"big\", \"jump\", \"in\", \"the\", \"number\", \"of\", \"kidnappings\", \".\"]                \n",
       " Any[\"On\", \"Wednesday\", \"guerrillas\", \"had\", \"kidnapped\", \"a\", \"cosmetic\", \"surgeon\", \"and\", \"his\", \"wife\", \"while\", \"they\", \"were\", \"on\", \"their\", \"way\", \"home\", \".\"]                                \n",
       " ⋮                                                                                                                                                                                                     \n",
       " Any[\"So\", \"I\", \"got\", \"just\", \"my\", \"other\", \"rear\", \"tire\", \"replaced\", \".\"]                                                                                                                         \n",
       " Any[\"They\", \"promised\", \"it\", \"'d\", \"be\", \"done\", \"within\", \"an\", \"hour\", \",\", \"so\", \"I\", \"waited\", \"in\", \"the\", \"lobby\", \".\"]                                                                        \n",
       " Any[\"Over\", \"two\", \"hours\", \"later\", \"(\", \"and\", \"ten\", \"minutes\", \"before\", \"they\", \"closed\", \")\", \"my\", \"car\", \"was\", \"finally\", \"finished\", \".\"]                                                   \n",
       " Any[\"A\", \"few\", \"minutes\", \"after\", \"I\", \"left\", \",\", \"I\", \"was\", \"called\"  …  \"which\", \"they\", \"should\", \"have\", \"left\", \"in\", \"the\", \"car\", \")\", \".\"]                                               \n",
       " Any[\"Of\", \"course\", \",\", \"they\", \"would\", \"be\", \"closing\", \"in\", \"5\", \"minutes\"  …  \"to\", \"hurry\", \"up\", \"or\", \"get\", \"it\", \"the\", \"next\", \"day\", \".\"]                                                \n",
       " Any[\"Of\", \"course\", \"I\", \"could\", \"n't\", \"make\", \"it\", \"back\", \"in\", \"time\"  …  \"stay\", \"5\", \"extra\", \"minutes\", \"to\", \"wait\", \"for\", \"me\", \")\", \".\"]                                                 \n",
       " Any[\"The\", \"next\", \"day\", \",\", \"no\", \"one\", \"could\", \"find\", \"my\", \"wheel\", \"lock\", \"and\", \"that\", \"particular\", \"technician\", \"was\", \"not\", \"in\", \".\"]                                               \n",
       " Any[\"Of\", \"course\", \",\", \"they\", \"could\", \"n't\", \"call\", \"him\", \"either\", \"to\"  …  \"no\", \"wheel\", \"lock\", \"should\", \"I\", \"get\", \"a\", \"flat\", \")\", \".\"]                                                \n",
       " Any[\"On\", \"Monday\", \"I\", \"called\", \"and\", \"again\", \"it\", \"was\", \"a\", \"big\"  …  \"do\", \"to\", \"find\", \"anyone\", \"who\", \"knew\", \"anything\", \"about\", \"it\", \".\"]                                           \n",
       " Any[\"Supposedly\", \"they\", \"will\", \"be\", \"holding\", \"it\", \"for\", \"me\", \"this\", \"evening\"  …  \"'m\", \"sure\", \"that\", \"will\", \"also\", \"be\", \"a\", \"huge\", \"ordeal\", \".\"]                                   \n",
       " Any[\"The\", \"employees\", \"at\", \"this\", \"Sear's\", \"are\", \"completely\", \"apathetic\", \"and\", \"there\"  …  \"be\", \"any\", \"sort\", \"of\", \"management\", \"that\", \"I\", \"could\", \"see\", \".\"]                       \n",
       " Any[\"I\", \"will\", \"never\", \"return\", \"there\", \"again\", \"(\", \"and\", \"now\", \"have\"  …  \"of\", \"work\", \"they\", \"actually\", \"performed\", \"on\", \"my\", \"car\", \")\", \".\"]                                       "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29-element Array{Any,1}:\n",
       " \"Al\"      \n",
       " \"-\"       \n",
       " \"Zaman\"   \n",
       " \":\"       \n",
       " \"American\"\n",
       " \"forces\"  \n",
       " \"killed\"  \n",
       " \"Shaikh\"  \n",
       " \"Abdullah\"\n",
       " \"al\"      \n",
       " \"-\"       \n",
       " \"Ani\"     \n",
       " \",\"       \n",
       " ⋮         \n",
       " \"mosque\"  \n",
       " \"in\"      \n",
       " \"the\"     \n",
       " \"town\"    \n",
       " \"of\"      \n",
       " \"Qaim\"    \n",
       " \",\"       \n",
       " \"near\"    \n",
       " \"the\"     \n",
       " \"Syrian\"  \n",
       " \"border\"  \n",
       " \".\"       "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datax[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12543-element Array{Any,1}:\n",
       " [0.0 0.0 … 0.0 0.0; 1.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 1.0 0.0 … 0.0 0.0]\n",
       " [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0]\n",
       " [0.0 0.0 … 0.0 0.0; 1.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 1.0 0.0 … 0.0 0.0]\n",
       " [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0]\n",
       " [0.0 1.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0]\n",
       " [0.0 1.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0]\n",
       " [0.0 1.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 1.0 … 0.0 0.0]\n",
       " [0.0 1.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0]\n",
       " [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0]\n",
       " [0.0 1.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 1.0 … 0.0 0.0]\n",
       " [0.0 0.0 … 0.0 0.0; 1.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 1.0 0.0 … 0.0 0.0]\n",
       " [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0]\n",
       " [0.0 1.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0]\n",
       " ⋮                                                                               \n",
       " [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0]\n",
       " [0.0 1.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 1.0 … 0.0 0.0]\n",
       " [0.0 1.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 1.0 0.0]\n",
       " [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0]\n",
       " [0.0 0.0 … 0.0 0.0; 1.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0]\n",
       " [0.0 0.0 … 0.0 0.0; 1.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0]\n",
       " [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0]\n",
       " [0.0 0.0 … 0.0 0.0; 1.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0]\n",
       " [0.0 1.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0]\n",
       " [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0]\n",
       " [0.0 1.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0]\n",
       " [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12543-element Array{Base.ReshapedArray{Float64,1,LinearAlgebra.Adjoint{Float64,Array{Float64,2}},Tuple{Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64}}},1}:\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0  …  0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
       " [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
       " [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
       " [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
       " [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
       " [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
       " [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
       " [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
       " ⋮                                                                                                      \n",
       " [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
       " [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
       " [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]\n",
       " [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
       " [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
       " [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
       " [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
       " [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
       " [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdatay = [reshape(y', size(y)[1] * size(y)[2]) for y in datay]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "nll(scores, answers; dims=1, average=true)\n",
       "\\end{verbatim}\n",
       "Given an unnormalized \\texttt{scores} matrix and an \\texttt{Integer} array of correct \\texttt{answers}, return the per-instance negative log likelihood. \\texttt{dims=1} means instances are in columns, \\texttt{dims=2} means instances are in rows.  Use \\texttt{average=false} to return the sum instead of per-instance average.\n",
       "\n",
       "\\begin{verbatim}\n",
       "nll(model, data; dims=1, average=true, o...)\n",
       "\\end{verbatim}\n",
       "Compute \\texttt{nll(model(x; o...), y; dims)} for \\texttt{(x,y)} in \\texttt{data} and return the per-instance average (if average=true) or total (if average=false) negative log likelihood.\n",
       "\n"
      ],
      "text/markdown": [
       "```\n",
       "nll(scores, answers; dims=1, average=true)\n",
       "```\n",
       "\n",
       "Given an unnormalized `scores` matrix and an `Integer` array of correct `answers`, return the per-instance negative log likelihood. `dims=1` means instances are in columns, `dims=2` means instances are in rows.  Use `average=false` to return the sum instead of per-instance average.\n",
       "\n",
       "```\n",
       "nll(model, data; dims=1, average=true, o...)\n",
       "```\n",
       "\n",
       "Compute `nll(model(x; o...), y; dims)` for `(x,y)` in `data` and return the per-instance average (if average=true) or total (if average=false) negative log likelihood.\n"
      ],
      "text/plain": [
       "\u001b[36m  nll(scores, answers; dims=1, average=true)\u001b[39m\n",
       "\n",
       "  Given an unnormalized \u001b[36mscores\u001b[39m matrix and an \u001b[36mInteger\u001b[39m array of correct\n",
       "  \u001b[36manswers\u001b[39m, return the per-instance negative log likelihood. \u001b[36mdims=1\u001b[39m\n",
       "  means instances are in columns, \u001b[36mdims=2\u001b[39m means instances are in rows.\n",
       "  Use \u001b[36maverage=false\u001b[39m to return the sum instead of per-instance average.\n",
       "\n",
       "\u001b[36m  nll(model, data; dims=1, average=true, o...)\u001b[39m\n",
       "\n",
       "  Compute \u001b[36mnll(model(x; o...), y; dims)\u001b[39m for \u001b[36m(x,y)\u001b[39m in \u001b[36mdata\u001b[39m and return\n",
       "  the per-instance average (if average=true) or total (if\n",
       "  average=false) negative log likelihood."
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@doc Knet.nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×2×3 reshape(::UnitRange{Int64}, 5, 2, 3) with eltype Int64:\n",
       "[:, :, 1] =\n",
       " 1   6\n",
       " 2   7\n",
       " 3   8\n",
       " 4   9\n",
       " 5  10\n",
       "\n",
       "[:, :, 2] =\n",
       " 11  16\n",
       " 12  17\n",
       " 13  18\n",
       " 14  19\n",
       " 15  20\n",
       "\n",
       "[:, :, 3] =\n",
       " 21  26\n",
       " 22  27\n",
       " 23  28\n",
       " 24  29\n",
       " 25  30"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat1 = reshape(1:30, (5,2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×6 reshape(::UnitRange{Int64}, 5, 6) with eltype Int64:\n",
       " 1   6  11  16  21  26\n",
       " 2   7  12  17  22  27\n",
       " 3   8  13  18  23  28\n",
       " 4   9  14  19  24  29\n",
       " 5  10  15  20  25  30"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat2 = Knet.mat(mat1, dims = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Embed; w; end\n",
    "\n",
    "Embed(vocab::Int,embed::Int)=Embed(param(embed,vocab))\n",
    "\n",
    "(e::Embed)(x) = e.w[:,x]  # (B,T)->(X,B,T)->rnn->(H,B,T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 reshape(::UnitRange{Int64}, 3, 3) with eltype Int64:\n",
       " 1  4  7\n",
       " 2  5  8\n",
       " 3  6  9"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wemb = reshape(1:9,3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "ename": "DimensionMismatch",
     "evalue": "DimensionMismatch(\"new dimensions (400000, 50) must be consistent with array size 400000\")",
     "output_type": "error",
     "traceback": [
      "DimensionMismatch(\"new dimensions (400000, 50) must be consistent with array size 400000\")",
      "",
      "Stacktrace:",
      " [1] (::getfield(Base, Symbol(\"#throw_dmrsa#193\")))(::Tuple{Int64,Int64}, ::Int64) at .\\reshapedarray.jl:41",
      " [2] reshape at .\\reshapedarray.jl:45 [inlined]",
      " [3] reshape(::Array{Any,1}, ::Int64, ::Int64) at .\\reshapedarray.jl:115",
      " [4] top-level scope at In[411]:1"
     ]
    }
   ],
   "source": [
    "reshape(wembedind, 400000, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length(wembedind[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "phi (generic function with 1 method)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phi(x, d) = [x.^i for i=0:d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×4 Array{Int64,2}:\n",
       " 1  1  1   1\n",
       " 1  2  4   8\n",
       " 1  3  9  27"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phi([1;2;3], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "phi (generic function with 1 method)"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phi(x, d) = x.^((0:d)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mb (generic function with 1 method)"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Minibatch data into (x,y,b) triples. This is the most complicated part of the code:\n",
    "# for language models x and y contain the same words shifted, x has an EOS in the beginning, y has an EOS at the end\n",
    "# x,y = [ s11,s21,s31,...,s12,s22,...] i.e. all the first words followed by all the second words etc.\n",
    "# b = [b1,b2,...,bT] i.e. how many sentences have first words, how many have second words etc.\n",
    "# length(x)==length(y)==sum(b) and length(b)=length(s1)+1 (+1 because of EOS)\n",
    "# sentences in batch should be sorted from longest to shortest, i.e. s1 is the longest sentence\n",
    "function mb(sentences,batchsize)\n",
    "    sentences = sort(sentences,by=length,rev=true)\n",
    "    data = []; eos = VOCABSIZE\n",
    "    for i = 1:batchsize:length(sentences)\n",
    "        j = min(i+batchsize-1,length(sentences))\n",
    "        sij = view(sentences,i:j)\n",
    "        T = 1+length(sij[1])\n",
    "        x = UInt16[]; y = UInt16[]; b = UInt16[]\n",
    "        for t=1:T\n",
    "            bt = 0\n",
    "            for s in sij\n",
    "                if t == 1\n",
    "                    push!(x,eos)\n",
    "                    push!(y,s[1])\n",
    "                elseif t <= length(s)\n",
    "                    push!(x,s[t-1])\n",
    "                    push!(y,s[t])\n",
    "                elseif t == 1+length(s)\n",
    "                    push!(x,s[t-1])\n",
    "                    push!(y,eos)\n",
    "                else\n",
    "                    break\n",
    "                end\n",
    "                bt += 1\n",
    "            end\n",
    "            push!(b,bt)\n",
    "        end\n",
    "        push!(data,(x,y,b))\n",
    "    end\n",
    "    return data\n",
    "end\n",
    "\n",
    "#mbtrn = mb(trn,BATCHSIZE)\n",
    "#mbval = mb(val,BATCHSIZE)\n",
    "#mbtst = mb(tst,BATCHSIZE)\n",
    "#map(length,(mbtrn,mbval,mbtst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "wembedmat = zeros(length(wembedind[1]), length(wembedind))\n",
    "for i=1:length(wembedind)\n",
    "    wembedmat[:,i] = wembedind[i]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50×400000 Array{Float64,2}:\n",
       "  0.418        0.013441  …  -0.51181   -0.75898    0.072617\n",
       "  0.24968      0.23682       0.058706  -0.47426   -0.51393 \n",
       " -0.41242     -0.16899       1.0913     0.4737     0.4728  \n",
       "  0.1217       0.40951      -0.55163    0.7725    -0.52202 \n",
       "  0.34527      0.63812      -0.10249   -0.78064   -0.35534 \n",
       " -0.044457     0.47709   …  -0.1265     0.23233    0.34629 \n",
       " -0.49688     -0.42852       0.99503    0.046114   0.23211 \n",
       " -0.17862     -0.55641       0.079711   0.84014    0.23096 \n",
       " -0.00066023  -0.364        -0.16246    0.24371    0.26694 \n",
       " -0.6566      -0.23938       0.56488    0.022978   0.41028 \n",
       "  0.27843      0.13001   …  -0.63306    0.53964    0.28031 \n",
       " -0.14767     -0.063734     -0.48592   -0.36101    0.14107 \n",
       " -0.55677     -0.39575       0.76247    0.94198   -0.30212 \n",
       "  ⋮                      ⋱                                 \n",
       "  0.012041     0.70358      -0.18204    0.035413  -0.83629 \n",
       " -0.054223     0.44858       0.041465   0.58834   -0.24698 \n",
       " -0.29871     -0.080262  …   0.024747   0.45439    0.6888  \n",
       " -0.15749      0.63003       0.20092   -0.84254   -0.17986 \n",
       " -0.34758      0.32111      -1.0851     0.1065    -0.066569\n",
       " -0.045637    -0.46765      -0.13626   -0.059397  -0.48044 \n",
       " -0.44251      0.22786       0.35052    0.090449  -0.55946 \n",
       "  0.18785      0.36034   …  -0.85891    0.30581   -0.27594 \n",
       "  0.0027849   -0.37818       0.067858  -0.61424    0.056072\n",
       " -0.18411     -0.56657      -0.25003    0.78954   -0.18907 \n",
       " -0.11514      0.044691     -1.125     -0.014116  -0.59021 \n",
       " -0.78581      0.30392       1.5863     0.6448     0.55559 "
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wembedmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Array{Int64,2}:\n",
       " 1  0  0\n",
       " 0  1  0\n",
       " 0  0  1"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xt = [1 0 0;\n",
    "     0 1 0;\n",
    "     0 0 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "softloss (generic function with 1 method)"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function softloss(ygold, ypred)\n",
    "    ynorm = ypred .- log(sum(exp(ypred),1))\n",
    "    -sum(ygold .* ynorm) / size(ygold,2)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "ename": "DimensionMismatch",
     "evalue": "DimensionMismatch(\"matrix is not square: dimensions are (3, 1)\")",
     "output_type": "error",
     "traceback": [
      "DimensionMismatch(\"matrix is not square: dimensions are (3, 1)\")",
      "",
      "Stacktrace:",
      " [1] checksquare at C:\\cygwin\\home\\Administrator\\buildbot\\worker\\package_win64\\build\\usr\\share\\julia\\stdlib\\v1.1\\LinearAlgebra\\src\\LinearAlgebra.jl:214 [inlined]",
      " [2] exp!(::Array{Float64,2}) at C:\\cygwin\\home\\Administrator\\buildbot\\worker\\package_win64\\build\\usr\\share\\julia\\stdlib\\v1.1\\LinearAlgebra\\src\\dense.jl:513",
      " [3] exp(::Array{Int64,2}) at C:\\cygwin\\home\\Administrator\\buildbot\\worker\\package_win64\\build\\usr\\share\\julia\\stdlib\\v1.1\\LinearAlgebra\\src\\dense.jl:482",
      " [4] softloss(::Array{Int64,2}, ::Array{Int64,2}) at .\\In[287]:2",
      " [5] top-level scope at In[288]:1"
     ]
    }
   ],
   "source": [
    "softloss(xta1,xts1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nlln1 (generic function with 1 method)"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For classification we use negative log likelihood loss (aka cross entropy, softmax loss, NLL)\n",
    "# This is the average -log probability assigned to correct answers by the model\n",
    "function nlln1(scores, y)\n",
    "    expscores = exp.(scores)\n",
    "    probabilities = expscores ./ sum(expscores, dims=1)\n",
    "    answerprobs = (probabilities[y[i],i] for i in 1:length(y))\n",
    "    mean(-log.(answerprobs))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×1 Array{Int64,2}:\n",
       " 1\n",
       " 0\n",
       " 0"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expscores = xts1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×1 Array{Float64,2}:\n",
       " 1.0\n",
       " 0.0\n",
       " 0.0"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities = expscores ./ sum(expscores, dims=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Base.Generator{UnitRange{Int64},getfield(Main, Symbol(\"##63#64\"))}(getfield(Main, Symbol(\"##63#64\"))(), 1:3)"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answerprobs = (probabilities[xta1[i],i] for i in 1:length(xta1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "ename": "BoundsError",
     "evalue": "BoundsError: attempt to access 3×1 Array{Float64,2} at index [0, 2]",
     "output_type": "error",
     "traceback": [
      "BoundsError: attempt to access 3×1 Array{Float64,2} at index [0, 2]",
      "",
      "Stacktrace:",
      " [1] getindex(::Array{Float64,2}, ::Int64, ::Int64) at .\\array.jl:730",
      " [2] (::getfield(Main, Symbol(\"##63#64\")))(::Int64) at .\\none:0",
      " [3] iterate(::Base.Generator{UnitRange{Int64},getfield(Main, Symbol(\"##63#64\"))}, ::Int64) at .\\generator.jl:47",
      " [4] top-level scope at In[285]:1"
     ]
    }
   ],
   "source": [
    "iterate(answerprobs, iterate(answerprobs)[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×2 Array{Int64,2}:\n",
       " 1  0\n",
       " 0  0\n",
       " 0  0"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xta1 = [1 0; 0 0; 0 0]\n",
    "xts1 = [1 0; 0 0; 0 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Int64,1}:\n",
       " 1\n",
       " 0\n",
       " 0"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xta1 = [1; 0; 0]\n",
    "xts1 = [1; 0; 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×3 Array{Int64,2}:\n",
       " 1  0  0"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xta1 = reshape(xta1, 3,1)\n",
    "xts1 = reshape(xts1, 1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.551444713932051"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nll(xts1, [3], dims=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "nll(scores, answers; dims=1, average=true)\n",
       "\\end{verbatim}\n",
       "Given an unnormalized \\texttt{scores} matrix and an \\texttt{Integer} array of correct \\texttt{answers}, return the per-instance negative log likelihood. \\texttt{dims=1} means instances are in columns, \\texttt{dims=2} means instances are in rows.  Use \\texttt{average=false} to return the sum instead of per-instance average.\n",
       "\n",
       "\\begin{verbatim}\n",
       "nll(model, data; dims=1, average=true, o...)\n",
       "\\end{verbatim}\n",
       "Compute \\texttt{nll(model(x; o...), y; dims)} for \\texttt{(x,y)} in \\texttt{data} and return the per-instance average (if average=true) or total (if average=false) negative log likelihood.\n",
       "\n"
      ],
      "text/markdown": [
       "```\n",
       "nll(scores, answers; dims=1, average=true)\n",
       "```\n",
       "\n",
       "Given an unnormalized `scores` matrix and an `Integer` array of correct `answers`, return the per-instance negative log likelihood. `dims=1` means instances are in columns, `dims=2` means instances are in rows.  Use `average=false` to return the sum instead of per-instance average.\n",
       "\n",
       "```\n",
       "nll(model, data; dims=1, average=true, o...)\n",
       "```\n",
       "\n",
       "Compute `nll(model(x; o...), y; dims)` for `(x,y)` in `data` and return the per-instance average (if average=true) or total (if average=false) negative log likelihood.\n"
      ],
      "text/plain": [
       "\u001b[36m  nll(scores, answers; dims=1, average=true)\u001b[39m\n",
       "\n",
       "  Given an unnormalized \u001b[36mscores\u001b[39m matrix and an \u001b[36mInteger\u001b[39m array of correct\n",
       "  \u001b[36manswers\u001b[39m, return the per-instance negative log likelihood. \u001b[36mdims=1\u001b[39m\n",
       "  means instances are in columns, \u001b[36mdims=2\u001b[39m means instances are in rows.\n",
       "  Use \u001b[36maverage=false\u001b[39m to return the sum instead of per-instance average.\n",
       "\n",
       "\u001b[36m  nll(model, data; dims=1, average=true, o...)\u001b[39m\n",
       "\n",
       "  Compute \u001b[36mnll(model(x; o...), y; dims)\u001b[39m for \u001b[36m(x,y)\u001b[39m in \u001b[36mdata\u001b[39m and return\n",
       "  the per-instance average (if average=true) or total (if\n",
       "  average=false) negative log likelihood."
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@doc Knet.nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "softmax(x; dims=1, algo=1)\n",
       "\\end{verbatim}\n",
       "The softmax function typically used in classification. Gives the same results as to \\texttt{exp.(logp(x, dims))}. \n",
       "\n",
       "If \\texttt{algo=1} computation is more accurate, if \\texttt{algo=0} it is  faster. \n",
       "\n",
       "See also \\texttt{logsoftmax}.\n",
       "\n"
      ],
      "text/markdown": [
       "```\n",
       "softmax(x; dims=1, algo=1)\n",
       "```\n",
       "\n",
       "The softmax function typically used in classification. Gives the same results as to `exp.(logp(x, dims))`. \n",
       "\n",
       "If `algo=1` computation is more accurate, if `algo=0` it is  faster. \n",
       "\n",
       "See also `logsoftmax`.\n"
      ],
      "text/plain": [
       "\u001b[36m  softmax(x; dims=1, algo=1)\u001b[39m\n",
       "\n",
       "  The softmax function typically used in classification. Gives the\n",
       "  same results as to \u001b[36mexp.(logp(x, dims))\u001b[39m. \n",
       "\n",
       "  If \u001b[36malgo=1\u001b[39m computation is more accurate, if \u001b[36malgo=0\u001b[39m it is faster. \n",
       "\n",
       "  See also \u001b[36mlogsoftmax\u001b[39m."
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@doc Knet.softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct wmat; wm; end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50×400000 Array{Float64,2}:\n",
       "  0.418        0.013441  …  -0.51181   -0.75898    0.072617\n",
       "  0.24968      0.23682       0.058706  -0.47426   -0.51393 \n",
       " -0.41242     -0.16899       1.0913     0.4737     0.4728  \n",
       "  0.1217       0.40951      -0.55163    0.7725    -0.52202 \n",
       "  0.34527      0.63812      -0.10249   -0.78064   -0.35534 \n",
       " -0.044457     0.47709   …  -0.1265     0.23233    0.34629 \n",
       " -0.49688     -0.42852       0.99503    0.046114   0.23211 \n",
       " -0.17862     -0.55641       0.079711   0.84014    0.23096 \n",
       " -0.00066023  -0.364        -0.16246    0.24371    0.26694 \n",
       " -0.6566      -0.23938       0.56488    0.022978   0.41028 \n",
       "  0.27843      0.13001   …  -0.63306    0.53964    0.28031 \n",
       " -0.14767     -0.063734     -0.48592   -0.36101    0.14107 \n",
       " -0.55677     -0.39575       0.76247    0.94198   -0.30212 \n",
       "  ⋮                      ⋱                                 \n",
       "  0.012041     0.70358      -0.18204    0.035413  -0.83629 \n",
       " -0.054223     0.44858       0.041465   0.58834   -0.24698 \n",
       " -0.29871     -0.080262  …   0.024747   0.45439    0.6888  \n",
       " -0.15749      0.63003       0.20092   -0.84254   -0.17986 \n",
       " -0.34758      0.32111      -1.0851     0.1065    -0.066569\n",
       " -0.045637    -0.46765      -0.13626   -0.059397  -0.48044 \n",
       " -0.44251      0.22786       0.35052    0.090449  -0.55946 \n",
       "  0.18785      0.36034   …  -0.85891    0.30581   -0.27594 \n",
       "  0.0027849   -0.37818       0.067858  -0.61424    0.056072\n",
       " -0.18411     -0.56657      -0.25003    0.78954   -0.18907 \n",
       " -0.11514      0.044691     -1.125     -0.014116  -0.59021 \n",
       " -0.78581      0.30392       1.5863     0.6448     0.55559 "
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wembedmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50-element Array{Float32,1}:\n",
       "  0.013441\n",
       "  0.23682 \n",
       " -0.16899 \n",
       "  0.40951 \n",
       "  0.63812 \n",
       "  0.47709 \n",
       " -0.42852 \n",
       " -0.55641 \n",
       " -0.364   \n",
       " -0.23938 \n",
       "  0.13001 \n",
       " -0.063734\n",
       " -0.39575 \n",
       "  ⋮       \n",
       "  0.70358 \n",
       "  0.44858 \n",
       " -0.080262\n",
       "  0.63003 \n",
       "  0.32111 \n",
       " -0.46765 \n",
       "  0.22786 \n",
       "  0.36034 \n",
       " -0.37818 \n",
       " -0.56657 \n",
       "  0.044691\n",
       "  0.30392 "
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wembedind[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2 Array{Int64,2}:\n",
       " 1  3\n",
       " 2  4"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [1 3;2 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Int64,1}:\n",
       " 51\n",
       " 52"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,1] = [51;52]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1,2] = 55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2 Array{Int64,2}:\n",
       " 51  3\n",
       " 52  4"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×2×2 Array{Int64,3}:\n",
       "[:, :, 1] =\n",
       " 1  7\n",
       " 2  8\n",
       " 3  9\n",
       "\n",
       "[:, :, 2] =\n",
       " 4  1\n",
       " 5  2\n",
       " 6  3"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wemb[:,x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Base.CodeUnits{UInt8,String}:\n",
       " 0x61\n",
       " 0x62"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa = \"ab\"\n",
    "sb = codeunits(sa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97\n",
      "98\n"
     ]
    }
   ],
   "source": [
    "for s in sb\n",
    "    print(s)\n",
    "    print(\"\\n\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "Usage:\n",
       "\n",
       "\\begin{verbatim}\n",
       "x = Param([1,2,3])          # user declares parameters with `Param`\n",
       "x => P([1,2,3])             # `Param` is just a struct wrapping a value\n",
       "value(x) => [1,2,3]         # `value` returns the thing wrapped\n",
       "sum(x .* x) => 14           # Params act like regular values\n",
       "y = @diff sum(x .* x)       # Except when we differentiate using `@diff`\n",
       "y => T(14)                  # you get another struct\n",
       "value(y) => 14              # which carries the same result\n",
       "params(y) => [x]            # and the Params that it depends on \n",
       "grad(y,x) => [2,4,6]        # and the gradients for all Params\n",
       "\\end{verbatim}\n",
       "\\texttt{Param(x)} returns a struct that acts like \\texttt{x} but marks it as a parameter you want to compute gradients with respect to.\n",
       "\n",
       "\\texttt{@diff expr} evaluates an expression and returns a struct that contains the result (which should be a scalar) and gradient information.\n",
       "\n",
       "\\texttt{grad(y, x)} returns the gradient of \\texttt{y} (output by @diff) with respect to any parameter \\texttt{x::Param}, or  \\texttt{nothing} if the gradient is 0.\n",
       "\n",
       "\\texttt{value(x)} returns the value associated with \\texttt{x} if \\texttt{x} is a \\texttt{Param} or the output of \\texttt{@diff}, otherwise returns \\texttt{x}.\n",
       "\n",
       "\\texttt{params(x)} returns an iterator of Params found by a recursive search of object \\texttt{x}.\n",
       "\n",
       "Alternative usage:\n",
       "\n",
       "\\begin{verbatim}\n",
       "x = [1 2 3]\n",
       "f(x) = sum(x .* x)\n",
       "f(x) => 14\n",
       "grad(f)(x) => [2 4 6]\n",
       "gradloss(f)(x) => ([2 4 6], 14)\n",
       "\\end{verbatim}\n",
       "Given a scalar valued function \\texttt{f}, \\texttt{grad(f,argnum=1)} returns another function \\texttt{g} which takes the same inputs as \\texttt{f} and returns the gradient of the output with respect to the argnum'th argument. \\texttt{gradloss} is similar except the resulting function also returns f's output.\n",
       "\n"
      ],
      "text/markdown": [
       "Usage:\n",
       "\n",
       "```\n",
       "x = Param([1,2,3])          # user declares parameters with `Param`\n",
       "x => P([1,2,3])             # `Param` is just a struct wrapping a value\n",
       "value(x) => [1,2,3]         # `value` returns the thing wrapped\n",
       "sum(x .* x) => 14           # Params act like regular values\n",
       "y = @diff sum(x .* x)       # Except when we differentiate using `@diff`\n",
       "y => T(14)                  # you get another struct\n",
       "value(y) => 14              # which carries the same result\n",
       "params(y) => [x]            # and the Params that it depends on \n",
       "grad(y,x) => [2,4,6]        # and the gradients for all Params\n",
       "```\n",
       "\n",
       "`Param(x)` returns a struct that acts like `x` but marks it as a parameter you want to compute gradients with respect to.\n",
       "\n",
       "`@diff expr` evaluates an expression and returns a struct that contains the result (which should be a scalar) and gradient information.\n",
       "\n",
       "`grad(y, x)` returns the gradient of `y` (output by @diff) with respect to any parameter `x::Param`, or  `nothing` if the gradient is 0.\n",
       "\n",
       "`value(x)` returns the value associated with `x` if `x` is a `Param` or the output of `@diff`, otherwise returns `x`.\n",
       "\n",
       "`params(x)` returns an iterator of Params found by a recursive search of object `x`.\n",
       "\n",
       "Alternative usage:\n",
       "\n",
       "```\n",
       "x = [1 2 3]\n",
       "f(x) = sum(x .* x)\n",
       "f(x) => 14\n",
       "grad(f)(x) => [2 4 6]\n",
       "gradloss(f)(x) => ([2 4 6], 14)\n",
       "```\n",
       "\n",
       "Given a scalar valued function `f`, `grad(f,argnum=1)` returns another function `g` which takes the same inputs as `f` and returns the gradient of the output with respect to the argnum'th argument. `gradloss` is similar except the resulting function also returns f's output.\n"
      ],
      "text/plain": [
       "  Usage:\n",
       "\n",
       "\u001b[36m  x = Param([1,2,3])          # user declares parameters with `Param`\u001b[39m\n",
       "\u001b[36m  x => P([1,2,3])             # `Param` is just a struct wrapping a value\u001b[39m\n",
       "\u001b[36m  value(x) => [1,2,3]         # `value` returns the thing wrapped\u001b[39m\n",
       "\u001b[36m  sum(x .* x) => 14           # Params act like regular values\u001b[39m\n",
       "\u001b[36m  y = @diff sum(x .* x)       # Except when we differentiate using `@diff`\u001b[39m\n",
       "\u001b[36m  y => T(14)                  # you get another struct\u001b[39m\n",
       "\u001b[36m  value(y) => 14              # which carries the same result\u001b[39m\n",
       "\u001b[36m  params(y) => [x]            # and the Params that it depends on \u001b[39m\n",
       "\u001b[36m  grad(y,x) => [2,4,6]        # and the gradients for all Params\u001b[39m\n",
       "\n",
       "  \u001b[36mParam(x)\u001b[39m returns a struct that acts like \u001b[36mx\u001b[39m but marks it as a\n",
       "  parameter you want to compute gradients with respect to.\n",
       "\n",
       "  \u001b[36m@diff expr\u001b[39m evaluates an expression and returns a struct that\n",
       "  contains the result (which should be a scalar) and gradient\n",
       "  information.\n",
       "\n",
       "  \u001b[36mgrad(y, x)\u001b[39m returns the gradient of \u001b[36my\u001b[39m (output by @diff) with respect\n",
       "  to any parameter \u001b[36mx::Param\u001b[39m, or \u001b[36mnothing\u001b[39m if the gradient is 0.\n",
       "\n",
       "  \u001b[36mvalue(x)\u001b[39m returns the value associated with \u001b[36mx\u001b[39m if \u001b[36mx\u001b[39m is a \u001b[36mParam\u001b[39m or the\n",
       "  output of \u001b[36m@diff\u001b[39m, otherwise returns \u001b[36mx\u001b[39m.\n",
       "\n",
       "  \u001b[36mparams(x)\u001b[39m returns an iterator of Params found by a recursive search\n",
       "  of object \u001b[36mx\u001b[39m.\n",
       "\n",
       "  Alternative usage:\n",
       "\n",
       "\u001b[36m  x = [1 2 3]\u001b[39m\n",
       "\u001b[36m  f(x) = sum(x .* x)\u001b[39m\n",
       "\u001b[36m  f(x) => 14\u001b[39m\n",
       "\u001b[36m  grad(f)(x) => [2 4 6]\u001b[39m\n",
       "\u001b[36m  gradloss(f)(x) => ([2 4 6], 14)\u001b[39m\n",
       "\n",
       "  Given a scalar valued function \u001b[36mf\u001b[39m, \u001b[36mgrad(f,argnum=1)\u001b[39m returns another\n",
       "  function \u001b[36mg\u001b[39m which takes the same inputs as \u001b[36mf\u001b[39m and returns the gradient\n",
       "  of the output with respect to the argnum'th argument. \u001b[36mgradloss\u001b[39m is\n",
       "  similar except the resulting function also returns f's output."
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@doc Knet.@diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tirtembed (generic function with 1 method)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function tirtembed(str::String)\n",
    "    strb = codeunits(str)\n",
    "    sum=0\n",
    "    for (i, aa) in enumerate(strb)\n",
    "        sum += 2^(i-1) * aa\n",
    "    end\n",
    "    atype = Knet.gpu()>=0 ? Knet.KnetArray : Array\n",
    "    Random.seed!(sum)\n",
    "    atype(randn(Float32, 20,1))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Dense; w; b; f; end\n",
    "Dense(i::Int,o::Int,f=identity) = Dense(param(o,i;atype=Array{Float64}), param0(o;atype=Array{Float64}), f)\n",
    "(d::Dense)(x) = d.f.(d.w * mat(x,dims=1) .+ d.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Linear; w; b; end\n",
    "\n",
    "Linear(input::Int, output::Int)=Linear(param(output,input;atype=Array{Float64}), param0(output;atype=Array{Float64}))\n",
    "\n",
    "(l::Linear)(x) = l.w * mat(x,dims=1) .+ l.b  # (H,B,T)->(H,B*T)->(V,B*T)\n",
    "(l::Linear)(x,y)= quadl(l(x),y)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "axpy!(a, X, Y)\n",
       "\\end{verbatim}\n",
       "Overwrite \\texttt{Y} with \\texttt{a*X + Y}, where \\texttt{a} is a scalar. Return \\texttt{Y}.\n",
       "\n",
       "\\section{Examples}\n",
       "\\begin{verbatim}\n",
       "julia> x = [1; 2; 3];\n",
       "\n",
       "julia> y = [4; 5; 6];\n",
       "\n",
       "julia> BLAS.axpy!(2, x, y)\n",
       "3-element Array{Int64,1}:\n",
       "  6\n",
       "  9\n",
       " 12\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "```\n",
       "axpy!(a, X, Y)\n",
       "```\n",
       "\n",
       "Overwrite `Y` with `a*X + Y`, where `a` is a scalar. Return `Y`.\n",
       "\n",
       "# Examples\n",
       "\n",
       "```jldoctest\n",
       "julia> x = [1; 2; 3];\n",
       "\n",
       "julia> y = [4; 5; 6];\n",
       "\n",
       "julia> BLAS.axpy!(2, x, y)\n",
       "3-element Array{Int64,1}:\n",
       "  6\n",
       "  9\n",
       " 12\n",
       "```\n"
      ],
      "text/plain": [
       "\u001b[36m  axpy!(a, X, Y)\u001b[39m\n",
       "\n",
       "  Overwrite \u001b[36mY\u001b[39m with \u001b[36ma*X + Y\u001b[39m, where \u001b[36ma\u001b[39m is a scalar. Return \u001b[36mY\u001b[39m.\n",
       "\n",
       "\u001b[1m  Examples\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "\u001b[36m  julia> x = [1; 2; 3];\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> y = [4; 5; 6];\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> BLAS.axpy!(2, x, y)\u001b[39m\n",
       "\u001b[36m  3-element Array{Int64,1}:\u001b[39m\n",
       "\u001b[36m    6\u001b[39m\n",
       "\u001b[36m    9\u001b[39m\n",
       "\u001b[36m   12\u001b[39m"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@doc LinearAlgebra.axpy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "axpy!(a, X, Y)\n",
       "\\end{verbatim}\n",
       "Overwrite \\texttt{Y} with \\texttt{a*X + Y}, where \\texttt{a} is a scalar. Return \\texttt{Y}.\n",
       "\n",
       "\\section{Examples}\n",
       "\\begin{verbatim}\n",
       "julia> x = [1; 2; 3];\n",
       "\n",
       "julia> y = [4; 5; 6];\n",
       "\n",
       "julia> BLAS.axpy!(2, x, y)\n",
       "3-element Array{Int64,1}:\n",
       "  6\n",
       "  9\n",
       " 12\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "```\n",
       "axpy!(a, X, Y)\n",
       "```\n",
       "\n",
       "Overwrite `Y` with `a*X + Y`, where `a` is a scalar. Return `Y`.\n",
       "\n",
       "# Examples\n",
       "\n",
       "```jldoctest\n",
       "julia> x = [1; 2; 3];\n",
       "\n",
       "julia> y = [4; 5; 6];\n",
       "\n",
       "julia> BLAS.axpy!(2, x, y)\n",
       "3-element Array{Int64,1}:\n",
       "  6\n",
       "  9\n",
       " 12\n",
       "```\n"
      ],
      "text/plain": [
       "\u001b[36m  axpy!(a, X, Y)\u001b[39m\n",
       "\n",
       "  Overwrite \u001b[36mY\u001b[39m with \u001b[36ma*X + Y\u001b[39m, where \u001b[36ma\u001b[39m is a scalar. Return \u001b[36mY\u001b[39m.\n",
       "\n",
       "\u001b[1m  Examples\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "\u001b[36m  julia> x = [1; 2; 3];\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> y = [4; 5; 6];\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> BLAS.axpy!(2, x, y)\u001b[39m\n",
       "\u001b[36m  3-element Array{Int64,1}:\u001b[39m\n",
       "\u001b[36m    6\u001b[39m\n",
       "\u001b[36m    9\u001b[39m\n",
       "\u001b[36m   12\u001b[39m"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@doc Knet.axpy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "quadl (generic function with 1 method)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quadl(yp, yg) = (yp[1]-yg[1]) * (yp[1]-yg[1])\n",
    "#quadl(yp::AbstractArray, yg::AbstractArray) = (yp[1]-yg[1]) * (yp[1]-yg[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quadl([5],[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: lin2 not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: lin2 not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[47]:1"
     ]
    }
   ],
   "source": [
    "Knet.params(lin2)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: lin1 not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: lin1 not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[48]:1"
     ]
    }
   ],
   "source": [
    "Knet.params(lin1)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stacktrace:\n",
      " [1] \u001b[1m(::getfield(Main, Symbol(\"##10#11\")))\u001b[22m\u001b[1m(\u001b[22m\u001b[1m)\u001b[22m at \u001b[1mC:\\Users\\dfhdhsd\\.julia\\packages\\AutoGrad\\KsPMr\\src\\core.jl:197\u001b[22m\n",
      " [2] \u001b[1m#differentiate#3\u001b[22m\u001b[1m(\u001b[22m::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}, ::Function, ::Function\u001b[1m)\u001b[22m at \u001b[1mC:\\Users\\dfhdhsd\\.julia\\packages\\AutoGrad\\KsPMr\\src\\core.jl:144\u001b[22m\n",
      " [3] \u001b[1mdifferentiate\u001b[22m\u001b[1m(\u001b[22m::Function\u001b[1m)\u001b[22m at \u001b[1mC:\\Users\\dfhdhsd\\.julia\\packages\\AutoGrad\\KsPMr\\src\\core.jl:135\u001b[22m\n",
      " [4] top-level scope at \u001b[1mIn[49]:1\u001b[22m\n",
      " [5] \u001b[1meval\u001b[22m at \u001b[1m.\\boot.jl:328\u001b[22m [inlined]\n",
      " [6] \u001b[1msoftscope_include_string\u001b[22m\u001b[1m(\u001b[22m::Module, ::String, ::String\u001b[1m)\u001b[22m at \u001b[1mC:\\Users\\dfhdhsd\\.julia\\packages\\SoftGlobalScope\\cSbw5\\src\\SoftGlobalScope.jl:218\u001b[22m\n",
      " [7] \u001b[1mexecute_request\u001b[22m\u001b[1m(\u001b[22m::ZMQ.Socket, ::IJulia.Msg\u001b[1m)\u001b[22m at \u001b[1mC:\\Users\\dfhdhsd\\.julia\\packages\\IJulia\\9ajf8\\src\\execute_request.jl:67\u001b[22m\n",
      " [8] \u001b[1m#invokelatest#1\u001b[22m at \u001b[1m.\\essentials.jl:742\u001b[22m [inlined]\n",
      " [9] \u001b[1minvokelatest\u001b[22m at \u001b[1m.\\essentials.jl:741\u001b[22m [inlined]\n",
      " [10] \u001b[1meventloop\u001b[22m\u001b[1m(\u001b[22m::ZMQ.Socket\u001b[1m)\u001b[22m at \u001b[1mC:\\Users\\dfhdhsd\\.julia\\packages\\IJulia\\9ajf8\\src\\eventloop.jl:8\u001b[22m\n",
      " [11] \u001b[1m(::getfield(IJulia, Symbol(\"##15#18\")))\u001b[22m\u001b[1m(\u001b[22m\u001b[1m)\u001b[22m at \u001b[1m.\\task.jl:259\u001b[22m\n"
     ]
    },
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: lin1 not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: lin1 not defined",
      "",
      "Stacktrace:",
      " [1] #differentiate#3(::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}, ::Function, ::Function) at C:\\Users\\dfhdhsd\\.julia\\packages\\AutoGrad\\KsPMr\\src\\core.jl:148",
      " [2] differentiate(::Function) at C:\\Users\\dfhdhsd\\.julia\\packages\\AutoGrad\\KsPMr\\src\\core.jl:135",
      " [3] top-level scope at In[49]:1"
     ]
    }
   ],
   "source": [
    "diff1 = Knet.@diff lin1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: diff1 not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: diff1 not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[50]:1"
     ]
    }
   ],
   "source": [
    "Knet.params(diff1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Test1; w1; w2; end\n",
    "Test1() = Test1(param(1), param(1))\n",
    "(l::Test1)(x) = begin\n",
    "    w_1 = l.w1\n",
    "    w_2 = l.w2\n",
    "    abc = false\n",
    "    ab = (l.w1[1] + x[1])\n",
    "    ba = (l.w2[1] + x[1])\n",
    "    if ab >= ba\n",
    "        abc = true\n",
    "    end\n",
    "    if abc\n",
    "            return l.w1 * x[1]\n",
    "    else\n",
    "        return return l.w2 * x[1]\n",
    "    end\n",
    "end\n",
    "(l::Test1)(x,y) = quadl(l(x),y)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Test2; w; end\n",
    "Test2() = Test2(param(2))\n",
    "(l::Test2)(x) = begin\n",
    "    ab = l.w + x\n",
    "    abc = false\n",
    "    if ab[1] >= ab[2]\n",
    "        abc = true\n",
    "    end\n",
    "    if abc\n",
    "            return l.w[1] * x[1]\n",
    "    else\n",
    "        return l.w[2] * x[1]\n",
    "    end\n",
    "end\n",
    "(l::Test2)(x,y) = quadl(l(x),y)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Test3; w; end\n",
    "Test3() = Test3(param(2))\n",
    "(l::Test3)(x) = begin\n",
    "    ab = []\n",
    "    for i = 1:length(x)\n",
    "        ab = cat(ab, l.w[i] * x[i] ; dims = 1)\n",
    "    end\n",
    "    return ab\n",
    "    #return l.w .* x\n",
    "end\n",
    "(l::Test3)(x,y) = quadl(l(x),y)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Int64,1}:\n",
       " 1\n",
       " 1"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = [1;1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Test3(P(Array{Float32,1}(2)))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testm3 = Test3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Param{Array{Float32,1}}:\n",
       " 0.71690845\n",
       " 0.09242151"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(Knet.params(testm3)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Any,1}:\n",
       " 0.71690845f0\n",
       " 0.09242151f0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testm3(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.68304f0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testm3(x1,[-5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.09e-13  100.00%┣█████████████████████▉┫ 100/100 [00:02/00:02, 65.80i/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100-element Array{Float32,1}:\n",
       " 32.68304     \n",
       " 20.917149    \n",
       " 13.386973    \n",
       "  8.567663    \n",
       "  5.4833045   \n",
       "  3.5093148   \n",
       "  2.2459614   \n",
       "  1.4374155   \n",
       "  0.91994554  \n",
       "  0.58876485  \n",
       "  0.37680963  \n",
       "  0.24115798  \n",
       "  0.15434118  \n",
       "  ⋮           \n",
       "  9.094947e-13\n",
       "  9.094947e-13\n",
       "  9.094947e-13\n",
       "  9.094947e-13\n",
       "  9.094947e-13\n",
       "  9.094947e-13\n",
       "  9.094947e-13\n",
       "  9.094947e-13\n",
       "  9.094947e-13\n",
       "  9.094947e-13\n",
       "  9.094947e-13\n",
       "  9.094947e-13"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collect(flatten(Knet.progress(Knet.sgd(testm3,Knet.repeat([([1;1], [-5])],100)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(P(Array{Float32,2}(1,2)), P(Array{Float32,1}(1)))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin1 = Linear(2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8994062f0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin1([1;1],[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×1 Array{Float32,2}:\n",
       " 1.2972357"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin1([1;1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8994062f0"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin1([1;1],[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: testm1 not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: testm1 not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[64]:1"
     ]
    }
   ],
   "source": [
    "testm1(x1, [2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Array{Tuple{Array{Int64,1},Array{Int64,1}},1}:\n",
       " ([1, 1], [3])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtrn = [([1;1], [3])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: testm1 not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: testm1 not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[66]:1"
     ]
    }
   ],
   "source": [
    "collect(flatten(Knet.progress(Knet.sgd(testm1,Knet.repeat([([1;1], [3])],100)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: trainresults not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: trainresults not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[67]:1"
     ]
    }
   ],
   "source": [
    "trainresults(\"df.jld2\",testm1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainresults (generic function with 1 method)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For running experiments\n",
    "function trainresults(file,model; o...)\n",
    "    if (print(\"Train from scratch? \"); readline()[1]=='y')\n",
    "        takeevery(n,itr) = (x for (i,x) in enumerate(itr) if i % n == 1)\n",
    "        r = ((model(dtrn), model(dtst), zeroone(model,dtrn), zeroone(model,dtst))\n",
    "             for x in takeevery(length(dtrn), progress(sgd(model,repeat(dtrn,100)))))\n",
    "        r = reshape(collect(Float32,flatten(r)),(4,:))\n",
    "        Knet.save(file,\"results\",r)\n",
    "        Knet.gc() # To save gpu memory\n",
    "    else\n",
    "        isfile(file) || download(\"http://people.csail.mit.edu/deniz/models/tutorial/$file\",file)\n",
    "        r = Knet.load(file,\"results\")\n",
    "    end\n",
    "    println(minimum(r,dims=2))\n",
    "    return r\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "flatten(iter)\n",
       "\\end{verbatim}\n",
       "Given an iterator that yields iterators, return an iterator that yields the elements of those iterators. Put differently, the elements of the argument iterator are concatenated.\n",
       "\n",
       "\\section{Examples}\n",
       "\\begin{verbatim}\n",
       "julia> collect(Iterators.flatten((1:2, 8:9)))\n",
       "4-element Array{Int64,1}:\n",
       " 1\n",
       " 2\n",
       " 8\n",
       " 9\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "```\n",
       "flatten(iter)\n",
       "```\n",
       "\n",
       "Given an iterator that yields iterators, return an iterator that yields the elements of those iterators. Put differently, the elements of the argument iterator are concatenated.\n",
       "\n",
       "# Examples\n",
       "\n",
       "```jldoctest\n",
       "julia> collect(Iterators.flatten((1:2, 8:9)))\n",
       "4-element Array{Int64,1}:\n",
       " 1\n",
       " 2\n",
       " 8\n",
       " 9\n",
       "```\n"
      ],
      "text/plain": [
       "\u001b[36m  flatten(iter)\u001b[39m\n",
       "\n",
       "  Given an iterator that yields iterators, return an iterator that\n",
       "  yields the elements of those iterators. Put differently, the\n",
       "  elements of the argument iterator are concatenated.\n",
       "\n",
       "\u001b[1m  Examples\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "\u001b[36m  julia> collect(Iterators.flatten((1:2, 8:9)))\u001b[39m\n",
       "\u001b[36m  4-element Array{Int64,1}:\u001b[39m\n",
       "\u001b[36m   1\u001b[39m\n",
       "\u001b[36m   2\u001b[39m\n",
       "\u001b[36m   8\u001b[39m\n",
       "\u001b[36m   9\u001b[39m"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@doc flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: testm1 not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: testm1 not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[70]:1"
     ]
    }
   ],
   "source": [
    "Knet.params(testm1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: testm1 not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: testm1 not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[71]:1"
     ]
    }
   ],
   "source": [
    "Knet.params(testm1)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "minimize(func, data, optimizer=Adam(); params)\n",
       "sgd     (func, data; lr=0.1,  gclip, params)\n",
       "momentum(func, data; lr=0.05, gamma=0.95, gclip, params)\n",
       "nesterov(func, data; lr=0.05, gamma=0.95, gclip, params)\n",
       "adagrad (func, data; lr=0.05, eps=1e-6, gclip, params)\n",
       "rmsprop (func, data; lr=0.01, rho=0.9, eps=1e-6, gclip, params)\n",
       "adadelta(func, data; lr=1.0,  rho=0.9, eps=1e-6, gclip, params)\n",
       "adam    (func, data; lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8, gclip, params)\n",
       "\\end{verbatim}\n",
       "Return an iterator which applies \\texttt{func} to arguments in \\texttt{data}, i.e.  \\texttt{(func(args...) for args in data)}, and updates the parameters every iteration to minimize \\texttt{func}.  \\texttt{func} should return a scalar value.\n",
       "\n",
       "The common keyword argument \\texttt{params} can be used to list the \\texttt{Param}s to be optimized.  If not specified, any \\texttt{Param} that takes part in the computation of \\texttt{func(args...)} will be updated.\n",
       "\n",
       "The common keyword argument \\texttt{gclip} can be used to implement per-parameter gradient clipping. For a parameter gradient \\texttt{g}, if \\texttt{norm(g) > gclip > 0}, \\texttt{g} is scaled so that its norm is equal to \\texttt{gclip}. If not specified no gradient clipping is performed.\n",
       "\n",
       "These functions do not perform optimization, but return an iterator that can. Any function that produces values from an iterator can be used with such an object, e.g. \\texttt{progress!(sgd(f,d))} iterates the sgd optimizer and displays a progress bar. For convenience, appending \\texttt{!} to the name of the function iterates and returns \\texttt{nothing}, i.e. \\texttt{sgd!(...)} is equivalent to \\texttt{(for x in sgd(...) end)}.\n",
       "\n",
       "We define optimizers as lazy iterators to have explicit control over them:\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item To report progress use \\texttt{progress(sgd(f,d))}.\n",
       "\n",
       "\n",
       "\\item To run until convergence use \\texttt{converge(sgd(f,cycle(d)))}.\n",
       "\n",
       "\n",
       "\\item To run multiple epochs use \\texttt{sgd(f,repeat(d,n))}.\n",
       "\n",
       "\n",
       "\\item To run a given number of iterations use \\texttt{sgd(f,take(cycle(d),n))}.\n",
       "\n",
       "\n",
       "\\item To do a task every n iterations use \\texttt{(task() for (i,j) in enumerate(sgd(f,d)) if i\\%n == 1)}.\n",
       "\n",
       "\\end{itemize}\n",
       "These functions apply the same algorithm with the same configuration to every parameter by default. \\texttt{minimize} takes an explicit optimizer argument, all others call \\texttt{minimize} with an appropriate optimizer argument (see \\texttt{@doc update!} for a list of possible optimizers). Before calling \\href{@ref}{\\texttt{update!}} on a \\texttt{Param}, \\texttt{minimize} sets its \\texttt{opt} field to a copy of this default optimizer if it is not already set. The \\texttt{opt} field is used by the \\texttt{update!} function to determine the type of update performed on that parameter.  If you need finer grained control, you can set the optimizer of an individual \\texttt{Param} by setting its \\texttt{opt} field before calling one of these functions. They will not override the \\texttt{opt} field if it is already set, e.g. \\texttt{sgd(model,data)} will perform an \\texttt{Adam} update for a parameter whose \\texttt{opt} field is an \\texttt{Adam} object. This also means you can stop and start the training without losing optimization state, the first call will set the \\texttt{opt} fields and the subsequent calls will not override them.\n",
       "\n",
       "Given a parameter \\texttt{w} and its gradient \\texttt{g} here are the updates applied by each optimizer:\n",
       "\n",
       "\\begin{verbatim}\n",
       "# sgd (http://en.wikipedia.org/wiki/Stochastic_gradient_descent)\n",
       "w .= w - lr * g\n",
       "\n",
       "# momentum (http://jlmelville.github.io/mize/nesterov.html)\n",
       "v .= gamma * v - lr * g\n",
       "w .= w + v\n",
       "\n",
       "# nesterov (http://jlmelville.github.io/mize/nesterov.html)\n",
       "w .= w - gamma * v\n",
       "v .= gamma * v - lr * g\n",
       "w .= w + (1 + gamma) * v\n",
       "\n",
       "# adagrad (http://www.jmlr.org/papers/v12/duchi11a.html)\n",
       "G .= G + g .^ 2\n",
       "w .= w - lr * g ./ sqrt(G + eps)\n",
       "\n",
       "# rmsprop (http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)\n",
       "G .= rho * G + (1-rho) * g .^ 2 \n",
       "w .= w - lr * g ./ sqrt(G + eps)\n",
       "\n",
       "# adadelta (http://arxiv.org/abs/1212.5701)\n",
       "G .= rho * G + (1-rho) * g .^ 2\n",
       "update = sqrt(delta + eps) .* g ./ sqrt(G + eps)\n",
       "w = w - lr * update\n",
       "delta = rho * delta + (1-rho) * update .^ 2\n",
       "\n",
       "# adam (http://arxiv.org/abs/1412.6980)\n",
       "v = beta1 * v + (1 - beta1) * g\n",
       "G = beta2 * G + (1 - beta2) * g .^ 2\n",
       "vhat = v ./ (1 - beta1 ^ t)\n",
       "Ghat = G ./ (1 - beta2 ^ t)\n",
       "w = w - (lr / (sqrt(Ghat) + eps)) * vhat\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "```\n",
       "minimize(func, data, optimizer=Adam(); params)\n",
       "sgd     (func, data; lr=0.1,  gclip, params)\n",
       "momentum(func, data; lr=0.05, gamma=0.95, gclip, params)\n",
       "nesterov(func, data; lr=0.05, gamma=0.95, gclip, params)\n",
       "adagrad (func, data; lr=0.05, eps=1e-6, gclip, params)\n",
       "rmsprop (func, data; lr=0.01, rho=0.9, eps=1e-6, gclip, params)\n",
       "adadelta(func, data; lr=1.0,  rho=0.9, eps=1e-6, gclip, params)\n",
       "adam    (func, data; lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8, gclip, params)\n",
       "```\n",
       "\n",
       "Return an iterator which applies `func` to arguments in `data`, i.e.  `(func(args...) for args in data)`, and updates the parameters every iteration to minimize `func`.  `func` should return a scalar value.\n",
       "\n",
       "The common keyword argument `params` can be used to list the `Param`s to be optimized.  If not specified, any `Param` that takes part in the computation of `func(args...)` will be updated.\n",
       "\n",
       "The common keyword argument `gclip` can be used to implement per-parameter gradient clipping. For a parameter gradient `g`, if `norm(g) > gclip > 0`, `g` is scaled so that its norm is equal to `gclip`. If not specified no gradient clipping is performed.\n",
       "\n",
       "These functions do not perform optimization, but return an iterator that can. Any function that produces values from an iterator can be used with such an object, e.g. `progress!(sgd(f,d))` iterates the sgd optimizer and displays a progress bar. For convenience, appending `!` to the name of the function iterates and returns `nothing`, i.e. `sgd!(...)` is equivalent to `(for x in sgd(...) end)`.\n",
       "\n",
       "We define optimizers as lazy iterators to have explicit control over them:\n",
       "\n",
       "  * To report progress use `progress(sgd(f,d))`.\n",
       "  * To run until convergence use `converge(sgd(f,cycle(d)))`.\n",
       "  * To run multiple epochs use `sgd(f,repeat(d,n))`.\n",
       "  * To run a given number of iterations use `sgd(f,take(cycle(d),n))`.\n",
       "  * To do a task every n iterations use `(task() for (i,j) in enumerate(sgd(f,d)) if i%n == 1)`.\n",
       "\n",
       "These functions apply the same algorithm with the same configuration to every parameter by default. `minimize` takes an explicit optimizer argument, all others call `minimize` with an appropriate optimizer argument (see `@doc update!` for a list of possible optimizers). Before calling [`update!`](@ref) on a `Param`, `minimize` sets its `opt` field to a copy of this default optimizer if it is not already set. The `opt` field is used by the `update!` function to determine the type of update performed on that parameter.  If you need finer grained control, you can set the optimizer of an individual `Param` by setting its `opt` field before calling one of these functions. They will not override the `opt` field if it is already set, e.g. `sgd(model,data)` will perform an `Adam` update for a parameter whose `opt` field is an `Adam` object. This also means you can stop and start the training without losing optimization state, the first call will set the `opt` fields and the subsequent calls will not override them.\n",
       "\n",
       "Given a parameter `w` and its gradient `g` here are the updates applied by each optimizer:\n",
       "\n",
       "```\n",
       "# sgd (http://en.wikipedia.org/wiki/Stochastic_gradient_descent)\n",
       "w .= w - lr * g\n",
       "\n",
       "# momentum (http://jlmelville.github.io/mize/nesterov.html)\n",
       "v .= gamma * v - lr * g\n",
       "w .= w + v\n",
       "\n",
       "# nesterov (http://jlmelville.github.io/mize/nesterov.html)\n",
       "w .= w - gamma * v\n",
       "v .= gamma * v - lr * g\n",
       "w .= w + (1 + gamma) * v\n",
       "\n",
       "# adagrad (http://www.jmlr.org/papers/v12/duchi11a.html)\n",
       "G .= G + g .^ 2\n",
       "w .= w - lr * g ./ sqrt(G + eps)\n",
       "\n",
       "# rmsprop (http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)\n",
       "G .= rho * G + (1-rho) * g .^ 2 \n",
       "w .= w - lr * g ./ sqrt(G + eps)\n",
       "\n",
       "# adadelta (http://arxiv.org/abs/1212.5701)\n",
       "G .= rho * G + (1-rho) * g .^ 2\n",
       "update = sqrt(delta + eps) .* g ./ sqrt(G + eps)\n",
       "w = w - lr * update\n",
       "delta = rho * delta + (1-rho) * update .^ 2\n",
       "\n",
       "# adam (http://arxiv.org/abs/1412.6980)\n",
       "v = beta1 * v + (1 - beta1) * g\n",
       "G = beta2 * G + (1 - beta2) * g .^ 2\n",
       "vhat = v ./ (1 - beta1 ^ t)\n",
       "Ghat = G ./ (1 - beta2 ^ t)\n",
       "w = w - (lr / (sqrt(Ghat) + eps)) * vhat\n",
       "```\n"
      ],
      "text/plain": [
       "\u001b[36m  minimize(func, data, optimizer=Adam(); params)\u001b[39m\n",
       "\u001b[36m  sgd     (func, data; lr=0.1,  gclip, params)\u001b[39m\n",
       "\u001b[36m  momentum(func, data; lr=0.05, gamma=0.95, gclip, params)\u001b[39m\n",
       "\u001b[36m  nesterov(func, data; lr=0.05, gamma=0.95, gclip, params)\u001b[39m\n",
       "\u001b[36m  adagrad (func, data; lr=0.05, eps=1e-6, gclip, params)\u001b[39m\n",
       "\u001b[36m  rmsprop (func, data; lr=0.01, rho=0.9, eps=1e-6, gclip, params)\u001b[39m\n",
       "\u001b[36m  adadelta(func, data; lr=1.0,  rho=0.9, eps=1e-6, gclip, params)\u001b[39m\n",
       "\u001b[36m  adam    (func, data; lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8, gclip, params)\u001b[39m\n",
       "\n",
       "  Return an iterator which applies \u001b[36mfunc\u001b[39m to arguments in \u001b[36mdata\u001b[39m, i.e.\n",
       "  \u001b[36m(func(args...) for args in data)\u001b[39m, and updates the parameters every\n",
       "  iteration to minimize \u001b[36mfunc\u001b[39m. \u001b[36mfunc\u001b[39m should return a scalar value.\n",
       "\n",
       "  The common keyword argument \u001b[36mparams\u001b[39m can be used to list the \u001b[36mParam\u001b[39ms to\n",
       "  be optimized. If not specified, any \u001b[36mParam\u001b[39m that takes part in the\n",
       "  computation of \u001b[36mfunc(args...)\u001b[39m will be updated.\n",
       "\n",
       "  The common keyword argument \u001b[36mgclip\u001b[39m can be used to implement\n",
       "  per-parameter gradient clipping. For a parameter gradient \u001b[36mg\u001b[39m, if\n",
       "  \u001b[36mnorm(g) > gclip > 0\u001b[39m, \u001b[36mg\u001b[39m is scaled so that its norm is equal to \u001b[36mgclip\u001b[39m.\n",
       "  If not specified no gradient clipping is performed.\n",
       "\n",
       "  These functions do not perform optimization, but return an iterator\n",
       "  that can. Any function that produces values from an iterator can be\n",
       "  used with such an object, e.g. \u001b[36mprogress!(sgd(f,d))\u001b[39m iterates the sgd\n",
       "  optimizer and displays a progress bar. For convenience, appending \u001b[36m!\u001b[39m\n",
       "  to the name of the function iterates and returns \u001b[36mnothing\u001b[39m, i.e.\n",
       "  \u001b[36msgd!(...)\u001b[39m is equivalent to \u001b[36m(for x in sgd(...) end)\u001b[39m.\n",
       "\n",
       "  We define optimizers as lazy iterators to have explicit control over\n",
       "  them:\n",
       "\n",
       "    •    To report progress use \u001b[36mprogress(sgd(f,d))\u001b[39m.\n",
       "\n",
       "    •    To run until convergence use \u001b[36mconverge(sgd(f,cycle(d)))\u001b[39m.\n",
       "\n",
       "    •    To run multiple epochs use \u001b[36msgd(f,repeat(d,n))\u001b[39m.\n",
       "\n",
       "    •    To run a given number of iterations use\n",
       "        \u001b[36msgd(f,take(cycle(d),n))\u001b[39m.\n",
       "\n",
       "    •    To do a task every n iterations use \u001b[36m(task() for (i,j) in\n",
       "        enumerate(sgd(f,d)) if i%n == 1)\u001b[39m.\n",
       "\n",
       "  These functions apply the same algorithm with the same configuration\n",
       "  to every parameter by default. \u001b[36mminimize\u001b[39m takes an explicit optimizer\n",
       "  argument, all others call \u001b[36mminimize\u001b[39m with an appropriate optimizer\n",
       "  argument (see \u001b[36m@doc update!\u001b[39m for a list of possible optimizers).\n",
       "  Before calling \u001b[36mupdate!\u001b[39m on a \u001b[36mParam\u001b[39m, \u001b[36mminimize\u001b[39m sets its \u001b[36mopt\u001b[39m field to a\n",
       "  copy of this default optimizer if it is not already set. The \u001b[36mopt\u001b[39m\n",
       "  field is used by the \u001b[36mupdate!\u001b[39m function to determine the type of\n",
       "  update performed on that parameter. If you need finer grained\n",
       "  control, you can set the optimizer of an individual \u001b[36mParam\u001b[39m by setting\n",
       "  its \u001b[36mopt\u001b[39m field before calling one of these functions. They will not\n",
       "  override the \u001b[36mopt\u001b[39m field if it is already set, e.g. \u001b[36msgd(model,data)\u001b[39m\n",
       "  will perform an \u001b[36mAdam\u001b[39m update for a parameter whose \u001b[36mopt\u001b[39m field is an\n",
       "  \u001b[36mAdam\u001b[39m object. This also means you can stop and start the training\n",
       "  without losing optimization state, the first call will set the \u001b[36mopt\u001b[39m\n",
       "  fields and the subsequent calls will not override them.\n",
       "\n",
       "  Given a parameter \u001b[36mw\u001b[39m and its gradient \u001b[36mg\u001b[39m here are the updates applied\n",
       "  by each optimizer:\n",
       "\n",
       "\u001b[36m  # sgd (http://en.wikipedia.org/wiki/Stochastic_gradient_descent)\u001b[39m\n",
       "\u001b[36m  w .= w - lr * g\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  # momentum (http://jlmelville.github.io/mize/nesterov.html)\u001b[39m\n",
       "\u001b[36m  v .= gamma * v - lr * g\u001b[39m\n",
       "\u001b[36m  w .= w + v\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  # nesterov (http://jlmelville.github.io/mize/nesterov.html)\u001b[39m\n",
       "\u001b[36m  w .= w - gamma * v\u001b[39m\n",
       "\u001b[36m  v .= gamma * v - lr * g\u001b[39m\n",
       "\u001b[36m  w .= w + (1 + gamma) * v\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  # adagrad (http://www.jmlr.org/papers/v12/duchi11a.html)\u001b[39m\n",
       "\u001b[36m  G .= G + g .^ 2\u001b[39m\n",
       "\u001b[36m  w .= w - lr * g ./ sqrt(G + eps)\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  # rmsprop (http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)\u001b[39m\n",
       "\u001b[36m  G .= rho * G + (1-rho) * g .^ 2 \u001b[39m\n",
       "\u001b[36m  w .= w - lr * g ./ sqrt(G + eps)\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  # adadelta (http://arxiv.org/abs/1212.5701)\u001b[39m\n",
       "\u001b[36m  G .= rho * G + (1-rho) * g .^ 2\u001b[39m\n",
       "\u001b[36m  update = sqrt(delta + eps) .* g ./ sqrt(G + eps)\u001b[39m\n",
       "\u001b[36m  w = w - lr * update\u001b[39m\n",
       "\u001b[36m  delta = rho * delta + (1-rho) * update .^ 2\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  # adam (http://arxiv.org/abs/1412.6980)\u001b[39m\n",
       "\u001b[36m  v = beta1 * v + (1 - beta1) * g\u001b[39m\n",
       "\u001b[36m  G = beta2 * G + (1 - beta2) * g .^ 2\u001b[39m\n",
       "\u001b[36m  vhat = v ./ (1 - beta1 ^ t)\u001b[39m\n",
       "\u001b[36m  Ghat = G ./ (1 - beta2 ^ t)\u001b[39m\n",
       "\u001b[36m  w = w - (lr / (sqrt(Ghat) + eps)) * vhat\u001b[39m"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@doc Knet.sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "rnn = RNN(inputSize, hiddenSize; opts...)\n",
       "rnn(x; batchSizes) => y\n",
       "rnn.h, rnn.c  # hidden and cell states\n",
       "\\end{verbatim}\n",
       "\\texttt{RNN} returns a callable RNN object \\texttt{rnn}. Given a minibatch of sequences \\texttt{x}, \\texttt{rnn(x)} returns \\texttt{y}, the hidden states of the final layer for each time step. \\texttt{rnn.h} and \\texttt{rnn.c} fields can be used to set the initial hidden states and read the final hidden states of all layers.  Note that the final time step of \\texttt{y} always contains the final hidden state of the last layer, equivalent to \\texttt{rnn.h} for a single layer network.\n",
       "\n",
       "\\textbf{Dimensions:} The input \\texttt{x} can be 1, 2, or 3 dimensional and \\texttt{y} will have the same number of dimensions as \\texttt{x}. size(x)=(X,[B,T]) and size(y)=(H/2H,[B,T]) where X is inputSize, B is batchSize, T is seqLength, H is hiddenSize, 2H is for bidirectional RNNs. By default a 1-D \\texttt{x} represents a single instance for a single time step, a 2-D \\texttt{x} represents a single minibatch for a single time step, and a 3-D \\texttt{x} represents a sequence of identically sized minibatches for multiple time steps. The output \\texttt{y} gives the hidden state (of the final layer for multi-layer RNNs) for each time step. The fields \\texttt{rnn.h} and \\texttt{rnn.c} represent the hidden states of all layers in a single time step and have size (H,B,L/2L) where L is numLayers and 2L is for bidirectional RNNs.\n",
       "\n",
       "\\textbf{batchSizes:} If \\texttt{batchSizes=nothing} (default), all sequences in a minibatch are assumed to be the same length. If \\texttt{batchSizes} is an array of (non-increasing) integers, it gives us the batch size for each time step (allowing different sequences in the minibatch to have different lengths). In this case \\texttt{x} will typically be 2-D with the second dimension representing variable size batches for time steps. If \\texttt{batchSizes} is used, \\texttt{sum(batchSizes)} should equal \\texttt{length(x) ÷ size(x,1)}. When the batch size is different in every time step, hidden states will have size (H,B,L/2L) where B is always the size of the first (largest) minibatch.\n",
       "\n",
       "\\textbf{Hidden states:} The hidden and cell states are kept in \\texttt{rnn.h} and \\texttt{rnn.c} fields (the cell state is only used by LSTM). They can be initialized during construction using the \\texttt{h} and \\texttt{c} keyword arguments, or modified later by direct assignment. Valid values are \\texttt{nothing} (default), \\texttt{0}, or an array of the right type and size possibly wrapped in a \\texttt{Param}. If the value is \\texttt{nothing} the initial state is assumed to be zero and the final state is discarded keeping the value \\texttt{nothing}. If the value is \\texttt{0} the initial state is assumed to be zero and \\texttt{0} is replaced by the final state on return. If the value is a valid state, it is used as the initial state and is replaced by the final state on return.\n",
       "\n",
       "In a differentiation context the returned final hidden states will be wrapped in \\texttt{Result} types. This is necessary if the same RNN object is to be called multiple times in a single iteration. Between iterations (i.e. after diff/update) the hidden states need to be unboxed with e.g. \\texttt{rnn.h = value(rnn.h)} to prevent spurious dependencies. This happens automatically during the backward pass for GPU RNNs but needs to be done manually for CPU RNNs. See the \\href{https://github.com/denizyuret/Knet.jl/blob/master/tutorial/80.charlm.ipynb}{CharLM Tutorial} for an example.\n",
       "\n",
       "\\textbf{Keyword arguments for RNN:}\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{h=nothing}: Initial hidden state.\n",
       "\n",
       "\n",
       "\\item \\texttt{c=nothing}: Initial cell state.\n",
       "\n",
       "\n",
       "\\item \\texttt{rnnType=:lstm} Type of RNN: One of :relu, :tanh, :lstm, :gru.\n",
       "\n",
       "\n",
       "\\item \\texttt{numLayers=1}: Number of RNN layers.\n",
       "\n",
       "\n",
       "\\item \\texttt{bidirectional=false}: Create a bidirectional RNN if \\texttt{true}.\n",
       "\n",
       "\n",
       "\\item \\texttt{dropout=0}: Dropout probability. Applied to input and between layers.\n",
       "\n",
       "\n",
       "\\item \\texttt{skipInput=false}: Do not multiply the input with a matrix if \\texttt{true}.\n",
       "\n",
       "\n",
       "\\item \\texttt{dataType=Float32}: Data type to use for weights.\n",
       "\n",
       "\n",
       "\\item \\texttt{algo=0}: Algorithm to use, see CUDNN docs for details.\n",
       "\n",
       "\n",
       "\\item \\texttt{seed=0}: Random number seed for dropout. Uses \\texttt{time()} if 0.\n",
       "\n",
       "\n",
       "\\item \\texttt{winit=xavier}: Weight initialization method for matrices.\n",
       "\n",
       "\n",
       "\\item \\texttt{binit=zeros}: Weight initialization method for bias vectors.\n",
       "\n",
       "\n",
       "\\item \\texttt{usegpu=(gpu()>=0)}: GPU used by default if one exists.\n",
       "\n",
       "\\end{itemize}\n",
       "\\textbf{Formulas:} RNNs compute the output h[t] for a given iteration from the recurrent input h[t-1] and the previous layer input x[t] given matrices W, R and biases bW, bR from the following equations:\n",
       "\n",
       "\\texttt{:relu} and \\texttt{:tanh}: Single gate RNN with activation function f:\n",
       "\n",
       "\\begin{verbatim}\n",
       "h[t] = f(W * x[t] .+ R * h[t-1] .+ bW .+ bR)\n",
       "\\end{verbatim}\n",
       "\\texttt{:gru}: Gated recurrent unit:\n",
       "\n",
       "\\begin{verbatim}\n",
       "i[t] = sigm(Wi * x[t] .+ Ri * h[t-1] .+ bWi .+ bRi) # input gate\n",
       "r[t] = sigm(Wr * x[t] .+ Rr * h[t-1] .+ bWr .+ bRr) # reset gate\n",
       "n[t] = tanh(Wn * x[t] .+ r[t] .* (Rn * h[t-1] .+ bRn) .+ bWn) # new gate\n",
       "h[t] = (1 - i[t]) .* n[t] .+ i[t] .* h[t-1]\n",
       "\\end{verbatim}\n",
       "\\texttt{:lstm}: Long short term memory unit with no peephole connections:\n",
       "\n",
       "\\begin{verbatim}\n",
       "i[t] = sigm(Wi * x[t] .+ Ri * h[t-1] .+ bWi .+ bRi) # input gate\n",
       "f[t] = sigm(Wf * x[t] .+ Rf * h[t-1] .+ bWf .+ bRf) # forget gate\n",
       "o[t] = sigm(Wo * x[t] .+ Ro * h[t-1] .+ bWo .+ bRo) # output gate\n",
       "n[t] = tanh(Wn * x[t] .+ Rn * h[t-1] .+ bWn .+ bRn) # new gate\n",
       "c[t] = f[t] .* c[t-1] .+ i[t] .* n[t]               # cell output\n",
       "h[t] = o[t] .* tanh(c[t])\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "```\n",
       "rnn = RNN(inputSize, hiddenSize; opts...)\n",
       "rnn(x; batchSizes) => y\n",
       "rnn.h, rnn.c  # hidden and cell states\n",
       "```\n",
       "\n",
       "`RNN` returns a callable RNN object `rnn`. Given a minibatch of sequences `x`, `rnn(x)` returns `y`, the hidden states of the final layer for each time step. `rnn.h` and `rnn.c` fields can be used to set the initial hidden states and read the final hidden states of all layers.  Note that the final time step of `y` always contains the final hidden state of the last layer, equivalent to `rnn.h` for a single layer network.\n",
       "\n",
       "**Dimensions:** The input `x` can be 1, 2, or 3 dimensional and `y` will have the same number of dimensions as `x`. size(x)=(X,[B,T]) and size(y)=(H/2H,[B,T]) where X is inputSize, B is batchSize, T is seqLength, H is hiddenSize, 2H is for bidirectional RNNs. By default a 1-D `x` represents a single instance for a single time step, a 2-D `x` represents a single minibatch for a single time step, and a 3-D `x` represents a sequence of identically sized minibatches for multiple time steps. The output `y` gives the hidden state (of the final layer for multi-layer RNNs) for each time step. The fields `rnn.h` and `rnn.c` represent the hidden states of all layers in a single time step and have size (H,B,L/2L) where L is numLayers and 2L is for bidirectional RNNs.\n",
       "\n",
       "**batchSizes:** If `batchSizes=nothing` (default), all sequences in a minibatch are assumed to be the same length. If `batchSizes` is an array of (non-increasing) integers, it gives us the batch size for each time step (allowing different sequences in the minibatch to have different lengths). In this case `x` will typically be 2-D with the second dimension representing variable size batches for time steps. If `batchSizes` is used, `sum(batchSizes)` should equal `length(x) ÷ size(x,1)`. When the batch size is different in every time step, hidden states will have size (H,B,L/2L) where B is always the size of the first (largest) minibatch.\n",
       "\n",
       "**Hidden states:** The hidden and cell states are kept in `rnn.h` and `rnn.c` fields (the cell state is only used by LSTM). They can be initialized during construction using the `h` and `c` keyword arguments, or modified later by direct assignment. Valid values are `nothing` (default), `0`, or an array of the right type and size possibly wrapped in a `Param`. If the value is `nothing` the initial state is assumed to be zero and the final state is discarded keeping the value `nothing`. If the value is `0` the initial state is assumed to be zero and `0` is replaced by the final state on return. If the value is a valid state, it is used as the initial state and is replaced by the final state on return.\n",
       "\n",
       "In a differentiation context the returned final hidden states will be wrapped in `Result` types. This is necessary if the same RNN object is to be called multiple times in a single iteration. Between iterations (i.e. after diff/update) the hidden states need to be unboxed with e.g. `rnn.h = value(rnn.h)` to prevent spurious dependencies. This happens automatically during the backward pass for GPU RNNs but needs to be done manually for CPU RNNs. See the [CharLM Tutorial](https://github.com/denizyuret/Knet.jl/blob/master/tutorial/80.charlm.ipynb) for an example.\n",
       "\n",
       "**Keyword arguments for RNN:**\n",
       "\n",
       "  * `h=nothing`: Initial hidden state.\n",
       "  * `c=nothing`: Initial cell state.\n",
       "  * `rnnType=:lstm` Type of RNN: One of :relu, :tanh, :lstm, :gru.\n",
       "  * `numLayers=1`: Number of RNN layers.\n",
       "  * `bidirectional=false`: Create a bidirectional RNN if `true`.\n",
       "  * `dropout=0`: Dropout probability. Applied to input and between layers.\n",
       "  * `skipInput=false`: Do not multiply the input with a matrix if `true`.\n",
       "  * `dataType=Float32`: Data type to use for weights.\n",
       "  * `algo=0`: Algorithm to use, see CUDNN docs for details.\n",
       "  * `seed=0`: Random number seed for dropout. Uses `time()` if 0.\n",
       "  * `winit=xavier`: Weight initialization method for matrices.\n",
       "  * `binit=zeros`: Weight initialization method for bias vectors.\n",
       "  * `usegpu=(gpu()>=0)`: GPU used by default if one exists.\n",
       "\n",
       "**Formulas:** RNNs compute the output h[t] for a given iteration from the recurrent input h[t-1] and the previous layer input x[t] given matrices W, R and biases bW, bR from the following equations:\n",
       "\n",
       "`:relu` and `:tanh`: Single gate RNN with activation function f:\n",
       "\n",
       "```\n",
       "h[t] = f(W * x[t] .+ R * h[t-1] .+ bW .+ bR)\n",
       "```\n",
       "\n",
       "`:gru`: Gated recurrent unit:\n",
       "\n",
       "```\n",
       "i[t] = sigm(Wi * x[t] .+ Ri * h[t-1] .+ bWi .+ bRi) # input gate\n",
       "r[t] = sigm(Wr * x[t] .+ Rr * h[t-1] .+ bWr .+ bRr) # reset gate\n",
       "n[t] = tanh(Wn * x[t] .+ r[t] .* (Rn * h[t-1] .+ bRn) .+ bWn) # new gate\n",
       "h[t] = (1 - i[t]) .* n[t] .+ i[t] .* h[t-1]\n",
       "```\n",
       "\n",
       "`:lstm`: Long short term memory unit with no peephole connections:\n",
       "\n",
       "```\n",
       "i[t] = sigm(Wi * x[t] .+ Ri * h[t-1] .+ bWi .+ bRi) # input gate\n",
       "f[t] = sigm(Wf * x[t] .+ Rf * h[t-1] .+ bWf .+ bRf) # forget gate\n",
       "o[t] = sigm(Wo * x[t] .+ Ro * h[t-1] .+ bWo .+ bRo) # output gate\n",
       "n[t] = tanh(Wn * x[t] .+ Rn * h[t-1] .+ bWn .+ bRn) # new gate\n",
       "c[t] = f[t] .* c[t-1] .+ i[t] .* n[t]               # cell output\n",
       "h[t] = o[t] .* tanh(c[t])\n",
       "```\n"
      ],
      "text/plain": [
       "\u001b[36m  rnn = RNN(inputSize, hiddenSize; opts...)\u001b[39m\n",
       "\u001b[36m  rnn(x; batchSizes) => y\u001b[39m\n",
       "\u001b[36m  rnn.h, rnn.c  # hidden and cell states\u001b[39m\n",
       "\n",
       "  \u001b[36mRNN\u001b[39m returns a callable RNN object \u001b[36mrnn\u001b[39m. Given a minibatch of\n",
       "  sequences \u001b[36mx\u001b[39m, \u001b[36mrnn(x)\u001b[39m returns \u001b[36my\u001b[39m, the hidden states of the final layer\n",
       "  for each time step. \u001b[36mrnn.h\u001b[39m and \u001b[36mrnn.c\u001b[39m fields can be used to set the\n",
       "  initial hidden states and read the final hidden states of all\n",
       "  layers. Note that the final time step of \u001b[36my\u001b[39m always contains the final\n",
       "  hidden state of the last layer, equivalent to \u001b[36mrnn.h\u001b[39m for a single\n",
       "  layer network.\n",
       "\n",
       "  \u001b[1mDimensions:\u001b[22m The input \u001b[36mx\u001b[39m can be 1, 2, or 3 dimensional and \u001b[36my\u001b[39m will\n",
       "  have the same number of dimensions as \u001b[36mx\u001b[39m. size(x)=(X,[B,T]) and\n",
       "  size(y)=(H/2H,[B,T]) where X is inputSize, B is batchSize, T is\n",
       "  seqLength, H is hiddenSize, 2H is for bidirectional RNNs. By default\n",
       "  a 1-D \u001b[36mx\u001b[39m represents a single instance for a single time step, a 2-D \u001b[36mx\u001b[39m\n",
       "  represents a single minibatch for a single time step, and a 3-D \u001b[36mx\u001b[39m\n",
       "  represents a sequence of identically sized minibatches for multiple\n",
       "  time steps. The output \u001b[36my\u001b[39m gives the hidden state (of the final layer\n",
       "  for multi-layer RNNs) for each time step. The fields \u001b[36mrnn.h\u001b[39m and \u001b[36mrnn.c\u001b[39m\n",
       "  represent the hidden states of all layers in a single time step and\n",
       "  have size (H,B,L/2L) where L is numLayers and 2L is for\n",
       "  bidirectional RNNs.\n",
       "\n",
       "  \u001b[1mbatchSizes:\u001b[22m If \u001b[36mbatchSizes=nothing\u001b[39m (default), all sequences in a\n",
       "  minibatch are assumed to be the same length. If \u001b[36mbatchSizes\u001b[39m is an\n",
       "  array of (non-increasing) integers, it gives us the batch size for\n",
       "  each time step (allowing different sequences in the minibatch to\n",
       "  have different lengths). In this case \u001b[36mx\u001b[39m will typically be 2-D with\n",
       "  the second dimension representing variable size batches for time\n",
       "  steps. If \u001b[36mbatchSizes\u001b[39m is used, \u001b[36msum(batchSizes)\u001b[39m should equal \u001b[36mlength(x)\n",
       "  ÷ size(x,1)\u001b[39m. When the batch size is different in every time step,\n",
       "  hidden states will have size (H,B,L/2L) where B is always the size\n",
       "  of the first (largest) minibatch.\n",
       "\n",
       "  \u001b[1mHidden states:\u001b[22m The hidden and cell states are kept in \u001b[36mrnn.h\u001b[39m and\n",
       "  \u001b[36mrnn.c\u001b[39m fields (the cell state is only used by LSTM). They can be\n",
       "  initialized during construction using the \u001b[36mh\u001b[39m and \u001b[36mc\u001b[39m keyword arguments,\n",
       "  or modified later by direct assignment. Valid values are \u001b[36mnothing\u001b[39m\n",
       "  (default), \u001b[36m0\u001b[39m, or an array of the right type and size possibly\n",
       "  wrapped in a \u001b[36mParam\u001b[39m. If the value is \u001b[36mnothing\u001b[39m the initial state is\n",
       "  assumed to be zero and the final state is discarded keeping the\n",
       "  value \u001b[36mnothing\u001b[39m. If the value is \u001b[36m0\u001b[39m the initial state is assumed to be\n",
       "  zero and \u001b[36m0\u001b[39m is replaced by the final state on return. If the value is\n",
       "  a valid state, it is used as the initial state and is replaced by\n",
       "  the final state on return.\n",
       "\n",
       "  In a differentiation context the returned final hidden states will\n",
       "  be wrapped in \u001b[36mResult\u001b[39m types. This is necessary if the same RNN object\n",
       "  is to be called multiple times in a single iteration. Between\n",
       "  iterations (i.e. after diff/update) the hidden states need to be\n",
       "  unboxed with e.g. \u001b[36mrnn.h = value(rnn.h)\u001b[39m to prevent spurious\n",
       "  dependencies. This happens automatically during the backward pass\n",
       "  for GPU RNNs but needs to be done manually for CPU RNNs. See the\n",
       "  CharLM Tutorial\n",
       "  (https://github.com/denizyuret/Knet.jl/blob/master/tutorial/80.charlm.ipynb)\n",
       "  for an example.\n",
       "\n",
       "  \u001b[1mKeyword arguments for RNN:\u001b[22m\n",
       "\n",
       "    •    \u001b[36mh=nothing\u001b[39m: Initial hidden state.\n",
       "\n",
       "    •    \u001b[36mc=nothing\u001b[39m: Initial cell state.\n",
       "\n",
       "    •    \u001b[36mrnnType=:lstm\u001b[39m Type of RNN: One of :relu, :tanh, :lstm,\n",
       "        :gru.\n",
       "\n",
       "    •    \u001b[36mnumLayers=1\u001b[39m: Number of RNN layers.\n",
       "\n",
       "    •    \u001b[36mbidirectional=false\u001b[39m: Create a bidirectional RNN if \u001b[36mtrue\u001b[39m.\n",
       "\n",
       "    •    \u001b[36mdropout=0\u001b[39m: Dropout probability. Applied to input and\n",
       "        between layers.\n",
       "\n",
       "    •    \u001b[36mskipInput=false\u001b[39m: Do not multiply the input with a matrix\n",
       "        if \u001b[36mtrue\u001b[39m.\n",
       "\n",
       "    •    \u001b[36mdataType=Float32\u001b[39m: Data type to use for weights.\n",
       "\n",
       "    •    \u001b[36malgo=0\u001b[39m: Algorithm to use, see CUDNN docs for details.\n",
       "\n",
       "    •    \u001b[36mseed=0\u001b[39m: Random number seed for dropout. Uses \u001b[36mtime()\u001b[39m if 0.\n",
       "\n",
       "    •    \u001b[36mwinit=xavier\u001b[39m: Weight initialization method for matrices.\n",
       "\n",
       "    •    \u001b[36mbinit=zeros\u001b[39m: Weight initialization method for bias\n",
       "        vectors.\n",
       "\n",
       "    •    \u001b[36musegpu=(gpu()>=0)\u001b[39m: GPU used by default if one exists.\n",
       "\n",
       "  \u001b[1mFormulas:\u001b[22m RNNs compute the output h[t] for a given iteration from\n",
       "  the recurrent input h[t-1] and the previous layer input x[t] given\n",
       "  matrices W, R and biases bW, bR from the following equations:\n",
       "\n",
       "  \u001b[36m:relu\u001b[39m and \u001b[36m:tanh\u001b[39m: Single gate RNN with activation function f:\n",
       "\n",
       "\u001b[36m  h[t] = f(W * x[t] .+ R * h[t-1] .+ bW .+ bR)\u001b[39m\n",
       "\n",
       "  \u001b[36m:gru\u001b[39m: Gated recurrent unit:\n",
       "\n",
       "\u001b[36m  i[t] = sigm(Wi * x[t] .+ Ri * h[t-1] .+ bWi .+ bRi) # input gate\u001b[39m\n",
       "\u001b[36m  r[t] = sigm(Wr * x[t] .+ Rr * h[t-1] .+ bWr .+ bRr) # reset gate\u001b[39m\n",
       "\u001b[36m  n[t] = tanh(Wn * x[t] .+ r[t] .* (Rn * h[t-1] .+ bRn) .+ bWn) # new gate\u001b[39m\n",
       "\u001b[36m  h[t] = (1 - i[t]) .* n[t] .+ i[t] .* h[t-1]\u001b[39m\n",
       "\n",
       "  \u001b[36m:lstm\u001b[39m: Long short term memory unit with no peephole connections:\n",
       "\n",
       "\u001b[36m  i[t] = sigm(Wi * x[t] .+ Ri * h[t-1] .+ bWi .+ bRi) # input gate\u001b[39m\n",
       "\u001b[36m  f[t] = sigm(Wf * x[t] .+ Rf * h[t-1] .+ bWf .+ bRf) # forget gate\u001b[39m\n",
       "\u001b[36m  o[t] = sigm(Wo * x[t] .+ Ro * h[t-1] .+ bWo .+ bRo) # output gate\u001b[39m\n",
       "\u001b[36m  n[t] = tanh(Wn * x[t] .+ Rn * h[t-1] .+ bWn .+ bRn) # new gate\u001b[39m\n",
       "\u001b[36m  c[t] = f[t] .* c[t-1] .+ i[t] .* n[t]               # cell output\u001b[39m\n",
       "\u001b[36m  h[t] = o[t] .* tanh(c[t])\u001b[39m"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@doc RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numinit (generic function with 1 method)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function numinit(dim1)\n",
    "    return 1:dim1\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: model2 not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: model2 not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[75]:1"
     ]
    }
   ],
   "source": [
    "Knet.update!(model2.layers[2].w ,model2.layers[2].w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: model2 not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: model2 not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[76]:1"
     ]
    }
   ],
   "source": [
    "model2.layers[2].w.value = reshape([100 200 300 400],1,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "param(array; atype)\n",
       "param(dims...; init, atype)\n",
       "param0(dims...; atype)\n",
       "\\end{verbatim}\n",
       "The first form returns \\texttt{Param(atype(array))} where \\texttt{atype=identity} is the default.\n",
       "\n",
       "The second form Returns a randomly initialized \\texttt{Param(atype(init(dims...)))}. By default, \\texttt{init} is \\texttt{xavier} and \\texttt{atype} is \\texttt{KnetArray\\{Float32\\}} if \\texttt{gpu() >= 0}, \\texttt{Array\\{Float32\\}} otherwise. \n",
       "\n",
       "The third form \\texttt{param0} is an alias for \\texttt{param(dims...; init=zeros)}.\n",
       "\n"
      ],
      "text/markdown": [
       "```\n",
       "param(array; atype)\n",
       "param(dims...; init, atype)\n",
       "param0(dims...; atype)\n",
       "```\n",
       "\n",
       "The first form returns `Param(atype(array))` where `atype=identity` is the default.\n",
       "\n",
       "The second form Returns a randomly initialized `Param(atype(init(dims...)))`. By default, `init` is `xavier` and `atype` is `KnetArray{Float32}` if `gpu() >= 0`, `Array{Float32}` otherwise. \n",
       "\n",
       "The third form `param0` is an alias for `param(dims...; init=zeros)`.\n"
      ],
      "text/plain": [
       "\u001b[36m  param(array; atype)\u001b[39m\n",
       "\u001b[36m  param(dims...; init, atype)\u001b[39m\n",
       "\u001b[36m  param0(dims...; atype)\u001b[39m\n",
       "\n",
       "  The first form returns \u001b[36mParam(atype(array))\u001b[39m where \u001b[36matype=identity\u001b[39m is\n",
       "  the default.\n",
       "\n",
       "  The second form Returns a randomly initialized\n",
       "  \u001b[36mParam(atype(init(dims...)))\u001b[39m. By default, \u001b[36minit\u001b[39m is \u001b[36mxavier\u001b[39m and \u001b[36matype\u001b[39m is\n",
       "  \u001b[36mKnetArray{Float32}\u001b[39m if \u001b[36mgpu() >= 0\u001b[39m, \u001b[36mArray{Float32}\u001b[39m otherwise. \n",
       "\n",
       "  The third form \u001b[36mparam0\u001b[39m is an alias for \u001b[36mparam(dims...; init=zeros)\u001b[39m."
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@doc Knet.param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: model2 not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: model2 not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[78]:1"
     ]
    }
   ],
   "source": [
    "model2.layers[2].w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: MLP not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: MLP not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[79]:3"
     ]
    }
   ],
   "source": [
    "rnn2 = RNN(1, 2; bidirectional=true, rnnType = :lstm)\n",
    "lin2 = Linear(4,1)\n",
    "mlp2 = MLP(10, 5, 2)\n",
    "model2 = Chain(rnn2, lin2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×1×1 Array{Float32,3}:\n",
       "[:, :, 1] =\n",
       " 0.08195159\n",
       " 0.22535662\n",
       " 0.06647501\n",
       " 0.2062958 "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn2(reshape([3], 1,1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0-element Array{Any,1}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = []\n",
    "push!(x, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(:w, :h, :c, :inputSize, :hiddenSize, :numLayers, :dropout, :seed, :inputMode, :direction, :mode, :algo, :dataType, :rnnDesc, :dropoutDesc, :dx, :dhx, :dcx)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fieldnames(RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: rx not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: rx not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[83]:1"
     ]
    }
   ],
   "source": [
    "size(rx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "xavier(a...)\n",
       "\\end{verbatim}\n",
       "Xavier initialization returns uniform random weights in the range \\texttt{±sqrt(2 / (fanin + fanout))}.  The \\texttt{a} arguments are passed to \\texttt{rand}.  See (\\href{http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf}{Glorot and Bengio 2010}) for a description. \\href{http://caffe.berkeleyvision.org/doxygen/classcaffe_1_1XavierFiller.html#details}{Caffe} implements this slightly differently. \\href{http://lasagne.readthedocs.org/en/latest/modules/init.html#lasagne.init.GlorotUniform}{Lasagne} calls it \\texttt{GlorotUniform}.\n",
       "\n"
      ],
      "text/markdown": [
       "```\n",
       "xavier(a...)\n",
       "```\n",
       "\n",
       "Xavier initialization returns uniform random weights in the range `±sqrt(2 / (fanin + fanout))`.  The `a` arguments are passed to `rand`.  See ([Glorot and Bengio 2010](http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf)) for a description. [Caffe](http://caffe.berkeleyvision.org/doxygen/classcaffe_1_1XavierFiller.html#details) implements this slightly differently. [Lasagne](http://lasagne.readthedocs.org/en/latest/modules/init.html#lasagne.init.GlorotUniform) calls it `GlorotUniform`.\n"
      ],
      "text/plain": [
       "\u001b[36m  xavier(a...)\u001b[39m\n",
       "\n",
       "  Xavier initialization returns uniform random weights in the range\n",
       "  \u001b[36m±sqrt(2 / (fanin + fanout))\u001b[39m. The \u001b[36ma\u001b[39m arguments are passed to \u001b[36mrand\u001b[39m. See\n",
       "  (Glorot and Bengio 2010\n",
       "  (http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf)) for\n",
       "  a description. Caffe\n",
       "  (http://caffe.berkeleyvision.org/doxygen/classcaffe_1_1XavierFiller.html#details)\n",
       "  implements this slightly differently. Lasagne\n",
       "  (http://lasagne.readthedocs.org/en/latest/modules/init.html#lasagne.init.GlorotUniform)\n",
       "  calls it \u001b[36mGlorotUniform\u001b[39m."
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@doc Knet.xavier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 80)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size(Knet.params(rnn2)[1].value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×1×80 Array{Float64,3}:\n",
       "[:, :, 1] =\n",
       " -0.5938544985116351\n",
       "\n",
       "[:, :, 2] =\n",
       " 0.6291218518797346\n",
       "\n",
       "[:, :, 3] =\n",
       " -0.7178674044902428\n",
       "\n",
       "...\n",
       "\n",
       "[:, :, 78] =\n",
       " 0.06665347752109804\n",
       "\n",
       "[:, :, 79] =\n",
       " 0.018619557828455063\n",
       "\n",
       "[:, :, 80] =\n",
       " 0.100441816190206"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Knet.params(rnn2)[1].value = Knet.params(rnn2)[1].value .+ Knet.xavier(size(Knet.params(rnn2)[1].value)...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×3×2 Array{Int64,3}:\n",
       "[:, :, 1] =\n",
       " 1  2  3\n",
       "\n",
       "[:, :, 2] =\n",
       " 4  5  6"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rx = cat([1 2 3],[4 5 6],dims=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×3×2 Array{Int64,3}:\n",
       "[:, :, 1] =\n",
       " 1  2  3\n",
       "\n",
       "[:, :, 2] =\n",
       " 4  5  0"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rx1 = cat([1 2 3], [4 5 0], dims=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×2×2 Array{Int64,3}:\n",
       "[:, :, 1] =\n",
       " 1  2\n",
       "\n",
       "[:, :, 2] =\n",
       " 4  5"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rx2 = cat([1 2], [4 5], dims=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×3×2 Array{Float32,3}:\n",
       "[:, :, 1] =\n",
       " 0.104257   0.0880124  0.057699 \n",
       " 0.0258949  0.0779188  0.14792  \n",
       " 0.193138   0.143937   0.0555855\n",
       " 0.24144    0.377849   0.372883 \n",
       "\n",
       "[:, :, 2] =\n",
       " 0.0416581  0.0233458  0.0569415  \n",
       " 0.219739   0.307609   0.0497138  \n",
       " 0.0368182  0.0218744  0.000652089\n",
       " 0.476165   0.558708   0.0196274  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn2(rx1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×3×2 Array{Int64,3}:\n",
       "[:, :, 1] =\n",
       " 1  2  3\n",
       "\n",
       "[:, :, 2] =\n",
       " 4  5  6"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rx = cat([1 2 3],[4 5 6],dims=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×1×1 Array{Float32,3}:\n",
       "[:, :, 1] =\n",
       " 0.05769895\n",
       " 0.14792015\n",
       " 0.05549529\n",
       " 0.36407632"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn2(reshape([3],1,1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×1×2 Array{Float32,3}:\n",
       "[:, :, 1] =\n",
       " 0.05769895 \n",
       " 0.14792015 \n",
       " 0.055585492\n",
       " 0.3728827  \n",
       "\n",
       "[:, :, 2] =\n",
       " 0.056941517 \n",
       " 0.049713776 \n",
       " 0.0006520893\n",
       " 0.019627368 "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn2(reshape([3 0],1,1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×3×2 Array{Float32,3}:\n",
       "[:, :, 1] =\n",
       " 0.104257   0.0880124  0.057699 \n",
       " 0.0258949  0.0779188  0.14792  \n",
       " 0.193138   0.143937   0.0877288\n",
       " 0.24144    0.377849   0.495215 \n",
       "\n",
       "[:, :, 2] =\n",
       " 0.0416581  0.0233458  0.0126705\n",
       " 0.219739   0.307609   0.391991 \n",
       " 0.0368182  0.0218744  0.0122218\n",
       " 0.476165   0.558708   0.616834 "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn2(rx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ErrorException",
     "evalue": "Implementation of batchSizes is not completed in CPU",
     "output_type": "error",
     "traceback": [
      "Implementation of batchSizes is not completed in CPU",
      "",
      "Stacktrace:",
      " [1] error(::String) at .\\error.jl:33",
      " [2] #rnntest_bs#648(::Bool, ::Bool, ::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}, ::Function, ::Array{Int64,2}, ::RNN{Array{Float32,3}}, ::Param{Array{Float32,3}}, ::Array{Int64,3}, ::Nothing, ::Nothing) at C:\\Users\\dfhdhsd\\.julia\\packages\\Knet\\05UDD\\src\\rnn.jl:908",
      " [3] (::getfield(Knet, Symbol(\"#kw##rnntest_bs\")))(::NamedTuple{(:hy, :cy),Tuple{Bool,Bool}}, ::typeof(Knet.rnntest_bs), ::Array{Int64,2}, ::RNN{Array{Float32,3}}, ::Param{Array{Float32,3}}, ::Array{Int64,3}, ::Nothing, ::Nothing) at .\\none:0",
      " [4] #rnntest#636(::Array{Int64,2}, ::Bool, ::Bool, ::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}, ::Function, ::RNN{Array{Float32,3}}, ::Param{Array{Float32,3}}, ::Array{Int64,3}, ::Nothing, ::Nothing) at C:\\Users\\dfhdhsd\\.julia\\packages\\Knet\\05UDD\\src\\rnn.jl:722",
      " [5] (::getfield(Knet, Symbol(\"#kw##rnntest\")))(::NamedTuple{(:hy, :cy, :batchSizes),Tuple{Bool,Bool,Array{Int64,2}}}, ::typeof(Knet.rnntest), ::RNN{Array{Float32,3}}, ::Param{Array{Float32,3}}, ::Array{Int64,3}, ::Nothing, ::Nothing) at .\\none:0",
      " [6] #rnnforw#635(::Base.Iterators.Pairs{Symbol,Any,Tuple{Symbol,Symbol,Symbol},NamedTuple{(:hy, :cy, :batchSizes),Tuple{Bool,Bool,Array{Int64,2}}}}, ::Function, ::RNN{Array{Float32,3}}, ::Param{Array{Float32,3}}, ::Array{Int64,3}, ::Vararg{Any,N} where N) at C:\\Users\\dfhdhsd\\.julia\\packages\\Knet\\05UDD\\src\\rnn.jl:712",
      " [7] (::getfield(Knet, Symbol(\"#kw##rnnforw\")))(::NamedTuple{(:hy, :cy, :batchSizes),Tuple{Bool,Bool,Array{Int64,2}}}, ::typeof(Knet.rnnforw), ::RNN{Array{Float32,3}}, ::Param{Array{Float32,3}}, ::Array{Int64,3}, ::Nothing, ::Nothing) at .\\none:0",
      " [8] #call#571(::Array{Int64,2}, ::RNN{Array{Float32,3}}, ::Array{Int64,3}) at C:\\Users\\dfhdhsd\\.julia\\packages\\Knet\\05UDD\\src\\rnn.jl:192",
      " [9] (::getfield(Knet, Symbol(\"#kw#RNN\")))(::NamedTuple{(:batchSizes,),Tuple{Array{Int64,2}}}, ::RNN{Array{Float32,3}}, ::Array{Int64,3}) at .\\none:0",
      " [10] top-level scope at In[95]:1"
     ]
    }
   ],
   "source": [
    "rnn2(rx, batchSizes = [3 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "param(array; atype)\n",
       "param(dims...; init, atype)\n",
       "param0(dims...; atype)\n",
       "\\end{verbatim}\n",
       "The first form returns \\texttt{Param(atype(array))} where \\texttt{atype=identity} is the default.\n",
       "\n",
       "The second form Returns a randomly initialized \\texttt{Param(atype(init(dims...)))}. By default, \\texttt{init} is \\texttt{xavier} and \\texttt{atype} is \\texttt{KnetArray\\{Float32\\}} if \\texttt{gpu() >= 0}, \\texttt{Array\\{Float32\\}} otherwise. \n",
       "\n",
       "The third form \\texttt{param0} is an alias for \\texttt{param(dims...; init=zeros)}.\n",
       "\n"
      ],
      "text/markdown": [
       "```\n",
       "param(array; atype)\n",
       "param(dims...; init, atype)\n",
       "param0(dims...; atype)\n",
       "```\n",
       "\n",
       "The first form returns `Param(atype(array))` where `atype=identity` is the default.\n",
       "\n",
       "The second form Returns a randomly initialized `Param(atype(init(dims...)))`. By default, `init` is `xavier` and `atype` is `KnetArray{Float32}` if `gpu() >= 0`, `Array{Float32}` otherwise. \n",
       "\n",
       "The third form `param0` is an alias for `param(dims...; init=zeros)`.\n"
      ],
      "text/plain": [
       "\u001b[36m  param(array; atype)\u001b[39m\n",
       "\u001b[36m  param(dims...; init, atype)\u001b[39m\n",
       "\u001b[36m  param0(dims...; atype)\u001b[39m\n",
       "\n",
       "  The first form returns \u001b[36mParam(atype(array))\u001b[39m where \u001b[36matype=identity\u001b[39m is\n",
       "  the default.\n",
       "\n",
       "  The second form Returns a randomly initialized\n",
       "  \u001b[36mParam(atype(init(dims...)))\u001b[39m. By default, \u001b[36minit\u001b[39m is \u001b[36mxavier\u001b[39m and \u001b[36matype\u001b[39m is\n",
       "  \u001b[36mKnetArray{Float32}\u001b[39m if \u001b[36mgpu() >= 0\u001b[39m, \u001b[36mArray{Float32}\u001b[39m otherwise. \n",
       "\n",
       "  The third form \u001b[36mparam0\u001b[39m is an alias for \u001b[36mparam(dims...; init=zeros)\u001b[39m."
      ]
     },
     "execution_count": 623,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@doc Knet.param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37-element Array{Float32,1}:\n",
       " -0.5938545  \n",
       " -0.71786743 \n",
       " -0.8065058  \n",
       " -0.2653079  \n",
       " -0.28603992 \n",
       " -0.5680328  \n",
       " -0.39744723 \n",
       " -0.10050218 \n",
       " -0.19192412 \n",
       " -0.3145375  \n",
       " -0.012506866\n",
       " -0.28059787 \n",
       " -0.6763631  \n",
       "  ⋮          \n",
       " -0.06324435 \n",
       " -0.11797717 \n",
       " -0.14855406 \n",
       " -0.13586521 \n",
       " -0.035933476\n",
       " -0.07106355 \n",
       " -0.13269262 \n",
       " -0.012461339\n",
       " -0.12050926 \n",
       " -0.010981156\n",
       " -0.11861693 \n",
       " -0.06984177 "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in rnn2.w if x<0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×3×2 Array{Float32,3}:\n",
       "[:, :, 1] =\n",
       " 0.104257   0.0880124  0.057699 \n",
       " 0.0258949  0.0779188  0.14792  \n",
       " 0.193138   0.143937   0.0877288\n",
       " 0.24144    0.377849   0.495215 \n",
       "\n",
       "[:, :, 2] =\n",
       " 0.0416581  0.0233458  0.0126705\n",
       " 0.219739   0.307609   0.391991 \n",
       " 0.0368182  0.0218744  0.0122218\n",
       " 0.476165   0.558708   0.616834 "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn2(rx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "rnn = RNN(inputSize, hiddenSize; opts...)\n",
       "rnn(x; batchSizes) => y\n",
       "rnn.h, rnn.c  # hidden and cell states\n",
       "\\end{verbatim}\n",
       "\\texttt{RNN} returns a callable RNN object \\texttt{rnn}. Given a minibatch of sequences \\texttt{x}, \\texttt{rnn(x)} returns \\texttt{y}, the hidden states of the final layer for each time step. \\texttt{rnn.h} and \\texttt{rnn.c} fields can be used to set the initial hidden states and read the final hidden states of all layers.  Note that the final time step of \\texttt{y} always contains the final hidden state of the last layer, equivalent to \\texttt{rnn.h} for a single layer network.\n",
       "\n",
       "\\textbf{Dimensions:} The input \\texttt{x} can be 1, 2, or 3 dimensional and \\texttt{y} will have the same number of dimensions as \\texttt{x}. size(x)=(X,[B,T]) and size(y)=(H/2H,[B,T]) where X is inputSize, B is batchSize, T is seqLength, H is hiddenSize, 2H is for bidirectional RNNs. By default a 1-D \\texttt{x} represents a single instance for a single time step, a 2-D \\texttt{x} represents a single minibatch for a single time step, and a 3-D \\texttt{x} represents a sequence of identically sized minibatches for multiple time steps. The output \\texttt{y} gives the hidden state (of the final layer for multi-layer RNNs) for each time step. The fields \\texttt{rnn.h} and \\texttt{rnn.c} represent the hidden states of all layers in a single time step and have size (H,B,L/2L) where L is numLayers and 2L is for bidirectional RNNs.\n",
       "\n",
       "\\textbf{batchSizes:} If \\texttt{batchSizes=nothing} (default), all sequences in a minibatch are assumed to be the same length. If \\texttt{batchSizes} is an array of (non-increasing) integers, it gives us the batch size for each time step (allowing different sequences in the minibatch to have different lengths). In this case \\texttt{x} will typically be 2-D with the second dimension representing variable size batches for time steps. If \\texttt{batchSizes} is used, \\texttt{sum(batchSizes)} should equal \\texttt{length(x) ÷ size(x,1)}. When the batch size is different in every time step, hidden states will have size (H,B,L/2L) where B is always the size of the first (largest) minibatch.\n",
       "\n",
       "\\textbf{Hidden states:} The hidden and cell states are kept in \\texttt{rnn.h} and \\texttt{rnn.c} fields (the cell state is only used by LSTM). They can be initialized during construction using the \\texttt{h} and \\texttt{c} keyword arguments, or modified later by direct assignment. Valid values are \\texttt{nothing} (default), \\texttt{0}, or an array of the right type and size possibly wrapped in a \\texttt{Param}. If the value is \\texttt{nothing} the initial state is assumed to be zero and the final state is discarded keeping the value \\texttt{nothing}. If the value is \\texttt{0} the initial state is assumed to be zero and \\texttt{0} is replaced by the final state on return. If the value is a valid state, it is used as the initial state and is replaced by the final state on return.\n",
       "\n",
       "In a differentiation context the returned final hidden states will be wrapped in \\texttt{Result} types. This is necessary if the same RNN object is to be called multiple times in a single iteration. Between iterations (i.e. after diff/update) the hidden states need to be unboxed with e.g. \\texttt{rnn.h = value(rnn.h)} to prevent spurious dependencies. This happens automatically during the backward pass for GPU RNNs but needs to be done manually for CPU RNNs. See the \\href{https://github.com/denizyuret/Knet.jl/blob/master/tutorial/80.charlm.ipynb}{CharLM Tutorial} for an example.\n",
       "\n",
       "\\textbf{Keyword arguments for RNN:}\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{h=nothing}: Initial hidden state.\n",
       "\n",
       "\n",
       "\\item \\texttt{c=nothing}: Initial cell state.\n",
       "\n",
       "\n",
       "\\item \\texttt{rnnType=:lstm} Type of RNN: One of :relu, :tanh, :lstm, :gru.\n",
       "\n",
       "\n",
       "\\item \\texttt{numLayers=1}: Number of RNN layers.\n",
       "\n",
       "\n",
       "\\item \\texttt{bidirectional=false}: Create a bidirectional RNN if \\texttt{true}.\n",
       "\n",
       "\n",
       "\\item \\texttt{dropout=0}: Dropout probability. Applied to input and between layers.\n",
       "\n",
       "\n",
       "\\item \\texttt{skipInput=false}: Do not multiply the input with a matrix if \\texttt{true}.\n",
       "\n",
       "\n",
       "\\item \\texttt{dataType=Float32}: Data type to use for weights.\n",
       "\n",
       "\n",
       "\\item \\texttt{algo=0}: Algorithm to use, see CUDNN docs for details.\n",
       "\n",
       "\n",
       "\\item \\texttt{seed=0}: Random number seed for dropout. Uses \\texttt{time()} if 0.\n",
       "\n",
       "\n",
       "\\item \\texttt{winit=xavier}: Weight initialization method for matrices.\n",
       "\n",
       "\n",
       "\\item \\texttt{binit=zeros}: Weight initialization method for bias vectors.\n",
       "\n",
       "\n",
       "\\item \\texttt{usegpu=(gpu()>=0)}: GPU used by default if one exists.\n",
       "\n",
       "\\end{itemize}\n",
       "\\textbf{Formulas:} RNNs compute the output h[t] for a given iteration from the recurrent input h[t-1] and the previous layer input x[t] given matrices W, R and biases bW, bR from the following equations:\n",
       "\n",
       "\\texttt{:relu} and \\texttt{:tanh}: Single gate RNN with activation function f:\n",
       "\n",
       "\\begin{verbatim}\n",
       "h[t] = f(W * x[t] .+ R * h[t-1] .+ bW .+ bR)\n",
       "\\end{verbatim}\n",
       "\\texttt{:gru}: Gated recurrent unit:\n",
       "\n",
       "\\begin{verbatim}\n",
       "i[t] = sigm(Wi * x[t] .+ Ri * h[t-1] .+ bWi .+ bRi) # input gate\n",
       "r[t] = sigm(Wr * x[t] .+ Rr * h[t-1] .+ bWr .+ bRr) # reset gate\n",
       "n[t] = tanh(Wn * x[t] .+ r[t] .* (Rn * h[t-1] .+ bRn) .+ bWn) # new gate\n",
       "h[t] = (1 - i[t]) .* n[t] .+ i[t] .* h[t-1]\n",
       "\\end{verbatim}\n",
       "\\texttt{:lstm}: Long short term memory unit with no peephole connections:\n",
       "\n",
       "\\begin{verbatim}\n",
       "i[t] = sigm(Wi * x[t] .+ Ri * h[t-1] .+ bWi .+ bRi) # input gate\n",
       "f[t] = sigm(Wf * x[t] .+ Rf * h[t-1] .+ bWf .+ bRf) # forget gate\n",
       "o[t] = sigm(Wo * x[t] .+ Ro * h[t-1] .+ bWo .+ bRo) # output gate\n",
       "n[t] = tanh(Wn * x[t] .+ Rn * h[t-1] .+ bWn .+ bRn) # new gate\n",
       "c[t] = f[t] .* c[t-1] .+ i[t] .* n[t]               # cell output\n",
       "h[t] = o[t] .* tanh(c[t])\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "```\n",
       "rnn = RNN(inputSize, hiddenSize; opts...)\n",
       "rnn(x; batchSizes) => y\n",
       "rnn.h, rnn.c  # hidden and cell states\n",
       "```\n",
       "\n",
       "`RNN` returns a callable RNN object `rnn`. Given a minibatch of sequences `x`, `rnn(x)` returns `y`, the hidden states of the final layer for each time step. `rnn.h` and `rnn.c` fields can be used to set the initial hidden states and read the final hidden states of all layers.  Note that the final time step of `y` always contains the final hidden state of the last layer, equivalent to `rnn.h` for a single layer network.\n",
       "\n",
       "**Dimensions:** The input `x` can be 1, 2, or 3 dimensional and `y` will have the same number of dimensions as `x`. size(x)=(X,[B,T]) and size(y)=(H/2H,[B,T]) where X is inputSize, B is batchSize, T is seqLength, H is hiddenSize, 2H is for bidirectional RNNs. By default a 1-D `x` represents a single instance for a single time step, a 2-D `x` represents a single minibatch for a single time step, and a 3-D `x` represents a sequence of identically sized minibatches for multiple time steps. The output `y` gives the hidden state (of the final layer for multi-layer RNNs) for each time step. The fields `rnn.h` and `rnn.c` represent the hidden states of all layers in a single time step and have size (H,B,L/2L) where L is numLayers and 2L is for bidirectional RNNs.\n",
       "\n",
       "**batchSizes:** If `batchSizes=nothing` (default), all sequences in a minibatch are assumed to be the same length. If `batchSizes` is an array of (non-increasing) integers, it gives us the batch size for each time step (allowing different sequences in the minibatch to have different lengths). In this case `x` will typically be 2-D with the second dimension representing variable size batches for time steps. If `batchSizes` is used, `sum(batchSizes)` should equal `length(x) ÷ size(x,1)`. When the batch size is different in every time step, hidden states will have size (H,B,L/2L) where B is always the size of the first (largest) minibatch.\n",
       "\n",
       "**Hidden states:** The hidden and cell states are kept in `rnn.h` and `rnn.c` fields (the cell state is only used by LSTM). They can be initialized during construction using the `h` and `c` keyword arguments, or modified later by direct assignment. Valid values are `nothing` (default), `0`, or an array of the right type and size possibly wrapped in a `Param`. If the value is `nothing` the initial state is assumed to be zero and the final state is discarded keeping the value `nothing`. If the value is `0` the initial state is assumed to be zero and `0` is replaced by the final state on return. If the value is a valid state, it is used as the initial state and is replaced by the final state on return.\n",
       "\n",
       "In a differentiation context the returned final hidden states will be wrapped in `Result` types. This is necessary if the same RNN object is to be called multiple times in a single iteration. Between iterations (i.e. after diff/update) the hidden states need to be unboxed with e.g. `rnn.h = value(rnn.h)` to prevent spurious dependencies. This happens automatically during the backward pass for GPU RNNs but needs to be done manually for CPU RNNs. See the [CharLM Tutorial](https://github.com/denizyuret/Knet.jl/blob/master/tutorial/80.charlm.ipynb) for an example.\n",
       "\n",
       "**Keyword arguments for RNN:**\n",
       "\n",
       "  * `h=nothing`: Initial hidden state.\n",
       "  * `c=nothing`: Initial cell state.\n",
       "  * `rnnType=:lstm` Type of RNN: One of :relu, :tanh, :lstm, :gru.\n",
       "  * `numLayers=1`: Number of RNN layers.\n",
       "  * `bidirectional=false`: Create a bidirectional RNN if `true`.\n",
       "  * `dropout=0`: Dropout probability. Applied to input and between layers.\n",
       "  * `skipInput=false`: Do not multiply the input with a matrix if `true`.\n",
       "  * `dataType=Float32`: Data type to use for weights.\n",
       "  * `algo=0`: Algorithm to use, see CUDNN docs for details.\n",
       "  * `seed=0`: Random number seed for dropout. Uses `time()` if 0.\n",
       "  * `winit=xavier`: Weight initialization method for matrices.\n",
       "  * `binit=zeros`: Weight initialization method for bias vectors.\n",
       "  * `usegpu=(gpu()>=0)`: GPU used by default if one exists.\n",
       "\n",
       "**Formulas:** RNNs compute the output h[t] for a given iteration from the recurrent input h[t-1] and the previous layer input x[t] given matrices W, R and biases bW, bR from the following equations:\n",
       "\n",
       "`:relu` and `:tanh`: Single gate RNN with activation function f:\n",
       "\n",
       "```\n",
       "h[t] = f(W * x[t] .+ R * h[t-1] .+ bW .+ bR)\n",
       "```\n",
       "\n",
       "`:gru`: Gated recurrent unit:\n",
       "\n",
       "```\n",
       "i[t] = sigm(Wi * x[t] .+ Ri * h[t-1] .+ bWi .+ bRi) # input gate\n",
       "r[t] = sigm(Wr * x[t] .+ Rr * h[t-1] .+ bWr .+ bRr) # reset gate\n",
       "n[t] = tanh(Wn * x[t] .+ r[t] .* (Rn * h[t-1] .+ bRn) .+ bWn) # new gate\n",
       "h[t] = (1 - i[t]) .* n[t] .+ i[t] .* h[t-1]\n",
       "```\n",
       "\n",
       "`:lstm`: Long short term memory unit with no peephole connections:\n",
       "\n",
       "```\n",
       "i[t] = sigm(Wi * x[t] .+ Ri * h[t-1] .+ bWi .+ bRi) # input gate\n",
       "f[t] = sigm(Wf * x[t] .+ Rf * h[t-1] .+ bWf .+ bRf) # forget gate\n",
       "o[t] = sigm(Wo * x[t] .+ Ro * h[t-1] .+ bWo .+ bRo) # output gate\n",
       "n[t] = tanh(Wn * x[t] .+ Rn * h[t-1] .+ bWn .+ bRn) # new gate\n",
       "c[t] = f[t] .* c[t-1] .+ i[t] .* n[t]               # cell output\n",
       "h[t] = o[t] .* tanh(c[t])\n",
       "```\n"
      ],
      "text/plain": [
       "\u001b[36m  rnn = RNN(inputSize, hiddenSize; opts...)\u001b[39m\n",
       "\u001b[36m  rnn(x; batchSizes) => y\u001b[39m\n",
       "\u001b[36m  rnn.h, rnn.c  # hidden and cell states\u001b[39m\n",
       "\n",
       "  \u001b[36mRNN\u001b[39m returns a callable RNN object \u001b[36mrnn\u001b[39m. Given a minibatch of\n",
       "  sequences \u001b[36mx\u001b[39m, \u001b[36mrnn(x)\u001b[39m returns \u001b[36my\u001b[39m, the hidden states of the final layer\n",
       "  for each time step. \u001b[36mrnn.h\u001b[39m and \u001b[36mrnn.c\u001b[39m fields can be used to set the\n",
       "  initial hidden states and read the final hidden states of all\n",
       "  layers. Note that the final time step of \u001b[36my\u001b[39m always contains the final\n",
       "  hidden state of the last layer, equivalent to \u001b[36mrnn.h\u001b[39m for a single\n",
       "  layer network.\n",
       "\n",
       "  \u001b[1mDimensions:\u001b[22m The input \u001b[36mx\u001b[39m can be 1, 2, or 3 dimensional and \u001b[36my\u001b[39m will\n",
       "  have the same number of dimensions as \u001b[36mx\u001b[39m. size(x)=(X,[B,T]) and\n",
       "  size(y)=(H/2H,[B,T]) where X is inputSize, B is batchSize, T is\n",
       "  seqLength, H is hiddenSize, 2H is for bidirectional RNNs. By default\n",
       "  a 1-D \u001b[36mx\u001b[39m represents a single instance for a single time step, a 2-D \u001b[36mx\u001b[39m\n",
       "  represents a single minibatch for a single time step, and a 3-D \u001b[36mx\u001b[39m\n",
       "  represents a sequence of identically sized minibatches for multiple\n",
       "  time steps. The output \u001b[36my\u001b[39m gives the hidden state (of the final layer\n",
       "  for multi-layer RNNs) for each time step. The fields \u001b[36mrnn.h\u001b[39m and \u001b[36mrnn.c\u001b[39m\n",
       "  represent the hidden states of all layers in a single time step and\n",
       "  have size (H,B,L/2L) where L is numLayers and 2L is for\n",
       "  bidirectional RNNs.\n",
       "\n",
       "  \u001b[1mbatchSizes:\u001b[22m If \u001b[36mbatchSizes=nothing\u001b[39m (default), all sequences in a\n",
       "  minibatch are assumed to be the same length. If \u001b[36mbatchSizes\u001b[39m is an\n",
       "  array of (non-increasing) integers, it gives us the batch size for\n",
       "  each time step (allowing different sequences in the minibatch to\n",
       "  have different lengths). In this case \u001b[36mx\u001b[39m will typically be 2-D with\n",
       "  the second dimension representing variable size batches for time\n",
       "  steps. If \u001b[36mbatchSizes\u001b[39m is used, \u001b[36msum(batchSizes)\u001b[39m should equal \u001b[36mlength(x)\n",
       "  ÷ size(x,1)\u001b[39m. When the batch size is different in every time step,\n",
       "  hidden states will have size (H,B,L/2L) where B is always the size\n",
       "  of the first (largest) minibatch.\n",
       "\n",
       "  \u001b[1mHidden states:\u001b[22m The hidden and cell states are kept in \u001b[36mrnn.h\u001b[39m and\n",
       "  \u001b[36mrnn.c\u001b[39m fields (the cell state is only used by LSTM). They can be\n",
       "  initialized during construction using the \u001b[36mh\u001b[39m and \u001b[36mc\u001b[39m keyword arguments,\n",
       "  or modified later by direct assignment. Valid values are \u001b[36mnothing\u001b[39m\n",
       "  (default), \u001b[36m0\u001b[39m, or an array of the right type and size possibly\n",
       "  wrapped in a \u001b[36mParam\u001b[39m. If the value is \u001b[36mnothing\u001b[39m the initial state is\n",
       "  assumed to be zero and the final state is discarded keeping the\n",
       "  value \u001b[36mnothing\u001b[39m. If the value is \u001b[36m0\u001b[39m the initial state is assumed to be\n",
       "  zero and \u001b[36m0\u001b[39m is replaced by the final state on return. If the value is\n",
       "  a valid state, it is used as the initial state and is replaced by\n",
       "  the final state on return.\n",
       "\n",
       "  In a differentiation context the returned final hidden states will\n",
       "  be wrapped in \u001b[36mResult\u001b[39m types. This is necessary if the same RNN object\n",
       "  is to be called multiple times in a single iteration. Between\n",
       "  iterations (i.e. after diff/update) the hidden states need to be\n",
       "  unboxed with e.g. \u001b[36mrnn.h = value(rnn.h)\u001b[39m to prevent spurious\n",
       "  dependencies. This happens automatically during the backward pass\n",
       "  for GPU RNNs but needs to be done manually for CPU RNNs. See the\n",
       "  CharLM Tutorial\n",
       "  (https://github.com/denizyuret/Knet.jl/blob/master/tutorial/80.charlm.ipynb)\n",
       "  for an example.\n",
       "\n",
       "  \u001b[1mKeyword arguments for RNN:\u001b[22m\n",
       "\n",
       "    •    \u001b[36mh=nothing\u001b[39m: Initial hidden state.\n",
       "\n",
       "    •    \u001b[36mc=nothing\u001b[39m: Initial cell state.\n",
       "\n",
       "    •    \u001b[36mrnnType=:lstm\u001b[39m Type of RNN: One of :relu, :tanh, :lstm,\n",
       "        :gru.\n",
       "\n",
       "    •    \u001b[36mnumLayers=1\u001b[39m: Number of RNN layers.\n",
       "\n",
       "    •    \u001b[36mbidirectional=false\u001b[39m: Create a bidirectional RNN if \u001b[36mtrue\u001b[39m.\n",
       "\n",
       "    •    \u001b[36mdropout=0\u001b[39m: Dropout probability. Applied to input and\n",
       "        between layers.\n",
       "\n",
       "    •    \u001b[36mskipInput=false\u001b[39m: Do not multiply the input with a matrix\n",
       "        if \u001b[36mtrue\u001b[39m.\n",
       "\n",
       "    •    \u001b[36mdataType=Float32\u001b[39m: Data type to use for weights.\n",
       "\n",
       "    •    \u001b[36malgo=0\u001b[39m: Algorithm to use, see CUDNN docs for details.\n",
       "\n",
       "    •    \u001b[36mseed=0\u001b[39m: Random number seed for dropout. Uses \u001b[36mtime()\u001b[39m if 0.\n",
       "\n",
       "    •    \u001b[36mwinit=xavier\u001b[39m: Weight initialization method for matrices.\n",
       "\n",
       "    •    \u001b[36mbinit=zeros\u001b[39m: Weight initialization method for bias\n",
       "        vectors.\n",
       "\n",
       "    •    \u001b[36musegpu=(gpu()>=0)\u001b[39m: GPU used by default if one exists.\n",
       "\n",
       "  \u001b[1mFormulas:\u001b[22m RNNs compute the output h[t] for a given iteration from\n",
       "  the recurrent input h[t-1] and the previous layer input x[t] given\n",
       "  matrices W, R and biases bW, bR from the following equations:\n",
       "\n",
       "  \u001b[36m:relu\u001b[39m and \u001b[36m:tanh\u001b[39m: Single gate RNN with activation function f:\n",
       "\n",
       "\u001b[36m  h[t] = f(W * x[t] .+ R * h[t-1] .+ bW .+ bR)\u001b[39m\n",
       "\n",
       "  \u001b[36m:gru\u001b[39m: Gated recurrent unit:\n",
       "\n",
       "\u001b[36m  i[t] = sigm(Wi * x[t] .+ Ri * h[t-1] .+ bWi .+ bRi) # input gate\u001b[39m\n",
       "\u001b[36m  r[t] = sigm(Wr * x[t] .+ Rr * h[t-1] .+ bWr .+ bRr) # reset gate\u001b[39m\n",
       "\u001b[36m  n[t] = tanh(Wn * x[t] .+ r[t] .* (Rn * h[t-1] .+ bRn) .+ bWn) # new gate\u001b[39m\n",
       "\u001b[36m  h[t] = (1 - i[t]) .* n[t] .+ i[t] .* h[t-1]\u001b[39m\n",
       "\n",
       "  \u001b[36m:lstm\u001b[39m: Long short term memory unit with no peephole connections:\n",
       "\n",
       "\u001b[36m  i[t] = sigm(Wi * x[t] .+ Ri * h[t-1] .+ bWi .+ bRi) # input gate\u001b[39m\n",
       "\u001b[36m  f[t] = sigm(Wf * x[t] .+ Rf * h[t-1] .+ bWf .+ bRf) # forget gate\u001b[39m\n",
       "\u001b[36m  o[t] = sigm(Wo * x[t] .+ Ro * h[t-1] .+ bWo .+ bRo) # output gate\u001b[39m\n",
       "\u001b[36m  n[t] = tanh(Wn * x[t] .+ Rn * h[t-1] .+ bWn .+ bRn) # new gate\u001b[39m\n",
       "\u001b[36m  c[t] = f[t] .* c[t-1] .+ i[t] .* n[t]               # cell output\u001b[39m\n",
       "\u001b[36m  h[t] = o[t] .* tanh(c[t])\u001b[39m"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@doc RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "axpy!(a, X, Y)\n",
       "\\end{verbatim}\n",
       "Overwrite \\texttt{Y} with \\texttt{a*X + Y}, where \\texttt{a} is a scalar. Return \\texttt{Y}.\n",
       "\n",
       "\\section{Examples}\n",
       "\\begin{verbatim}\n",
       "julia> x = [1; 2; 3];\n",
       "\n",
       "julia> y = [4; 5; 6];\n",
       "\n",
       "julia> BLAS.axpy!(2, x, y)\n",
       "3-element Array{Int64,1}:\n",
       "  6\n",
       "  9\n",
       " 12\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "```\n",
       "axpy!(a, X, Y)\n",
       "```\n",
       "\n",
       "Overwrite `Y` with `a*X + Y`, where `a` is a scalar. Return `Y`.\n",
       "\n",
       "# Examples\n",
       "\n",
       "```jldoctest\n",
       "julia> x = [1; 2; 3];\n",
       "\n",
       "julia> y = [4; 5; 6];\n",
       "\n",
       "julia> BLAS.axpy!(2, x, y)\n",
       "3-element Array{Int64,1}:\n",
       "  6\n",
       "  9\n",
       " 12\n",
       "```\n"
      ],
      "text/plain": [
       "\u001b[36m  axpy!(a, X, Y)\u001b[39m\n",
       "\n",
       "  Overwrite \u001b[36mY\u001b[39m with \u001b[36ma*X + Y\u001b[39m, where \u001b[36ma\u001b[39m is a scalar. Return \u001b[36mY\u001b[39m.\n",
       "\n",
       "\u001b[1m  Examples\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "\u001b[36m  julia> x = [1; 2; 3];\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> y = [4; 5; 6];\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> BLAS.axpy!(2, x, y)\u001b[39m\n",
       "\u001b[36m  3-element Array{Int64,1}:\u001b[39m\n",
       "\u001b[36m    6\u001b[39m\n",
       "\u001b[36m    9\u001b[39m\n",
       "\u001b[36m   12\u001b[39m"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@doc LinearAlgebra.axpy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: model2 not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: model2 not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[100]:1"
     ]
    }
   ],
   "source": [
    "out = model2(reshape([-1 6 -1 5 1 0], 1,2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: out not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: out not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[101]:1"
     ]
    }
   ],
   "source": [
    "reshape(out, 1,2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(P(Array{Float32,2}(1,2)), P(Array{Float32,1}(1)))"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3 = Linear(2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×4 Param{Array{Float32,2}}:\n",
       " 0.271739  -0.546023  -0.0261645  -0.560696"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Knet.params(lin2)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Param{Array{Float32,1}}:\n",
       " 1.0\n",
       " 2.0\n",
       " 3.0\n",
       " 4.0\n",
       " 5.0"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asd = Knet.param(5, init= numinit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×2×3 Array{Int64,3}:\n",
       "[:, :, 1] =\n",
       " 2  3\n",
       "\n",
       "[:, :, 2] =\n",
       " 4  5\n",
       "\n",
       "[:, :, 3] =\n",
       " 6  7"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input2 = [2 3]\n",
    "input2 = cat(input2, [4 5], dims=3)\n",
    "input2 = cat(input2, [6 7], dims=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: model2 not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: model2 not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[106]:1"
     ]
    }
   ],
   "source": [
    "model2(input2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "MethodError",
     "evalue": "MethodError: no method matching reshape()\nClosest candidates are:\n  reshape(!Matched::Array{T,M}, !Matched::Tuple{Vararg{Int64,N}}) where {T, N, M} at reshapedarray.jl:41\n  reshape(!Matched::AbstractArray, !Matched::Int64...) at reshapedarray.jl:115\n  reshape(!Matched::AbstractArray, !Matched::Union{Int64, AbstractUnitRange}...) at reshapedarray.jl:110\n  ...",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching reshape()\nClosest candidates are:\n  reshape(!Matched::Array{T,M}, !Matched::Tuple{Vararg{Int64,N}}) where {T, N, M} at reshapedarray.jl:41\n  reshape(!Matched::AbstractArray, !Matched::Int64...) at reshapedarray.jl:115\n  reshape(!Matched::AbstractArray, !Matched::Union{Int64, AbstractUnitRange}...) at reshapedarray.jl:110\n  ...",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[107]:1"
     ]
    }
   ],
   "source": [
    "reshape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "mat(x; dims = ndims(x) - 1)\n",
       "\\end{verbatim}\n",
       "Reshape \\texttt{x} into a two-dimensional matrix by joining the first dims dimensions, i.e.  \\texttt{reshape(x, prod(size(x,i) for i in 1:dims), :)}\n",
       "\n",
       "\\texttt{dims=ndims(x)-1} (default) is typically used when turning the output of a 4-D convolution result into a 2-D input for a fully connected layer.\n",
       "\n",
       "\\texttt{dims=1} is typically used when turning the 3-D output of an RNN layer into a 2-D input for a fully connected layer.\n",
       "\n",
       "\\texttt{dims=0} will turn the input into a row vector, \\texttt{dims=ndims(x)} will turn it into a column vector.\n",
       "\n"
      ],
      "text/markdown": [
       "```\n",
       "mat(x; dims = ndims(x) - 1)\n",
       "```\n",
       "\n",
       "Reshape `x` into a two-dimensional matrix by joining the first dims dimensions, i.e.  `reshape(x, prod(size(x,i) for i in 1:dims), :)`\n",
       "\n",
       "`dims=ndims(x)-1` (default) is typically used when turning the output of a 4-D convolution result into a 2-D input for a fully connected layer.\n",
       "\n",
       "`dims=1` is typically used when turning the 3-D output of an RNN layer into a 2-D input for a fully connected layer.\n",
       "\n",
       "`dims=0` will turn the input into a row vector, `dims=ndims(x)` will turn it into a column vector.\n"
      ],
      "text/plain": [
       "\u001b[36m  mat(x; dims = ndims(x) - 1)\u001b[39m\n",
       "\n",
       "  Reshape \u001b[36mx\u001b[39m into a two-dimensional matrix by joining the first dims\n",
       "  dimensions, i.e. \u001b[36mreshape(x, prod(size(x,i) for i in 1:dims), :)\u001b[39m\n",
       "\n",
       "  \u001b[36mdims=ndims(x)-1\u001b[39m (default) is typically used when turning the output\n",
       "  of a 4-D convolution result into a 2-D input for a fully connected\n",
       "  layer.\n",
       "\n",
       "  \u001b[36mdims=1\u001b[39m is typically used when turning the 3-D output of an RNN layer\n",
       "  into a 2-D input for a fully connected layer.\n",
       "\n",
       "  \u001b[36mdims=0\u001b[39m will turn the input into a row vector, \u001b[36mdims=ndims(x)\u001b[39m will\n",
       "  turn it into a column vector."
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@doc Knet.mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "prod(f, itr)\n",
       "\\end{verbatim}\n",
       "Returns the product of \\texttt{f} applied to each element of \\texttt{itr}.\n",
       "\n",
       "The return type is \\texttt{Int} for signed integers of less than system word size, and \\texttt{UInt} for unsigned integers of less than system word size.  For all other arguments, a common return type is found to which all arguments are promoted.\n",
       "\n",
       "\\section{Examples}\n",
       "\\begin{verbatim}\n",
       "julia> prod(abs2, [2; 3; 4])\n",
       "576\n",
       "\\end{verbatim}\n",
       "\\begin{verbatim}\n",
       "prod(itr)\n",
       "\\end{verbatim}\n",
       "Returns the product of all elements of a collection.\n",
       "\n",
       "The return type is \\texttt{Int} for signed integers of less than system word size, and \\texttt{UInt} for unsigned integers of less than system word size.  For all other arguments, a common return type is found to which all arguments are promoted.\n",
       "\n",
       "\\section{Examples}\n",
       "\\begin{verbatim}\n",
       "julia> prod(1:20)\n",
       "2432902008176640000\n",
       "\\end{verbatim}\n",
       "\\begin{verbatim}\n",
       "prod(A::AbstractArray; dims)\n",
       "\\end{verbatim}\n",
       "Multiply elements of an array over the given dimensions.\n",
       "\n",
       "\\section{Examples}\n",
       "\\begin{verbatim}\n",
       "julia> A = [1 2; 3 4]\n",
       "2×2 Array{Int64,2}:\n",
       " 1  2\n",
       " 3  4\n",
       "\n",
       "julia> prod(A, dims=1)\n",
       "1×2 Array{Int64,2}:\n",
       " 3  8\n",
       "\n",
       "julia> prod(A, dims=2)\n",
       "2×1 Array{Int64,2}:\n",
       "  2\n",
       " 12\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "```\n",
       "prod(f, itr)\n",
       "```\n",
       "\n",
       "Returns the product of `f` applied to each element of `itr`.\n",
       "\n",
       "The return type is `Int` for signed integers of less than system word size, and `UInt` for unsigned integers of less than system word size.  For all other arguments, a common return type is found to which all arguments are promoted.\n",
       "\n",
       "# Examples\n",
       "\n",
       "```jldoctest\n",
       "julia> prod(abs2, [2; 3; 4])\n",
       "576\n",
       "```\n",
       "\n",
       "```\n",
       "prod(itr)\n",
       "```\n",
       "\n",
       "Returns the product of all elements of a collection.\n",
       "\n",
       "The return type is `Int` for signed integers of less than system word size, and `UInt` for unsigned integers of less than system word size.  For all other arguments, a common return type is found to which all arguments are promoted.\n",
       "\n",
       "# Examples\n",
       "\n",
       "```jldoctest\n",
       "julia> prod(1:20)\n",
       "2432902008176640000\n",
       "```\n",
       "\n",
       "```\n",
       "prod(A::AbstractArray; dims)\n",
       "```\n",
       "\n",
       "Multiply elements of an array over the given dimensions.\n",
       "\n",
       "# Examples\n",
       "\n",
       "```jldoctest\n",
       "julia> A = [1 2; 3 4]\n",
       "2×2 Array{Int64,2}:\n",
       " 1  2\n",
       " 3  4\n",
       "\n",
       "julia> prod(A, dims=1)\n",
       "1×2 Array{Int64,2}:\n",
       " 3  8\n",
       "\n",
       "julia> prod(A, dims=2)\n",
       "2×1 Array{Int64,2}:\n",
       "  2\n",
       " 12\n",
       "```\n"
      ],
      "text/plain": [
       "\u001b[36m  prod(f, itr)\u001b[39m\n",
       "\n",
       "  Returns the product of \u001b[36mf\u001b[39m applied to each element of \u001b[36mitr\u001b[39m.\n",
       "\n",
       "  The return type is \u001b[36mInt\u001b[39m for signed integers of less than system word\n",
       "  size, and \u001b[36mUInt\u001b[39m for unsigned integers of less than system word size.\n",
       "  For all other arguments, a common return type is found to which all\n",
       "  arguments are promoted.\n",
       "\n",
       "\u001b[1m  Examples\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "\u001b[36m  julia> prod(abs2, [2; 3; 4])\u001b[39m\n",
       "\u001b[36m  576\u001b[39m\n",
       "\n",
       "\u001b[36m  prod(itr)\u001b[39m\n",
       "\n",
       "  Returns the product of all elements of a collection.\n",
       "\n",
       "  The return type is \u001b[36mInt\u001b[39m for signed integers of less than system word\n",
       "  size, and \u001b[36mUInt\u001b[39m for unsigned integers of less than system word size.\n",
       "  For all other arguments, a common return type is found to which all\n",
       "  arguments are promoted.\n",
       "\n",
       "\u001b[1m  Examples\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "\u001b[36m  julia> prod(1:20)\u001b[39m\n",
       "\u001b[36m  2432902008176640000\u001b[39m\n",
       "\n",
       "\u001b[36m  prod(A::AbstractArray; dims)\u001b[39m\n",
       "\n",
       "  Multiply elements of an array over the given dimensions.\n",
       "\n",
       "\u001b[1m  Examples\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "\u001b[36m  julia> A = [1 2; 3 4]\u001b[39m\n",
       "\u001b[36m  2×2 Array{Int64,2}:\u001b[39m\n",
       "\u001b[36m   1  2\u001b[39m\n",
       "\u001b[36m   3  4\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> prod(A, dims=1)\u001b[39m\n",
       "\u001b[36m  1×2 Array{Int64,2}:\u001b[39m\n",
       "\u001b[36m   3  8\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> prod(A, dims=2)\u001b[39m\n",
       "\u001b[36m  2×1 Array{Int64,2}:\u001b[39m\n",
       "\u001b[36m    2\u001b[39m\n",
       "\u001b[36m   12\u001b[39m"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@doc prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(P(Array{Float32,2}(4,3)), P(Array{Float32,1}(4)))"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_lin = Linear(3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Param,1}:\n",
       " P(Array{Float32,2}(4,3))\n",
       " P(Array{Float32,1}(4))  "
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Knet.params(a_lin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct filtter; end\n",
    "(f::filtter)(x) = x[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "rnn = RNN(inputSize, hiddenSize; opts...)\n",
       "rnn(x; batchSizes) => y\n",
       "rnn.h, rnn.c  # hidden and cell states\n",
       "\\end{verbatim}\n",
       "\\texttt{RNN} returns a callable RNN object \\texttt{rnn}. Given a minibatch of sequences \\texttt{x}, \\texttt{rnn(x)} returns \\texttt{y}, the hidden states of the final layer for each time step. \\texttt{rnn.h} and \\texttt{rnn.c} fields can be used to set the initial hidden states and read the final hidden states of all layers.  Note that the final time step of \\texttt{y} always contains the final hidden state of the last layer, equivalent to \\texttt{rnn.h} for a single layer network.\n",
       "\n",
       "\\textbf{Dimensions:} The input \\texttt{x} can be 1, 2, or 3 dimensional and \\texttt{y} will have the same number of dimensions as \\texttt{x}. size(x)=(X,[B,T]) and size(y)=(H/2H,[B,T]) where X is inputSize, B is batchSize, T is seqLength, H is hiddenSize, 2H is for bidirectional RNNs. By default a 1-D \\texttt{x} represents a single instance for a single time step, a 2-D \\texttt{x} represents a single minibatch for a single time step, and a 3-D \\texttt{x} represents a sequence of identically sized minibatches for multiple time steps. The output \\texttt{y} gives the hidden state (of the final layer for multi-layer RNNs) for each time step. The fields \\texttt{rnn.h} and \\texttt{rnn.c} represent the hidden states of all layers in a single time step and have size (H,B,L/2L) where L is numLayers and 2L is for bidirectional RNNs.\n",
       "\n",
       "\\textbf{batchSizes:} If \\texttt{batchSizes=nothing} (default), all sequences in a minibatch are assumed to be the same length. If \\texttt{batchSizes} is an array of (non-increasing) integers, it gives us the batch size for each time step (allowing different sequences in the minibatch to have different lengths). In this case \\texttt{x} will typically be 2-D with the second dimension representing variable size batches for time steps. If \\texttt{batchSizes} is used, \\texttt{sum(batchSizes)} should equal \\texttt{length(x) ÷ size(x,1)}. When the batch size is different in every time step, hidden states will have size (H,B,L/2L) where B is always the size of the first (largest) minibatch.\n",
       "\n",
       "\\textbf{Hidden states:} The hidden and cell states are kept in \\texttt{rnn.h} and \\texttt{rnn.c} fields (the cell state is only used by LSTM). They can be initialized during construction using the \\texttt{h} and \\texttt{c} keyword arguments, or modified later by direct assignment. Valid values are \\texttt{nothing} (default), \\texttt{0}, or an array of the right type and size possibly wrapped in a \\texttt{Param}. If the value is \\texttt{nothing} the initial state is assumed to be zero and the final state is discarded keeping the value \\texttt{nothing}. If the value is \\texttt{0} the initial state is assumed to be zero and \\texttt{0} is replaced by the final state on return. If the value is a valid state, it is used as the initial state and is replaced by the final state on return.\n",
       "\n",
       "In a differentiation context the returned final hidden states will be wrapped in \\texttt{Result} types. This is necessary if the same RNN object is to be called multiple times in a single iteration. Between iterations (i.e. after diff/update) the hidden states need to be unboxed with e.g. \\texttt{rnn.h = value(rnn.h)} to prevent spurious dependencies. This happens automatically during the backward pass for GPU RNNs but needs to be done manually for CPU RNNs. See the \\href{https://github.com/denizyuret/Knet.jl/blob/master/tutorial/80.charlm.ipynb}{CharLM Tutorial} for an example.\n",
       "\n",
       "\\textbf{Keyword arguments for RNN:}\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{h=nothing}: Initial hidden state.\n",
       "\n",
       "\n",
       "\\item \\texttt{c=nothing}: Initial cell state.\n",
       "\n",
       "\n",
       "\\item \\texttt{rnnType=:lstm} Type of RNN: One of :relu, :tanh, :lstm, :gru.\n",
       "\n",
       "\n",
       "\\item \\texttt{numLayers=1}: Number of RNN layers.\n",
       "\n",
       "\n",
       "\\item \\texttt{bidirectional=false}: Create a bidirectional RNN if \\texttt{true}.\n",
       "\n",
       "\n",
       "\\item \\texttt{dropout=0}: Dropout probability. Applied to input and between layers.\n",
       "\n",
       "\n",
       "\\item \\texttt{skipInput=false}: Do not multiply the input with a matrix if \\texttt{true}.\n",
       "\n",
       "\n",
       "\\item \\texttt{dataType=Float32}: Data type to use for weights.\n",
       "\n",
       "\n",
       "\\item \\texttt{algo=0}: Algorithm to use, see CUDNN docs for details.\n",
       "\n",
       "\n",
       "\\item \\texttt{seed=0}: Random number seed for dropout. Uses \\texttt{time()} if 0.\n",
       "\n",
       "\n",
       "\\item \\texttt{winit=xavier}: Weight initialization method for matrices.\n",
       "\n",
       "\n",
       "\\item \\texttt{binit=zeros}: Weight initialization method for bias vectors.\n",
       "\n",
       "\n",
       "\\item \\texttt{usegpu=(gpu()>=0)}: GPU used by default if one exists.\n",
       "\n",
       "\\end{itemize}\n",
       "\\textbf{Formulas:} RNNs compute the output h[t] for a given iteration from the recurrent input h[t-1] and the previous layer input x[t] given matrices W, R and biases bW, bR from the following equations:\n",
       "\n",
       "\\texttt{:relu} and \\texttt{:tanh}: Single gate RNN with activation function f:\n",
       "\n",
       "\\begin{verbatim}\n",
       "h[t] = f(W * x[t] .+ R * h[t-1] .+ bW .+ bR)\n",
       "\\end{verbatim}\n",
       "\\texttt{:gru}: Gated recurrent unit:\n",
       "\n",
       "\\begin{verbatim}\n",
       "i[t] = sigm(Wi * x[t] .+ Ri * h[t-1] .+ bWi .+ bRi) # input gate\n",
       "r[t] = sigm(Wr * x[t] .+ Rr * h[t-1] .+ bWr .+ bRr) # reset gate\n",
       "n[t] = tanh(Wn * x[t] .+ r[t] .* (Rn * h[t-1] .+ bRn) .+ bWn) # new gate\n",
       "h[t] = (1 - i[t]) .* n[t] .+ i[t] .* h[t-1]\n",
       "\\end{verbatim}\n",
       "\\texttt{:lstm}: Long short term memory unit with no peephole connections:\n",
       "\n",
       "\\begin{verbatim}\n",
       "i[t] = sigm(Wi * x[t] .+ Ri * h[t-1] .+ bWi .+ bRi) # input gate\n",
       "f[t] = sigm(Wf * x[t] .+ Rf * h[t-1] .+ bWf .+ bRf) # forget gate\n",
       "o[t] = sigm(Wo * x[t] .+ Ro * h[t-1] .+ bWo .+ bRo) # output gate\n",
       "n[t] = tanh(Wn * x[t] .+ Rn * h[t-1] .+ bWn .+ bRn) # new gate\n",
       "c[t] = f[t] .* c[t-1] .+ i[t] .* n[t]               # cell output\n",
       "h[t] = o[t] .* tanh(c[t])\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "```\n",
       "rnn = RNN(inputSize, hiddenSize; opts...)\n",
       "rnn(x; batchSizes) => y\n",
       "rnn.h, rnn.c  # hidden and cell states\n",
       "```\n",
       "\n",
       "`RNN` returns a callable RNN object `rnn`. Given a minibatch of sequences `x`, `rnn(x)` returns `y`, the hidden states of the final layer for each time step. `rnn.h` and `rnn.c` fields can be used to set the initial hidden states and read the final hidden states of all layers.  Note that the final time step of `y` always contains the final hidden state of the last layer, equivalent to `rnn.h` for a single layer network.\n",
       "\n",
       "**Dimensions:** The input `x` can be 1, 2, or 3 dimensional and `y` will have the same number of dimensions as `x`. size(x)=(X,[B,T]) and size(y)=(H/2H,[B,T]) where X is inputSize, B is batchSize, T is seqLength, H is hiddenSize, 2H is for bidirectional RNNs. By default a 1-D `x` represents a single instance for a single time step, a 2-D `x` represents a single minibatch for a single time step, and a 3-D `x` represents a sequence of identically sized minibatches for multiple time steps. The output `y` gives the hidden state (of the final layer for multi-layer RNNs) for each time step. The fields `rnn.h` and `rnn.c` represent the hidden states of all layers in a single time step and have size (H,B,L/2L) where L is numLayers and 2L is for bidirectional RNNs.\n",
       "\n",
       "**batchSizes:** If `batchSizes=nothing` (default), all sequences in a minibatch are assumed to be the same length. If `batchSizes` is an array of (non-increasing) integers, it gives us the batch size for each time step (allowing different sequences in the minibatch to have different lengths). In this case `x` will typically be 2-D with the second dimension representing variable size batches for time steps. If `batchSizes` is used, `sum(batchSizes)` should equal `length(x) ÷ size(x,1)`. When the batch size is different in every time step, hidden states will have size (H,B,L/2L) where B is always the size of the first (largest) minibatch.\n",
       "\n",
       "**Hidden states:** The hidden and cell states are kept in `rnn.h` and `rnn.c` fields (the cell state is only used by LSTM). They can be initialized during construction using the `h` and `c` keyword arguments, or modified later by direct assignment. Valid values are `nothing` (default), `0`, or an array of the right type and size possibly wrapped in a `Param`. If the value is `nothing` the initial state is assumed to be zero and the final state is discarded keeping the value `nothing`. If the value is `0` the initial state is assumed to be zero and `0` is replaced by the final state on return. If the value is a valid state, it is used as the initial state and is replaced by the final state on return.\n",
       "\n",
       "In a differentiation context the returned final hidden states will be wrapped in `Result` types. This is necessary if the same RNN object is to be called multiple times in a single iteration. Between iterations (i.e. after diff/update) the hidden states need to be unboxed with e.g. `rnn.h = value(rnn.h)` to prevent spurious dependencies. This happens automatically during the backward pass for GPU RNNs but needs to be done manually for CPU RNNs. See the [CharLM Tutorial](https://github.com/denizyuret/Knet.jl/blob/master/tutorial/80.charlm.ipynb) for an example.\n",
       "\n",
       "**Keyword arguments for RNN:**\n",
       "\n",
       "  * `h=nothing`: Initial hidden state.\n",
       "  * `c=nothing`: Initial cell state.\n",
       "  * `rnnType=:lstm` Type of RNN: One of :relu, :tanh, :lstm, :gru.\n",
       "  * `numLayers=1`: Number of RNN layers.\n",
       "  * `bidirectional=false`: Create a bidirectional RNN if `true`.\n",
       "  * `dropout=0`: Dropout probability. Applied to input and between layers.\n",
       "  * `skipInput=false`: Do not multiply the input with a matrix if `true`.\n",
       "  * `dataType=Float32`: Data type to use for weights.\n",
       "  * `algo=0`: Algorithm to use, see CUDNN docs for details.\n",
       "  * `seed=0`: Random number seed for dropout. Uses `time()` if 0.\n",
       "  * `winit=xavier`: Weight initialization method for matrices.\n",
       "  * `binit=zeros`: Weight initialization method for bias vectors.\n",
       "  * `usegpu=(gpu()>=0)`: GPU used by default if one exists.\n",
       "\n",
       "**Formulas:** RNNs compute the output h[t] for a given iteration from the recurrent input h[t-1] and the previous layer input x[t] given matrices W, R and biases bW, bR from the following equations:\n",
       "\n",
       "`:relu` and `:tanh`: Single gate RNN with activation function f:\n",
       "\n",
       "```\n",
       "h[t] = f(W * x[t] .+ R * h[t-1] .+ bW .+ bR)\n",
       "```\n",
       "\n",
       "`:gru`: Gated recurrent unit:\n",
       "\n",
       "```\n",
       "i[t] = sigm(Wi * x[t] .+ Ri * h[t-1] .+ bWi .+ bRi) # input gate\n",
       "r[t] = sigm(Wr * x[t] .+ Rr * h[t-1] .+ bWr .+ bRr) # reset gate\n",
       "n[t] = tanh(Wn * x[t] .+ r[t] .* (Rn * h[t-1] .+ bRn) .+ bWn) # new gate\n",
       "h[t] = (1 - i[t]) .* n[t] .+ i[t] .* h[t-1]\n",
       "```\n",
       "\n",
       "`:lstm`: Long short term memory unit with no peephole connections:\n",
       "\n",
       "```\n",
       "i[t] = sigm(Wi * x[t] .+ Ri * h[t-1] .+ bWi .+ bRi) # input gate\n",
       "f[t] = sigm(Wf * x[t] .+ Rf * h[t-1] .+ bWf .+ bRf) # forget gate\n",
       "o[t] = sigm(Wo * x[t] .+ Ro * h[t-1] .+ bWo .+ bRo) # output gate\n",
       "n[t] = tanh(Wn * x[t] .+ Rn * h[t-1] .+ bWn .+ bRn) # new gate\n",
       "c[t] = f[t] .* c[t-1] .+ i[t] .* n[t]               # cell output\n",
       "h[t] = o[t] .* tanh(c[t])\n",
       "```\n"
      ],
      "text/plain": [
       "\u001b[36m  rnn = RNN(inputSize, hiddenSize; opts...)\u001b[39m\n",
       "\u001b[36m  rnn(x; batchSizes) => y\u001b[39m\n",
       "\u001b[36m  rnn.h, rnn.c  # hidden and cell states\u001b[39m\n",
       "\n",
       "  \u001b[36mRNN\u001b[39m returns a callable RNN object \u001b[36mrnn\u001b[39m. Given a minibatch of\n",
       "  sequences \u001b[36mx\u001b[39m, \u001b[36mrnn(x)\u001b[39m returns \u001b[36my\u001b[39m, the hidden states of the final layer\n",
       "  for each time step. \u001b[36mrnn.h\u001b[39m and \u001b[36mrnn.c\u001b[39m fields can be used to set the\n",
       "  initial hidden states and read the final hidden states of all\n",
       "  layers. Note that the final time step of \u001b[36my\u001b[39m always contains the final\n",
       "  hidden state of the last layer, equivalent to \u001b[36mrnn.h\u001b[39m for a single\n",
       "  layer network.\n",
       "\n",
       "  \u001b[1mDimensions:\u001b[22m The input \u001b[36mx\u001b[39m can be 1, 2, or 3 dimensional and \u001b[36my\u001b[39m will\n",
       "  have the same number of dimensions as \u001b[36mx\u001b[39m. size(x)=(X,[B,T]) and\n",
       "  size(y)=(H/2H,[B,T]) where X is inputSize, B is batchSize, T is\n",
       "  seqLength, H is hiddenSize, 2H is for bidirectional RNNs. By default\n",
       "  a 1-D \u001b[36mx\u001b[39m represents a single instance for a single time step, a 2-D \u001b[36mx\u001b[39m\n",
       "  represents a single minibatch for a single time step, and a 3-D \u001b[36mx\u001b[39m\n",
       "  represents a sequence of identically sized minibatches for multiple\n",
       "  time steps. The output \u001b[36my\u001b[39m gives the hidden state (of the final layer\n",
       "  for multi-layer RNNs) for each time step. The fields \u001b[36mrnn.h\u001b[39m and \u001b[36mrnn.c\u001b[39m\n",
       "  represent the hidden states of all layers in a single time step and\n",
       "  have size (H,B,L/2L) where L is numLayers and 2L is for\n",
       "  bidirectional RNNs.\n",
       "\n",
       "  \u001b[1mbatchSizes:\u001b[22m If \u001b[36mbatchSizes=nothing\u001b[39m (default), all sequences in a\n",
       "  minibatch are assumed to be the same length. If \u001b[36mbatchSizes\u001b[39m is an\n",
       "  array of (non-increasing) integers, it gives us the batch size for\n",
       "  each time step (allowing different sequences in the minibatch to\n",
       "  have different lengths). In this case \u001b[36mx\u001b[39m will typically be 2-D with\n",
       "  the second dimension representing variable size batches for time\n",
       "  steps. If \u001b[36mbatchSizes\u001b[39m is used, \u001b[36msum(batchSizes)\u001b[39m should equal \u001b[36mlength(x)\n",
       "  ÷ size(x,1)\u001b[39m. When the batch size is different in every time step,\n",
       "  hidden states will have size (H,B,L/2L) where B is always the size\n",
       "  of the first (largest) minibatch.\n",
       "\n",
       "  \u001b[1mHidden states:\u001b[22m The hidden and cell states are kept in \u001b[36mrnn.h\u001b[39m and\n",
       "  \u001b[36mrnn.c\u001b[39m fields (the cell state is only used by LSTM). They can be\n",
       "  initialized during construction using the \u001b[36mh\u001b[39m and \u001b[36mc\u001b[39m keyword arguments,\n",
       "  or modified later by direct assignment. Valid values are \u001b[36mnothing\u001b[39m\n",
       "  (default), \u001b[36m0\u001b[39m, or an array of the right type and size possibly\n",
       "  wrapped in a \u001b[36mParam\u001b[39m. If the value is \u001b[36mnothing\u001b[39m the initial state is\n",
       "  assumed to be zero and the final state is discarded keeping the\n",
       "  value \u001b[36mnothing\u001b[39m. If the value is \u001b[36m0\u001b[39m the initial state is assumed to be\n",
       "  zero and \u001b[36m0\u001b[39m is replaced by the final state on return. If the value is\n",
       "  a valid state, it is used as the initial state and is replaced by\n",
       "  the final state on return.\n",
       "\n",
       "  In a differentiation context the returned final hidden states will\n",
       "  be wrapped in \u001b[36mResult\u001b[39m types. This is necessary if the same RNN object\n",
       "  is to be called multiple times in a single iteration. Between\n",
       "  iterations (i.e. after diff/update) the hidden states need to be\n",
       "  unboxed with e.g. \u001b[36mrnn.h = value(rnn.h)\u001b[39m to prevent spurious\n",
       "  dependencies. This happens automatically during the backward pass\n",
       "  for GPU RNNs but needs to be done manually for CPU RNNs. See the\n",
       "  CharLM Tutorial\n",
       "  (https://github.com/denizyuret/Knet.jl/blob/master/tutorial/80.charlm.ipynb)\n",
       "  for an example.\n",
       "\n",
       "  \u001b[1mKeyword arguments for RNN:\u001b[22m\n",
       "\n",
       "    •    \u001b[36mh=nothing\u001b[39m: Initial hidden state.\n",
       "\n",
       "    •    \u001b[36mc=nothing\u001b[39m: Initial cell state.\n",
       "\n",
       "    •    \u001b[36mrnnType=:lstm\u001b[39m Type of RNN: One of :relu, :tanh, :lstm,\n",
       "        :gru.\n",
       "\n",
       "    •    \u001b[36mnumLayers=1\u001b[39m: Number of RNN layers.\n",
       "\n",
       "    •    \u001b[36mbidirectional=false\u001b[39m: Create a bidirectional RNN if \u001b[36mtrue\u001b[39m.\n",
       "\n",
       "    •    \u001b[36mdropout=0\u001b[39m: Dropout probability. Applied to input and\n",
       "        between layers.\n",
       "\n",
       "    •    \u001b[36mskipInput=false\u001b[39m: Do not multiply the input with a matrix\n",
       "        if \u001b[36mtrue\u001b[39m.\n",
       "\n",
       "    •    \u001b[36mdataType=Float32\u001b[39m: Data type to use for weights.\n",
       "\n",
       "    •    \u001b[36malgo=0\u001b[39m: Algorithm to use, see CUDNN docs for details.\n",
       "\n",
       "    •    \u001b[36mseed=0\u001b[39m: Random number seed for dropout. Uses \u001b[36mtime()\u001b[39m if 0.\n",
       "\n",
       "    •    \u001b[36mwinit=xavier\u001b[39m: Weight initialization method for matrices.\n",
       "\n",
       "    •    \u001b[36mbinit=zeros\u001b[39m: Weight initialization method for bias\n",
       "        vectors.\n",
       "\n",
       "    •    \u001b[36musegpu=(gpu()>=0)\u001b[39m: GPU used by default if one exists.\n",
       "\n",
       "  \u001b[1mFormulas:\u001b[22m RNNs compute the output h[t] for a given iteration from\n",
       "  the recurrent input h[t-1] and the previous layer input x[t] given\n",
       "  matrices W, R and biases bW, bR from the following equations:\n",
       "\n",
       "  \u001b[36m:relu\u001b[39m and \u001b[36m:tanh\u001b[39m: Single gate RNN with activation function f:\n",
       "\n",
       "\u001b[36m  h[t] = f(W * x[t] .+ R * h[t-1] .+ bW .+ bR)\u001b[39m\n",
       "\n",
       "  \u001b[36m:gru\u001b[39m: Gated recurrent unit:\n",
       "\n",
       "\u001b[36m  i[t] = sigm(Wi * x[t] .+ Ri * h[t-1] .+ bWi .+ bRi) # input gate\u001b[39m\n",
       "\u001b[36m  r[t] = sigm(Wr * x[t] .+ Rr * h[t-1] .+ bWr .+ bRr) # reset gate\u001b[39m\n",
       "\u001b[36m  n[t] = tanh(Wn * x[t] .+ r[t] .* (Rn * h[t-1] .+ bRn) .+ bWn) # new gate\u001b[39m\n",
       "\u001b[36m  h[t] = (1 - i[t]) .* n[t] .+ i[t] .* h[t-1]\u001b[39m\n",
       "\n",
       "  \u001b[36m:lstm\u001b[39m: Long short term memory unit with no peephole connections:\n",
       "\n",
       "\u001b[36m  i[t] = sigm(Wi * x[t] .+ Ri * h[t-1] .+ bWi .+ bRi) # input gate\u001b[39m\n",
       "\u001b[36m  f[t] = sigm(Wf * x[t] .+ Rf * h[t-1] .+ bWf .+ bRf) # forget gate\u001b[39m\n",
       "\u001b[36m  o[t] = sigm(Wo * x[t] .+ Ro * h[t-1] .+ bWo .+ bRo) # output gate\u001b[39m\n",
       "\u001b[36m  n[t] = tanh(Wn * x[t] .+ Rn * h[t-1] .+ bWn .+ bRn) # new gate\u001b[39m\n",
       "\u001b[36m  c[t] = f[t] .* c[t-1] .+ i[t] .* n[t]               # cell output\u001b[39m\n",
       "\u001b[36m  h[t] = o[t] .* tanh(c[t])\u001b[39m"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@doc Knet.RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "mat(x; dims = ndims(x) - 1)\n",
       "\\end{verbatim}\n",
       "Reshape \\texttt{x} into a two-dimensional matrix by joining the first dims dimensions, i.e.  \\texttt{reshape(x, prod(size(x,i) for i in 1:dims), :)}\n",
       "\n",
       "\\texttt{dims=ndims(x)-1} (default) is typically used when turning the output of a 4-D convolution result into a 2-D input for a fully connected layer.\n",
       "\n",
       "\\texttt{dims=1} is typically used when turning the 3-D output of an RNN layer into a 2-D input for a fully connected layer.\n",
       "\n",
       "\\texttt{dims=0} will turn the input into a row vector, \\texttt{dims=ndims(x)} will turn it into a column vector.\n",
       "\n"
      ],
      "text/markdown": [
       "```\n",
       "mat(x; dims = ndims(x) - 1)\n",
       "```\n",
       "\n",
       "Reshape `x` into a two-dimensional matrix by joining the first dims dimensions, i.e.  `reshape(x, prod(size(x,i) for i in 1:dims), :)`\n",
       "\n",
       "`dims=ndims(x)-1` (default) is typically used when turning the output of a 4-D convolution result into a 2-D input for a fully connected layer.\n",
       "\n",
       "`dims=1` is typically used when turning the 3-D output of an RNN layer into a 2-D input for a fully connected layer.\n",
       "\n",
       "`dims=0` will turn the input into a row vector, `dims=ndims(x)` will turn it into a column vector.\n"
      ],
      "text/plain": [
       "\u001b[36m  mat(x; dims = ndims(x) - 1)\u001b[39m\n",
       "\n",
       "  Reshape \u001b[36mx\u001b[39m into a two-dimensional matrix by joining the first dims\n",
       "  dimensions, i.e. \u001b[36mreshape(x, prod(size(x,i) for i in 1:dims), :)\u001b[39m\n",
       "\n",
       "  \u001b[36mdims=ndims(x)-1\u001b[39m (default) is typically used when turning the output\n",
       "  of a 4-D convolution result into a 2-D input for a fully connected\n",
       "  layer.\n",
       "\n",
       "  \u001b[36mdims=1\u001b[39m is typically used when turning the 3-D output of an RNN layer\n",
       "  into a 2-D input for a fully connected layer.\n",
       "\n",
       "  \u001b[36mdims=0\u001b[39m will turn the input into a row vector, \u001b[36mdims=ndims(x)\u001b[39m will\n",
       "  turn it into a column vector."
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@doc Knet.mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "prod(f, itr)\n",
       "\\end{verbatim}\n",
       "Returns the product of \\texttt{f} applied to each element of \\texttt{itr}.\n",
       "\n",
       "The return type is \\texttt{Int} for signed integers of less than system word size, and \\texttt{UInt} for unsigned integers of less than system word size.  For all other arguments, a common return type is found to which all arguments are promoted.\n",
       "\n",
       "\\section{Examples}\n",
       "\\begin{verbatim}\n",
       "julia> prod(abs2, [2; 3; 4])\n",
       "576\n",
       "\\end{verbatim}\n",
       "\\begin{verbatim}\n",
       "prod(itr)\n",
       "\\end{verbatim}\n",
       "Returns the product of all elements of a collection.\n",
       "\n",
       "The return type is \\texttt{Int} for signed integers of less than system word size, and \\texttt{UInt} for unsigned integers of less than system word size.  For all other arguments, a common return type is found to which all arguments are promoted.\n",
       "\n",
       "\\section{Examples}\n",
       "\\begin{verbatim}\n",
       "julia> prod(1:20)\n",
       "2432902008176640000\n",
       "\\end{verbatim}\n",
       "\\begin{verbatim}\n",
       "prod(A::AbstractArray; dims)\n",
       "\\end{verbatim}\n",
       "Multiply elements of an array over the given dimensions.\n",
       "\n",
       "\\section{Examples}\n",
       "\\begin{verbatim}\n",
       "julia> A = [1 2; 3 4]\n",
       "2×2 Array{Int64,2}:\n",
       " 1  2\n",
       " 3  4\n",
       "\n",
       "julia> prod(A, dims=1)\n",
       "1×2 Array{Int64,2}:\n",
       " 3  8\n",
       "\n",
       "julia> prod(A, dims=2)\n",
       "2×1 Array{Int64,2}:\n",
       "  2\n",
       " 12\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "```\n",
       "prod(f, itr)\n",
       "```\n",
       "\n",
       "Returns the product of `f` applied to each element of `itr`.\n",
       "\n",
       "The return type is `Int` for signed integers of less than system word size, and `UInt` for unsigned integers of less than system word size.  For all other arguments, a common return type is found to which all arguments are promoted.\n",
       "\n",
       "# Examples\n",
       "\n",
       "```jldoctest\n",
       "julia> prod(abs2, [2; 3; 4])\n",
       "576\n",
       "```\n",
       "\n",
       "```\n",
       "prod(itr)\n",
       "```\n",
       "\n",
       "Returns the product of all elements of a collection.\n",
       "\n",
       "The return type is `Int` for signed integers of less than system word size, and `UInt` for unsigned integers of less than system word size.  For all other arguments, a common return type is found to which all arguments are promoted.\n",
       "\n",
       "# Examples\n",
       "\n",
       "```jldoctest\n",
       "julia> prod(1:20)\n",
       "2432902008176640000\n",
       "```\n",
       "\n",
       "```\n",
       "prod(A::AbstractArray; dims)\n",
       "```\n",
       "\n",
       "Multiply elements of an array over the given dimensions.\n",
       "\n",
       "# Examples\n",
       "\n",
       "```jldoctest\n",
       "julia> A = [1 2; 3 4]\n",
       "2×2 Array{Int64,2}:\n",
       " 1  2\n",
       " 3  4\n",
       "\n",
       "julia> prod(A, dims=1)\n",
       "1×2 Array{Int64,2}:\n",
       " 3  8\n",
       "\n",
       "julia> prod(A, dims=2)\n",
       "2×1 Array{Int64,2}:\n",
       "  2\n",
       " 12\n",
       "```\n"
      ],
      "text/plain": [
       "\u001b[36m  prod(f, itr)\u001b[39m\n",
       "\n",
       "  Returns the product of \u001b[36mf\u001b[39m applied to each element of \u001b[36mitr\u001b[39m.\n",
       "\n",
       "  The return type is \u001b[36mInt\u001b[39m for signed integers of less than system word\n",
       "  size, and \u001b[36mUInt\u001b[39m for unsigned integers of less than system word size.\n",
       "  For all other arguments, a common return type is found to which all\n",
       "  arguments are promoted.\n",
       "\n",
       "\u001b[1m  Examples\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "\u001b[36m  julia> prod(abs2, [2; 3; 4])\u001b[39m\n",
       "\u001b[36m  576\u001b[39m\n",
       "\n",
       "\u001b[36m  prod(itr)\u001b[39m\n",
       "\n",
       "  Returns the product of all elements of a collection.\n",
       "\n",
       "  The return type is \u001b[36mInt\u001b[39m for signed integers of less than system word\n",
       "  size, and \u001b[36mUInt\u001b[39m for unsigned integers of less than system word size.\n",
       "  For all other arguments, a common return type is found to which all\n",
       "  arguments are promoted.\n",
       "\n",
       "\u001b[1m  Examples\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "\u001b[36m  julia> prod(1:20)\u001b[39m\n",
       "\u001b[36m  2432902008176640000\u001b[39m\n",
       "\n",
       "\u001b[36m  prod(A::AbstractArray; dims)\u001b[39m\n",
       "\n",
       "  Multiply elements of an array over the given dimensions.\n",
       "\n",
       "\u001b[1m  Examples\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "\u001b[36m  julia> A = [1 2; 3 4]\u001b[39m\n",
       "\u001b[36m  2×2 Array{Int64,2}:\u001b[39m\n",
       "\u001b[36m   1  2\u001b[39m\n",
       "\u001b[36m   3  4\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> prod(A, dims=1)\u001b[39m\n",
       "\u001b[36m  1×2 Array{Int64,2}:\u001b[39m\n",
       "\u001b[36m   3  8\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> prod(A, dims=2)\u001b[39m\n",
       "\u001b[36m  2×1 Array{Int64,2}:\u001b[39m\n",
       "\u001b[36m    2\u001b[39m\n",
       "\u001b[36m   12\u001b[39m"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@doc Knet.prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct ABC; a; end;\n",
    "(ab::ABC)(x) = 3\n",
    "(ab::ABC)(x,y) = 4\n",
    "(ab::ABC)(x;dat=:hop) = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ABC(7)"
      ]
     },
     "execution_count": 730,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ab1 = ABC(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 731,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ab1(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Chain\n",
    "    layers\n",
    "    Chain(layers...) = new(layers)\n",
    "end\n",
    "(c::Chain)(x) = (for l in c.layers; x = l(x); end; x)\n",
    "(c::Chain)(x,y) = nll(c(x),y)\n",
    "(c::Chain)(d::Data) = mean(c(x,y) for (x,y) in d)\n",
    "(c::Chain)(d,a,b) = mean(c(x,y) for (x,y) in d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#struct MLP; w1; b1; f1; w2; b2; f2; end\n",
    "#MLP()\n",
    "#(m::MLP)(x) = Chain(Dense(m.w1, m.b1, m.f1), Dense(m.w1, m.b1, m.f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP (generic function with 1 method)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLP(i::Int, h::Int, o::Int) = Chain(Dense(i,h, Knet.relu), Dense(h,o,Knet.relu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2 Param{Array{Int64,2}}:\n",
       " 3  2\n",
       " 1  2"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ab1 = Knet.param([3 2; 1 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain((Dense(P(Array{Float32,2}(2,3)), P(Array{Float32,1}(2)), Knet.relu), Dense(P(Array{Float32,2}(2,2)), P(Array{Float32,1}(2)), Knet.relu)))"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_ex1 = MLP(3,2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(P(Array{Float32,2}(2,1)), P(Array{Float32,1}(2)))"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_ex1 = Linear(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×1 Param{Array{Float32,2}}:\n",
       " 0.48963422\n",
       " 0.5708092 "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Knet.params(lin_ex1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×1 Array{Float32,2}:\n",
       " 146.89026\n",
       " 171.24275"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_ex1([300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×1×2×2×1 Array{Float64,5}:\n",
       "[:, :, 1, 1, 1] =\n",
       " 0.0\n",
       " 0.0\n",
       "\n",
       "[:, :, 2, 1, 1] =\n",
       " 0.0\n",
       " 0.0\n",
       "\n",
       "[:, :, 1, 2, 1] =\n",
       " 0.0\n",
       " 0.0\n",
       "\n",
       "[:, :, 2, 2, 1] =\n",
       " 0.0\n",
       " 0.0"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abc = zeros(2,1,2,2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimlen(x) = length(size(x)); #ndims does this nevermind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndims(abc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matmul (generic function with 1 method)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matmul(x,y) = reshapex*y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: matmatmul not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: matmatmul not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[128]:1"
     ]
    }
   ],
   "source": [
    "matmatmul(a,b,dim1, dim2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2×2 Array{Int64,3}:\n",
       "[:, :, 1] =\n",
       " 1  3\n",
       " 2  4\n",
       "\n",
       "[:, :, 2] =\n",
       " 5  7\n",
       " 6  8"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = reshape(Vector(1:8),(2,2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2×2 Array{Int64,3}:\n",
       "[:, :, 1] =\n",
       " 1  3\n",
       " 2  4\n",
       "\n",
       "[:, :, 2] =\n",
       " 5  7\n",
       " 6  8"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = reshape(Vector(1:8),(2,2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "ename": "MethodError",
     "evalue": "MethodError: no method matching mapslices(::typeof(sum), ::Tuple{Array{Int64,3},Array{Int64,3}}; dims=[3])\nClosest candidates are:\n  mapslices(::Any, !Matched::AbstractArray; dims) at abstractarray.jl:1919",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching mapslices(::typeof(sum), ::Tuple{Array{Int64,3},Array{Int64,3}}; dims=[3])\nClosest candidates are:\n  mapslices(::Any, !Matched::AbstractArray; dims) at abstractarray.jl:1919",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[131]:1"
     ]
    }
   ],
   "source": [
    "mapslices(sum, (a,b), dims = [3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×2×2 Array{DataType,3}:\n",
       "[:, :, 1] =\n",
       " Array{Int64,1}  Array{Int64,1}\n",
       "\n",
       "[:, :, 2] =\n",
       " Array{Int64,1}  Array{Int64,1}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapslices(typeof, a, dims = [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2×2 Array{Int64,3}:\n",
       "[:, :, 1] =\n",
       " 1  3\n",
       " 2  4\n",
       "\n",
       "[:, :, 2] =\n",
       " 5  7\n",
       " 6  8"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = reshape(Vector(1:8), (2,2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2×2 Array{Int64,3}:\n",
       "[:, :, 1] =\n",
       " 1  2\n",
       " 5  6\n",
       "\n",
       "[:, :, 2] =\n",
       " 3  4\n",
       " 7  8"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "permutedims(A, [3, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: S not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: S not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[135]:1"
     ]
    }
   ],
   "source": [
    "S[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0-element Array{Any,1}"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Array{Any,1}:\n",
       " [1 3; 2 4]\n",
       "\n",
       "[5 7; 6 8]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "push!(S, A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "rnn = RNN(inputSize, hiddenSize; opts...)\n",
       "rnn(x; batchSizes) => y\n",
       "rnn.h, rnn.c  # hidden and cell states\n",
       "\\end{verbatim}\n",
       "\\texttt{RNN} returns a callable RNN object \\texttt{rnn}. Given a minibatch of sequences \\texttt{x}, \\texttt{rnn(x)} returns \\texttt{y}, the hidden states of the final layer for each time step. \\texttt{rnn.h} and \\texttt{rnn.c} fields can be used to set the initial hidden states and read the final hidden states of all layers.  Note that the final time step of \\texttt{y} always contains the final hidden state of the last layer, equivalent to \\texttt{rnn.h} for a single layer network.\n",
       "\n",
       "\\textbf{Dimensions:} The input \\texttt{x} can be 1, 2, or 3 dimensional and \\texttt{y} will have the same number of dimensions as \\texttt{x}. size(x)=(X,[B,T]) and size(y)=(H/2H,[B,T]) where X is inputSize, B is batchSize, T is seqLength, H is hiddenSize, 2H is for bidirectional RNNs. By default a 1-D \\texttt{x} represents a single instance for a single time step, a 2-D \\texttt{x} represents a single minibatch for a single time step, and a 3-D \\texttt{x} represents a sequence of identically sized minibatches for multiple time steps. The output \\texttt{y} gives the hidden state (of the final layer for multi-layer RNNs) for each time step. The fields \\texttt{rnn.h} and \\texttt{rnn.c} represent the hidden states of all layers in a single time step and have size (H,B,L/2L) where L is numLayers and 2L is for bidirectional RNNs.\n",
       "\n",
       "\\textbf{batchSizes:} If \\texttt{batchSizes=nothing} (default), all sequences in a minibatch are assumed to be the same length. If \\texttt{batchSizes} is an array of (non-increasing) integers, it gives us the batch size for each time step (allowing different sequences in the minibatch to have different lengths). In this case \\texttt{x} will typically be 2-D with the second dimension representing variable size batches for time steps. If \\texttt{batchSizes} is used, \\texttt{sum(batchSizes)} should equal \\texttt{length(x) ÷ size(x,1)}. When the batch size is different in every time step, hidden states will have size (H,B,L/2L) where B is always the size of the first (largest) minibatch.\n",
       "\n",
       "\\textbf{Hidden states:} The hidden and cell states are kept in \\texttt{rnn.h} and \\texttt{rnn.c} fields (the cell state is only used by LSTM). They can be initialized during construction using the \\texttt{h} and \\texttt{c} keyword arguments, or modified later by direct assignment. Valid values are \\texttt{nothing} (default), \\texttt{0}, or an array of the right type and size possibly wrapped in a \\texttt{Param}. If the value is \\texttt{nothing} the initial state is assumed to be zero and the final state is discarded keeping the value \\texttt{nothing}. If the value is \\texttt{0} the initial state is assumed to be zero and \\texttt{0} is replaced by the final state on return. If the value is a valid state, it is used as the initial state and is replaced by the final state on return.\n",
       "\n",
       "In a differentiation context the returned final hidden states will be wrapped in \\texttt{Result} types. This is necessary if the same RNN object is to be called multiple times in a single iteration. Between iterations (i.e. after diff/update) the hidden states need to be unboxed with e.g. \\texttt{rnn.h = value(rnn.h)} to prevent spurious dependencies. This happens automatically during the backward pass for GPU RNNs but needs to be done manually for CPU RNNs. See the \\href{https://github.com/denizyuret/Knet.jl/blob/master/tutorial/80.charlm.ipynb}{CharLM Tutorial} for an example.\n",
       "\n",
       "\\textbf{Keyword arguments for RNN:}\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{h=nothing}: Initial hidden state.\n",
       "\n",
       "\n",
       "\\item \\texttt{c=nothing}: Initial cell state.\n",
       "\n",
       "\n",
       "\\item \\texttt{rnnType=:lstm} Type of RNN: One of :relu, :tanh, :lstm, :gru.\n",
       "\n",
       "\n",
       "\\item \\texttt{numLayers=1}: Number of RNN layers.\n",
       "\n",
       "\n",
       "\\item \\texttt{bidirectional=false}: Create a bidirectional RNN if \\texttt{true}.\n",
       "\n",
       "\n",
       "\\item \\texttt{dropout=0}: Dropout probability. Applied to input and between layers.\n",
       "\n",
       "\n",
       "\\item \\texttt{skipInput=false}: Do not multiply the input with a matrix if \\texttt{true}.\n",
       "\n",
       "\n",
       "\\item \\texttt{dataType=Float32}: Data type to use for weights.\n",
       "\n",
       "\n",
       "\\item \\texttt{algo=0}: Algorithm to use, see CUDNN docs for details.\n",
       "\n",
       "\n",
       "\\item \\texttt{seed=0}: Random number seed for dropout. Uses \\texttt{time()} if 0.\n",
       "\n",
       "\n",
       "\\item \\texttt{winit=xavier}: Weight initialization method for matrices.\n",
       "\n",
       "\n",
       "\\item \\texttt{binit=zeros}: Weight initialization method for bias vectors.\n",
       "\n",
       "\n",
       "\\item \\texttt{usegpu=(gpu()>=0)}: GPU used by default if one exists.\n",
       "\n",
       "\\end{itemize}\n",
       "\\textbf{Formulas:} RNNs compute the output h[t] for a given iteration from the recurrent input h[t-1] and the previous layer input x[t] given matrices W, R and biases bW, bR from the following equations:\n",
       "\n",
       "\\texttt{:relu} and \\texttt{:tanh}: Single gate RNN with activation function f:\n",
       "\n",
       "\\begin{verbatim}\n",
       "h[t] = f(W * x[t] .+ R * h[t-1] .+ bW .+ bR)\n",
       "\\end{verbatim}\n",
       "\\texttt{:gru}: Gated recurrent unit:\n",
       "\n",
       "\\begin{verbatim}\n",
       "i[t] = sigm(Wi * x[t] .+ Ri * h[t-1] .+ bWi .+ bRi) # input gate\n",
       "r[t] = sigm(Wr * x[t] .+ Rr * h[t-1] .+ bWr .+ bRr) # reset gate\n",
       "n[t] = tanh(Wn * x[t] .+ r[t] .* (Rn * h[t-1] .+ bRn) .+ bWn) # new gate\n",
       "h[t] = (1 - i[t]) .* n[t] .+ i[t] .* h[t-1]\n",
       "\\end{verbatim}\n",
       "\\texttt{:lstm}: Long short term memory unit with no peephole connections:\n",
       "\n",
       "\\begin{verbatim}\n",
       "i[t] = sigm(Wi * x[t] .+ Ri * h[t-1] .+ bWi .+ bRi) # input gate\n",
       "f[t] = sigm(Wf * x[t] .+ Rf * h[t-1] .+ bWf .+ bRf) # forget gate\n",
       "o[t] = sigm(Wo * x[t] .+ Ro * h[t-1] .+ bWo .+ bRo) # output gate\n",
       "n[t] = tanh(Wn * x[t] .+ Rn * h[t-1] .+ bWn .+ bRn) # new gate\n",
       "c[t] = f[t] .* c[t-1] .+ i[t] .* n[t]               # cell output\n",
       "h[t] = o[t] .* tanh(c[t])\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "```\n",
       "rnn = RNN(inputSize, hiddenSize; opts...)\n",
       "rnn(x; batchSizes) => y\n",
       "rnn.h, rnn.c  # hidden and cell states\n",
       "```\n",
       "\n",
       "`RNN` returns a callable RNN object `rnn`. Given a minibatch of sequences `x`, `rnn(x)` returns `y`, the hidden states of the final layer for each time step. `rnn.h` and `rnn.c` fields can be used to set the initial hidden states and read the final hidden states of all layers.  Note that the final time step of `y` always contains the final hidden state of the last layer, equivalent to `rnn.h` for a single layer network.\n",
       "\n",
       "**Dimensions:** The input `x` can be 1, 2, or 3 dimensional and `y` will have the same number of dimensions as `x`. size(x)=(X,[B,T]) and size(y)=(H/2H,[B,T]) where X is inputSize, B is batchSize, T is seqLength, H is hiddenSize, 2H is for bidirectional RNNs. By default a 1-D `x` represents a single instance for a single time step, a 2-D `x` represents a single minibatch for a single time step, and a 3-D `x` represents a sequence of identically sized minibatches for multiple time steps. The output `y` gives the hidden state (of the final layer for multi-layer RNNs) for each time step. The fields `rnn.h` and `rnn.c` represent the hidden states of all layers in a single time step and have size (H,B,L/2L) where L is numLayers and 2L is for bidirectional RNNs.\n",
       "\n",
       "**batchSizes:** If `batchSizes=nothing` (default), all sequences in a minibatch are assumed to be the same length. If `batchSizes` is an array of (non-increasing) integers, it gives us the batch size for each time step (allowing different sequences in the minibatch to have different lengths). In this case `x` will typically be 2-D with the second dimension representing variable size batches for time steps. If `batchSizes` is used, `sum(batchSizes)` should equal `length(x) ÷ size(x,1)`. When the batch size is different in every time step, hidden states will have size (H,B,L/2L) where B is always the size of the first (largest) minibatch.\n",
       "\n",
       "**Hidden states:** The hidden and cell states are kept in `rnn.h` and `rnn.c` fields (the cell state is only used by LSTM). They can be initialized during construction using the `h` and `c` keyword arguments, or modified later by direct assignment. Valid values are `nothing` (default), `0`, or an array of the right type and size possibly wrapped in a `Param`. If the value is `nothing` the initial state is assumed to be zero and the final state is discarded keeping the value `nothing`. If the value is `0` the initial state is assumed to be zero and `0` is replaced by the final state on return. If the value is a valid state, it is used as the initial state and is replaced by the final state on return.\n",
       "\n",
       "In a differentiation context the returned final hidden states will be wrapped in `Result` types. This is necessary if the same RNN object is to be called multiple times in a single iteration. Between iterations (i.e. after diff/update) the hidden states need to be unboxed with e.g. `rnn.h = value(rnn.h)` to prevent spurious dependencies. This happens automatically during the backward pass for GPU RNNs but needs to be done manually for CPU RNNs. See the [CharLM Tutorial](https://github.com/denizyuret/Knet.jl/blob/master/tutorial/80.charlm.ipynb) for an example.\n",
       "\n",
       "**Keyword arguments for RNN:**\n",
       "\n",
       "  * `h=nothing`: Initial hidden state.\n",
       "  * `c=nothing`: Initial cell state.\n",
       "  * `rnnType=:lstm` Type of RNN: One of :relu, :tanh, :lstm, :gru.\n",
       "  * `numLayers=1`: Number of RNN layers.\n",
       "  * `bidirectional=false`: Create a bidirectional RNN if `true`.\n",
       "  * `dropout=0`: Dropout probability. Applied to input and between layers.\n",
       "  * `skipInput=false`: Do not multiply the input with a matrix if `true`.\n",
       "  * `dataType=Float32`: Data type to use for weights.\n",
       "  * `algo=0`: Algorithm to use, see CUDNN docs for details.\n",
       "  * `seed=0`: Random number seed for dropout. Uses `time()` if 0.\n",
       "  * `winit=xavier`: Weight initialization method for matrices.\n",
       "  * `binit=zeros`: Weight initialization method for bias vectors.\n",
       "  * `usegpu=(gpu()>=0)`: GPU used by default if one exists.\n",
       "\n",
       "**Formulas:** RNNs compute the output h[t] for a given iteration from the recurrent input h[t-1] and the previous layer input x[t] given matrices W, R and biases bW, bR from the following equations:\n",
       "\n",
       "`:relu` and `:tanh`: Single gate RNN with activation function f:\n",
       "\n",
       "```\n",
       "h[t] = f(W * x[t] .+ R * h[t-1] .+ bW .+ bR)\n",
       "```\n",
       "\n",
       "`:gru`: Gated recurrent unit:\n",
       "\n",
       "```\n",
       "i[t] = sigm(Wi * x[t] .+ Ri * h[t-1] .+ bWi .+ bRi) # input gate\n",
       "r[t] = sigm(Wr * x[t] .+ Rr * h[t-1] .+ bWr .+ bRr) # reset gate\n",
       "n[t] = tanh(Wn * x[t] .+ r[t] .* (Rn * h[t-1] .+ bRn) .+ bWn) # new gate\n",
       "h[t] = (1 - i[t]) .* n[t] .+ i[t] .* h[t-1]\n",
       "```\n",
       "\n",
       "`:lstm`: Long short term memory unit with no peephole connections:\n",
       "\n",
       "```\n",
       "i[t] = sigm(Wi * x[t] .+ Ri * h[t-1] .+ bWi .+ bRi) # input gate\n",
       "f[t] = sigm(Wf * x[t] .+ Rf * h[t-1] .+ bWf .+ bRf) # forget gate\n",
       "o[t] = sigm(Wo * x[t] .+ Ro * h[t-1] .+ bWo .+ bRo) # output gate\n",
       "n[t] = tanh(Wn * x[t] .+ Rn * h[t-1] .+ bWn .+ bRn) # new gate\n",
       "c[t] = f[t] .* c[t-1] .+ i[t] .* n[t]               # cell output\n",
       "h[t] = o[t] .* tanh(c[t])\n",
       "```\n"
      ],
      "text/plain": [
       "\u001b[36m  rnn = RNN(inputSize, hiddenSize; opts...)\u001b[39m\n",
       "\u001b[36m  rnn(x; batchSizes) => y\u001b[39m\n",
       "\u001b[36m  rnn.h, rnn.c  # hidden and cell states\u001b[39m\n",
       "\n",
       "  \u001b[36mRNN\u001b[39m returns a callable RNN object \u001b[36mrnn\u001b[39m. Given a minibatch of\n",
       "  sequences \u001b[36mx\u001b[39m, \u001b[36mrnn(x)\u001b[39m returns \u001b[36my\u001b[39m, the hidden states of the final layer\n",
       "  for each time step. \u001b[36mrnn.h\u001b[39m and \u001b[36mrnn.c\u001b[39m fields can be used to set the\n",
       "  initial hidden states and read the final hidden states of all\n",
       "  layers. Note that the final time step of \u001b[36my\u001b[39m always contains the final\n",
       "  hidden state of the last layer, equivalent to \u001b[36mrnn.h\u001b[39m for a single\n",
       "  layer network.\n",
       "\n",
       "  \u001b[1mDimensions:\u001b[22m The input \u001b[36mx\u001b[39m can be 1, 2, or 3 dimensional and \u001b[36my\u001b[39m will\n",
       "  have the same number of dimensions as \u001b[36mx\u001b[39m. size(x)=(X,[B,T]) and\n",
       "  size(y)=(H/2H,[B,T]) where X is inputSize, B is batchSize, T is\n",
       "  seqLength, H is hiddenSize, 2H is for bidirectional RNNs. By default\n",
       "  a 1-D \u001b[36mx\u001b[39m represents a single instance for a single time step, a 2-D \u001b[36mx\u001b[39m\n",
       "  represents a single minibatch for a single time step, and a 3-D \u001b[36mx\u001b[39m\n",
       "  represents a sequence of identically sized minibatches for multiple\n",
       "  time steps. The output \u001b[36my\u001b[39m gives the hidden state (of the final layer\n",
       "  for multi-layer RNNs) for each time step. The fields \u001b[36mrnn.h\u001b[39m and \u001b[36mrnn.c\u001b[39m\n",
       "  represent the hidden states of all layers in a single time step and\n",
       "  have size (H,B,L/2L) where L is numLayers and 2L is for\n",
       "  bidirectional RNNs.\n",
       "\n",
       "  \u001b[1mbatchSizes:\u001b[22m If \u001b[36mbatchSizes=nothing\u001b[39m (default), all sequences in a\n",
       "  minibatch are assumed to be the same length. If \u001b[36mbatchSizes\u001b[39m is an\n",
       "  array of (non-increasing) integers, it gives us the batch size for\n",
       "  each time step (allowing different sequences in the minibatch to\n",
       "  have different lengths). In this case \u001b[36mx\u001b[39m will typically be 2-D with\n",
       "  the second dimension representing variable size batches for time\n",
       "  steps. If \u001b[36mbatchSizes\u001b[39m is used, \u001b[36msum(batchSizes)\u001b[39m should equal \u001b[36mlength(x)\n",
       "  ÷ size(x,1)\u001b[39m. When the batch size is different in every time step,\n",
       "  hidden states will have size (H,B,L/2L) where B is always the size\n",
       "  of the first (largest) minibatch.\n",
       "\n",
       "  \u001b[1mHidden states:\u001b[22m The hidden and cell states are kept in \u001b[36mrnn.h\u001b[39m and\n",
       "  \u001b[36mrnn.c\u001b[39m fields (the cell state is only used by LSTM). They can be\n",
       "  initialized during construction using the \u001b[36mh\u001b[39m and \u001b[36mc\u001b[39m keyword arguments,\n",
       "  or modified later by direct assignment. Valid values are \u001b[36mnothing\u001b[39m\n",
       "  (default), \u001b[36m0\u001b[39m, or an array of the right type and size possibly\n",
       "  wrapped in a \u001b[36mParam\u001b[39m. If the value is \u001b[36mnothing\u001b[39m the initial state is\n",
       "  assumed to be zero and the final state is discarded keeping the\n",
       "  value \u001b[36mnothing\u001b[39m. If the value is \u001b[36m0\u001b[39m the initial state is assumed to be\n",
       "  zero and \u001b[36m0\u001b[39m is replaced by the final state on return. If the value is\n",
       "  a valid state, it is used as the initial state and is replaced by\n",
       "  the final state on return.\n",
       "\n",
       "  In a differentiation context the returned final hidden states will\n",
       "  be wrapped in \u001b[36mResult\u001b[39m types. This is necessary if the same RNN object\n",
       "  is to be called multiple times in a single iteration. Between\n",
       "  iterations (i.e. after diff/update) the hidden states need to be\n",
       "  unboxed with e.g. \u001b[36mrnn.h = value(rnn.h)\u001b[39m to prevent spurious\n",
       "  dependencies. This happens automatically during the backward pass\n",
       "  for GPU RNNs but needs to be done manually for CPU RNNs. See the\n",
       "  CharLM Tutorial\n",
       "  (https://github.com/denizyuret/Knet.jl/blob/master/tutorial/80.charlm.ipynb)\n",
       "  for an example.\n",
       "\n",
       "  \u001b[1mKeyword arguments for RNN:\u001b[22m\n",
       "\n",
       "    •    \u001b[36mh=nothing\u001b[39m: Initial hidden state.\n",
       "\n",
       "    •    \u001b[36mc=nothing\u001b[39m: Initial cell state.\n",
       "\n",
       "    •    \u001b[36mrnnType=:lstm\u001b[39m Type of RNN: One of :relu, :tanh, :lstm,\n",
       "        :gru.\n",
       "\n",
       "    •    \u001b[36mnumLayers=1\u001b[39m: Number of RNN layers.\n",
       "\n",
       "    •    \u001b[36mbidirectional=false\u001b[39m: Create a bidirectional RNN if \u001b[36mtrue\u001b[39m.\n",
       "\n",
       "    •    \u001b[36mdropout=0\u001b[39m: Dropout probability. Applied to input and\n",
       "        between layers.\n",
       "\n",
       "    •    \u001b[36mskipInput=false\u001b[39m: Do not multiply the input with a matrix\n",
       "        if \u001b[36mtrue\u001b[39m.\n",
       "\n",
       "    •    \u001b[36mdataType=Float32\u001b[39m: Data type to use for weights.\n",
       "\n",
       "    •    \u001b[36malgo=0\u001b[39m: Algorithm to use, see CUDNN docs for details.\n",
       "\n",
       "    •    \u001b[36mseed=0\u001b[39m: Random number seed for dropout. Uses \u001b[36mtime()\u001b[39m if 0.\n",
       "\n",
       "    •    \u001b[36mwinit=xavier\u001b[39m: Weight initialization method for matrices.\n",
       "\n",
       "    •    \u001b[36mbinit=zeros\u001b[39m: Weight initialization method for bias\n",
       "        vectors.\n",
       "\n",
       "    •    \u001b[36musegpu=(gpu()>=0)\u001b[39m: GPU used by default if one exists.\n",
       "\n",
       "  \u001b[1mFormulas:\u001b[22m RNNs compute the output h[t] for a given iteration from\n",
       "  the recurrent input h[t-1] and the previous layer input x[t] given\n",
       "  matrices W, R and biases bW, bR from the following equations:\n",
       "\n",
       "  \u001b[36m:relu\u001b[39m and \u001b[36m:tanh\u001b[39m: Single gate RNN with activation function f:\n",
       "\n",
       "\u001b[36m  h[t] = f(W * x[t] .+ R * h[t-1] .+ bW .+ bR)\u001b[39m\n",
       "\n",
       "  \u001b[36m:gru\u001b[39m: Gated recurrent unit:\n",
       "\n",
       "\u001b[36m  i[t] = sigm(Wi * x[t] .+ Ri * h[t-1] .+ bWi .+ bRi) # input gate\u001b[39m\n",
       "\u001b[36m  r[t] = sigm(Wr * x[t] .+ Rr * h[t-1] .+ bWr .+ bRr) # reset gate\u001b[39m\n",
       "\u001b[36m  n[t] = tanh(Wn * x[t] .+ r[t] .* (Rn * h[t-1] .+ bRn) .+ bWn) # new gate\u001b[39m\n",
       "\u001b[36m  h[t] = (1 - i[t]) .* n[t] .+ i[t] .* h[t-1]\u001b[39m\n",
       "\n",
       "  \u001b[36m:lstm\u001b[39m: Long short term memory unit with no peephole connections:\n",
       "\n",
       "\u001b[36m  i[t] = sigm(Wi * x[t] .+ Ri * h[t-1] .+ bWi .+ bRi) # input gate\u001b[39m\n",
       "\u001b[36m  f[t] = sigm(Wf * x[t] .+ Rf * h[t-1] .+ bWf .+ bRf) # forget gate\u001b[39m\n",
       "\u001b[36m  o[t] = sigm(Wo * x[t] .+ Ro * h[t-1] .+ bWo .+ bRo) # output gate\u001b[39m\n",
       "\u001b[36m  n[t] = tanh(Wn * x[t] .+ Rn * h[t-1] .+ bWn .+ bRn) # new gate\u001b[39m\n",
       "\u001b[36m  c[t] = f[t] .* c[t-1] .+ i[t] .* n[t]               # cell output\u001b[39m\n",
       "\u001b[36m  h[t] = o[t] .* tanh(c[t])\u001b[39m"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@doc RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct BatchIter; mat; end;\n",
    "function Base.iterate(bmat::BatchIter, s...)\n",
    "    s === nothing && return bmat.mat[]\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "struct Biaff; mlphead; mlpdep; w; b; end;\n",
    "#Biaff(mlph::MLP, mlpd::MLP, input::Int, output::Int)=Biaff(mlph, mlpd , param(output,input), param0(output))\n",
    "Biaff(rhid::Int, mlph::Int, m::Int, input::Int, output::Int) =\n",
    "    Biaff(MLP(rhid,mlph, m) , MLP(rhid,mlph, m) , param(m,m;atype=Array{Float64}), param0(m;atype=Array{Float64}))\n",
    "\n",
    "(bi::Biaff)(x) = begin\n",
    "    xp = permutedims(x, [3, 1, 2])\n",
    "    hidsize = size(x)[1]\n",
    "    T = size(x)[ndims(x)]\n",
    "    #x = reshape(x, hidsize, T)\n",
    "    archead = bi.mlphead(x)\n",
    "    arcdep = bi.mlpdep(x[:,:,2:end])\n",
    "    archeadp = permutedims(reshape(archead, size(archead,1), size(x,2), size(x,3)), [3, 1, 2])\n",
    "    arcdepp = permutedims(reshape(arcdep, size(arcdep,1), size(x,2), size(x,3)-1), [1, 3, 2])\n",
    "    HW = batchmult(archeadp,bi.w)\n",
    "    HWH = batchmult(HW, arcdepp)\n",
    "    Hb = batchmult(archeadp,bi.b)\n",
    "    S = HWH .+ Hb    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "param(array; atype)\n",
       "param(dims...; init, atype)\n",
       "param0(dims...; atype)\n",
       "\\end{verbatim}\n",
       "The first form returns \\texttt{Param(atype(array))} where \\texttt{atype=identity} is the default.\n",
       "\n",
       "The second form Returns a randomly initialized \\texttt{Param(atype(init(dims...)))}. By default, \\texttt{init} is \\texttt{xavier} and \\texttt{atype} is \\texttt{KnetArray\\{Float32\\}} if \\texttt{gpu() >= 0}, \\texttt{Array\\{Float32\\}} otherwise. \n",
       "\n",
       "The third form \\texttt{param0} is an alias for \\texttt{param(dims...; init=zeros)}.\n",
       "\n"
      ],
      "text/markdown": [
       "```\n",
       "param(array; atype)\n",
       "param(dims...; init, atype)\n",
       "param0(dims...; atype)\n",
       "```\n",
       "\n",
       "The first form returns `Param(atype(array))` where `atype=identity` is the default.\n",
       "\n",
       "The second form Returns a randomly initialized `Param(atype(init(dims...)))`. By default, `init` is `xavier` and `atype` is `KnetArray{Float32}` if `gpu() >= 0`, `Array{Float32}` otherwise. \n",
       "\n",
       "The third form `param0` is an alias for `param(dims...; init=zeros)`.\n"
      ],
      "text/plain": [
       "\u001b[36m  param(array; atype)\u001b[39m\n",
       "\u001b[36m  param(dims...; init, atype)\u001b[39m\n",
       "\u001b[36m  param0(dims...; atype)\u001b[39m\n",
       "\n",
       "  The first form returns \u001b[36mParam(atype(array))\u001b[39m where \u001b[36matype=identity\u001b[39m is\n",
       "  the default.\n",
       "\n",
       "  The second form Returns a randomly initialized\n",
       "  \u001b[36mParam(atype(init(dims...)))\u001b[39m. By default, \u001b[36minit\u001b[39m is \u001b[36mxavier\u001b[39m and \u001b[36matype\u001b[39m is\n",
       "  \u001b[36mKnetArray{Float32}\u001b[39m if \u001b[36mgpu() >= 0\u001b[39m, \u001b[36mArray{Float32}\u001b[39m otherwise. \n",
       "\n",
       "  The third form \u001b[36mparam0\u001b[39m is an alias for \u001b[36mparam(dims...; init=zeros)\u001b[39m."
      ]
     },
     "execution_count": 629,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@doc param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0; 0 0 0; 0 0 1]"
     ]
    }
   ],
   "source": [
    "print(s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Array{Int64,2}:\n",
       " 1  1  0\n",
       " 0  0  0\n",
       " 0  0  1"
      ]
     },
     "execution_count": 511,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = [1 1 0;\n",
    "      0 0 0;\n",
    "      0 0 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Int64,1}:\n",
       " 1\n",
       " 2\n",
       " 1"
      ]
     },
     "execution_count": 527,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1 = [1; 2; 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2181113805987176"
      ]
     },
     "execution_count": 528,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Knet.nll(s1,a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "map(f, c...) -> collection\n",
       "\\end{verbatim}\n",
       "Transform collection \\texttt{c} by applying \\texttt{f} to each element. For multiple collection arguments, apply \\texttt{f} elementwise.\n",
       "\n",
       "See also: \\href{@ref}{\\texttt{mapslices}}\n",
       "\n",
       "\\section{Examples}\n",
       "\\begin{verbatim}\n",
       "julia> map(x -> x * 2, [1, 2, 3])\n",
       "3-element Array{Int64,1}:\n",
       " 2\n",
       " 4\n",
       " 6\n",
       "\n",
       "julia> map(+, [1, 2, 3], [10, 20, 30])\n",
       "3-element Array{Int64,1}:\n",
       " 11\n",
       " 22\n",
       " 33\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "```\n",
       "map(f, c...) -> collection\n",
       "```\n",
       "\n",
       "Transform collection `c` by applying `f` to each element. For multiple collection arguments, apply `f` elementwise.\n",
       "\n",
       "See also: [`mapslices`](@ref)\n",
       "\n",
       "# Examples\n",
       "\n",
       "```jldoctest\n",
       "julia> map(x -> x * 2, [1, 2, 3])\n",
       "3-element Array{Int64,1}:\n",
       " 2\n",
       " 4\n",
       " 6\n",
       "\n",
       "julia> map(+, [1, 2, 3], [10, 20, 30])\n",
       "3-element Array{Int64,1}:\n",
       " 11\n",
       " 22\n",
       " 33\n",
       "```\n"
      ],
      "text/plain": [
       "\u001b[36m  map(f, c...) -> collection\u001b[39m\n",
       "\n",
       "  Transform collection \u001b[36mc\u001b[39m by applying \u001b[36mf\u001b[39m to each element. For multiple\n",
       "  collection arguments, apply \u001b[36mf\u001b[39m elementwise.\n",
       "\n",
       "  See also: \u001b[36mmapslices\u001b[39m\n",
       "\n",
       "\u001b[1m  Examples\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "\u001b[36m  julia> map(x -> x * 2, [1, 2, 3])\u001b[39m\n",
       "\u001b[36m  3-element Array{Int64,1}:\u001b[39m\n",
       "\u001b[36m   2\u001b[39m\n",
       "\u001b[36m   4\u001b[39m\n",
       "\u001b[36m   6\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> map(+, [1, 2, 3], [10, 20, 30])\u001b[39m\n",
       "\u001b[36m  3-element Array{Int64,1}:\u001b[39m\n",
       "\u001b[36m   11\u001b[39m\n",
       "\u001b[36m   22\u001b[39m\n",
       "\u001b[36m   33\u001b[39m"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@doc map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array[3,2,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3×3 reshape(::UnitRange{Int64}, 3, 3, 3) with eltype Int64:\n",
       "[:, :, 1] =\n",
       " 1  4  7\n",
       " 2  5  8\n",
       " 3  6  9\n",
       "\n",
       "[:, :, 2] =\n",
       " 10  13  16\n",
       " 11  14  17\n",
       " 12  15  18\n",
       "\n",
       "[:, :, 3] =\n",
       " 19  22  25\n",
       " 20  23  26\n",
       " 21  24  27"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array = reshape(1:27,3,3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 reshape(::UnitRange{Int64}, 3, 3) with eltype Int64:\n",
       " 1  4  7\n",
       " 2  5  8\n",
       " 3  6  9"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array2 = reshape(1:9,3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "view(A, inds...)\n",
       "\\end{verbatim}\n",
       "Like \\href{@ref}{\\texttt{getindex}}, but returns a view into the parent array \\texttt{A} with the given indices instead of making a copy.  Calling \\href{@ref}{\\texttt{getindex}} or \\href{@ref}{\\texttt{setindex!}} on the returned \\texttt{SubArray} computes the indices to the parent array on the fly without checking bounds.\n",
       "\n",
       "\\section{Examples}\n",
       "\\begin{verbatim}\n",
       "julia> A = [1 2; 3 4]\n",
       "2×2 Array{Int64,2}:\n",
       " 1  2\n",
       " 3  4\n",
       "\n",
       "julia> b = view(A, :, 1)\n",
       "2-element view(::Array{Int64,2}, :, 1) with eltype Int64:\n",
       " 1\n",
       " 3\n",
       "\n",
       "julia> fill!(b, 0)\n",
       "2-element view(::Array{Int64,2}, :, 1) with eltype Int64:\n",
       " 0\n",
       " 0\n",
       "\n",
       "julia> A # Note A has changed even though we modified b\n",
       "2×2 Array{Int64,2}:\n",
       " 0  2\n",
       " 0  4\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "```\n",
       "view(A, inds...)\n",
       "```\n",
       "\n",
       "Like [`getindex`](@ref), but returns a view into the parent array `A` with the given indices instead of making a copy.  Calling [`getindex`](@ref) or [`setindex!`](@ref) on the returned `SubArray` computes the indices to the parent array on the fly without checking bounds.\n",
       "\n",
       "# Examples\n",
       "\n",
       "```jldoctest\n",
       "julia> A = [1 2; 3 4]\n",
       "2×2 Array{Int64,2}:\n",
       " 1  2\n",
       " 3  4\n",
       "\n",
       "julia> b = view(A, :, 1)\n",
       "2-element view(::Array{Int64,2}, :, 1) with eltype Int64:\n",
       " 1\n",
       " 3\n",
       "\n",
       "julia> fill!(b, 0)\n",
       "2-element view(::Array{Int64,2}, :, 1) with eltype Int64:\n",
       " 0\n",
       " 0\n",
       "\n",
       "julia> A # Note A has changed even though we modified b\n",
       "2×2 Array{Int64,2}:\n",
       " 0  2\n",
       " 0  4\n",
       "```\n"
      ],
      "text/plain": [
       "\u001b[36m  view(A, inds...)\u001b[39m\n",
       "\n",
       "  Like \u001b[36mgetindex\u001b[39m, but returns a view into the parent array \u001b[36mA\u001b[39m with the\n",
       "  given indices instead of making a copy. Calling \u001b[36mgetindex\u001b[39m or\n",
       "  \u001b[36msetindex!\u001b[39m on the returned \u001b[36mSubArray\u001b[39m computes the indices to the\n",
       "  parent array on the fly without checking bounds.\n",
       "\n",
       "\u001b[1m  Examples\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "\u001b[36m  julia> A = [1 2; 3 4]\u001b[39m\n",
       "\u001b[36m  2×2 Array{Int64,2}:\u001b[39m\n",
       "\u001b[36m   1  2\u001b[39m\n",
       "\u001b[36m   3  4\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> b = view(A, :, 1)\u001b[39m\n",
       "\u001b[36m  2-element view(::Array{Int64,2}, :, 1) with eltype Int64:\u001b[39m\n",
       "\u001b[36m   1\u001b[39m\n",
       "\u001b[36m   3\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> fill!(b, 0)\u001b[39m\n",
       "\u001b[36m  2-element view(::Array{Int64,2}, :, 1) with eltype Int64:\u001b[39m\n",
       "\u001b[36m   0\u001b[39m\n",
       "\u001b[36m   0\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> A # Note A has changed even though we modified b\u001b[39m\n",
       "\u001b[36m  2×2 Array{Int64,2}:\u001b[39m\n",
       "\u001b[36m   0  2\u001b[39m\n",
       "\u001b[36m   0  4\u001b[39m"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@doc view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Array{Int64,2}:\n",
       " 2  2  2\n",
       " 2  2  2\n",
       " 2  2  2"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wmat = [2 2 2; 2 2 2; 2 2 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Array{Int64,2}:\n",
       " 24  24  24\n",
       " 30  30  30\n",
       " 36  36  36"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array2 * wmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: i not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: i not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[147]:1"
     ]
    }
   ],
   "source": [
    "(array2*wmat for i in array2[:,:, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×3 Array{Int64,2}:\n",
       " 4  5  6"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [1 2 3]\n",
    "y = [4 5 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Any,1}:\n",
       " [1 2 3]\n",
       " [4 5 6]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = []\n",
    "push!(a,x)\n",
    "push!(a,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×1 Array{Any,2}:\n",
       " [1 2 3]\n",
       " [4 5 6]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat(a, dims=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Array{Int64,2},1}:\n",
       " [12 30 48; 12 30 48; 12 30 48]         \n",
       " [66 84 102; 66 84 102; 66 84 102]      \n",
       " [120 138 156; 120 138 156; 120 138 156]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[wmat*array[:,:,i] for i in 1:size(array,3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3×3 Array{Int64,3}:\n",
       "[:, :, 1] =\n",
       " 12  30  48\n",
       " 12  30  48\n",
       " 12  30  48\n",
       "\n",
       "[:, :, 2] =\n",
       " 66  84  102\n",
       " 66  84  102\n",
       " 66  84  102\n",
       "\n",
       "[:, :, 3] =\n",
       " 120  138  156\n",
       " 120  138  156\n",
       " 120  138  156"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat([wmat*array[:,:,i] for i in 1:size(array,3)]..., dims=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "batchmult (generic function with 3 methods)"
      ]
     },
     "execution_count": 499,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function batchmult(w,x;dim=3)\n",
    "    xb = ndims(x)>=3\n",
    "    wb = ndims(w)>=3\n",
    "    if(wb && xb)\n",
    "        return cat(collect([w[:,:,i]*x[:,:,i] for i in 1:size(x,dim)])..., dims=dim)\n",
    "    elseif(wb)\n",
    "        return cat(collect([w[:,:,i]*x for i in 1:size(x,dim)])..., dims=dim)\n",
    "    elseif(xb)\n",
    "        return cat(collect([w*x[:,:,i] for i in 1:size(x,dim)])..., dims=dim)\n",
    "    else\n",
    "        return wb*xb\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 495,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndims(wmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3×3 Array{Int64,3}:\n",
       "[:, :, 1] =\n",
       " 30  66  102\n",
       " 36  81  126\n",
       " 42  96  150\n",
       "\n",
       "[:, :, 2] =\n",
       " 435  552  669\n",
       " 468  594  720\n",
       " 501  636  771\n",
       "\n",
       "[:, :, 3] =\n",
       " 1326  1524  1722\n",
       " 1386  1593  1800\n",
       " 1446  1662  1878"
      ]
     },
     "execution_count": 502,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batchmult(array,array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3×3 Array{Int64,3}:\n",
       "[:, :, 1] =\n",
       " 2   8  14\n",
       " 4  10  16\n",
       " 6  12  18\n",
       "\n",
       "[:, :, 2] =\n",
       " 20  26  32\n",
       " 22  28  34\n",
       " 24  30  36\n",
       "\n",
       "[:, :, 3] =\n",
       " 38  44  50\n",
       " 40  46  52\n",
       " 42  48  54"
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wmat.*array[:,:,1:size(array,3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "ename": "MethodError",
     "evalue": "MethodError: no method matching getindex(::Int64, ::Colon, ::Colon, ::Int64)\nClosest candidates are:\n  getindex(::Number, !Matched::Integer...) at number.jl:82\n  getindex(::Number) at number.jl:75\n  getindex(::Number, !Matched::Integer) at number.jl:77\n  ...",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching getindex(::Int64, ::Colon, ::Colon, ::Int64)\nClosest candidates are:\n  getindex(::Number, !Matched::Integer...) at number.jl:82\n  getindex(::Number) at number.jl:75\n  getindex(::Number, !Matched::Integer) at number.jl:77\n  ...",
      "",
      "Stacktrace:",
      " [1] (::getfield(Main, Symbol(\"##141#142\")))(::Tuple{Int64,Int64}) at .\\none:0",
      " [2] iterate at .\\generator.jl:47 [inlined]",
      " [3] collect(::Base.Generator{Base.Iterators.ProductIterator{Tuple{Base.ReshapedArray{Int64,3,UnitRange{Int64},Tuple{}},UnitRange{Int64}}},getfield(Main, Symbol(\"##141#142\"))}) at .\\array.jl:606",
      " [4] top-level scope at In[477]:1"
     ]
    }
   ],
   "source": [
    "[w*x[:,:,i] for x in array, i in 1:size(x,3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "MethodError",
     "evalue": "MethodError: no method matching *(::Array{Int64,3}, ::Array{Int64,3})\nClosest candidates are:\n  *(::Any, ::Any, !Matched::Any, !Matched::Any...) at operators.jl:502\n  *(!Matched::Number, ::AbstractArray) at arraymath.jl:52\n  *(::AbstractArray, !Matched::Number) at arraymath.jl:55\n  ...",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching *(::Array{Int64,3}, ::Array{Int64,3})\nClosest candidates are:\n  *(::Any, ::Any, !Matched::Any, !Matched::Any...) at operators.jl:502\n  *(!Matched::Number, ::AbstractArray) at arraymath.jl:52\n  *(::AbstractArray, !Matched::Number) at arraymath.jl:55\n  ...",
      "",
      "Stacktrace:",
      " [1] (::getfield(Main, Symbol(\"##147#148\")){Array{Int64,2},Base.ReshapedArray{Int64,3,UnitRange{Int64},Tuple{}}})(::Int64) at .\\none:0",
      " [2] iterate at .\\generator.jl:47 [inlined]",
      " [3] collect at .\\array.jl:606 [inlined]",
      " [4] #batchmult#146(::Int64, ::Function, ::Array{Int64,2}, ::Base.ReshapedArray{Int64,3,UnitRange{Int64},Tuple{}}) at .\\In[480]:2",
      " [5] batchmult(::Array{Int64,2}, ::Base.ReshapedArray{Int64,3,UnitRange{Int64},Tuple{}}) at .\\In[480]:2",
      " [6] top-level scope at In[482]:1"
     ]
    }
   ],
   "source": [
    "batchmult(wmat,array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Array{Int64,2}:\n",
       " 2  2  2\n",
       " 2  2  2\n",
       " 2  2  2"
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3×3 reshape(::UnitRange{Int64}, 3, 3, 3) with eltype Int64:\n",
       "[:, :, 1] =\n",
       " 1  4  7\n",
       " 2  5  8\n",
       " 3  6  9\n",
       "\n",
       "[:, :, 2] =\n",
       " 10  13  16\n",
       " 11  14  17\n",
       " 12  15  18\n",
       "\n",
       "[:, :, 3] =\n",
       " 19  22  25\n",
       " 20  23  26\n",
       " 21  24  27"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size(array,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "MethodError",
     "evalue": "MethodError: no method matching (::getfield(Main, Symbol(\"##35#36\")))(::Base.ReshapedArray{Int64,2,UnitRange{Int64},Tuple{}})\nClosest candidates are:\n  #35(::Any, !Matched::Any) at In[156]:1",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching (::getfield(Main, Symbol(\"##35#36\")))(::Base.ReshapedArray{Int64,2,UnitRange{Int64},Tuple{}})\nClosest candidates are:\n  #35(::Any, !Matched::Any) at In[156]:1",
      "",
      "Stacktrace:",
      " [1] map(::getfield(Main, Symbol(\"##35#36\")), ::Tuple{Base.ReshapedArray{Int64,2,UnitRange{Int64},Tuple{}},Array{Int64,2}}) at .\\tuple.jl:166",
      " [2] top-level scope at In[156]:1"
     ]
    }
   ],
   "source": [
    "asdres = map((x,y)->(x[1]*x[2]), (array2,wmat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "No documentation found.\n",
       "\n",
       "Binding \\texttt{LinearAlgebra.eye} does not exist.\n",
       "\n"
      ],
      "text/markdown": [
       "No documentation found.\n",
       "\n",
       "Binding `LinearAlgebra.eye` does not exist.\n"
      ],
      "text/plain": [
       "  No documentation found.\n",
       "\n",
       "  Binding \u001b[36mLinearAlgebra.eye\u001b[39m does not exist."
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@doc LinearAlgebra.eye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×5 Array{Float64,2}:\n",
       " 0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = zeros(3,2)\n",
    "b = zeros(2,5)\n",
    "a*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: ita2 not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: ita2 not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[159]:1"
     ]
    }
   ],
   "source": [
    "a,s = iterate(ita2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "nll(scores, answers; dims=1, average=true)\n",
       "\\end{verbatim}\n",
       "Given an unnormalized \\texttt{scores} matrix and an \\texttt{Integer} array of correct \\texttt{answers}, return the per-instance negative log likelihood. \\texttt{dims=1} means instances are in columns, \\texttt{dims=2} means instances are in rows.  Use \\texttt{average=false} to return the sum instead of per-instance average.\n",
       "\n",
       "\\begin{verbatim}\n",
       "nll(model, data; dims=1, average=true, o...)\n",
       "\\end{verbatim}\n",
       "Compute \\texttt{nll(model(x; o...), y; dims)} for \\texttt{(x,y)} in \\texttt{data} and return the per-instance average (if average=true) or total (if average=false) negative log likelihood.\n",
       "\n"
      ],
      "text/markdown": [
       "```\n",
       "nll(scores, answers; dims=1, average=true)\n",
       "```\n",
       "\n",
       "Given an unnormalized `scores` matrix and an `Integer` array of correct `answers`, return the per-instance negative log likelihood. `dims=1` means instances are in columns, `dims=2` means instances are in rows.  Use `average=false` to return the sum instead of per-instance average.\n",
       "\n",
       "```\n",
       "nll(model, data; dims=1, average=true, o...)\n",
       "```\n",
       "\n",
       "Compute `nll(model(x; o...), y; dims)` for `(x,y)` in `data` and return the per-instance average (if average=true) or total (if average=false) negative log likelihood.\n"
      ],
      "text/plain": [
       "\u001b[36m  nll(scores, answers; dims=1, average=true)\u001b[39m\n",
       "\n",
       "  Given an unnormalized \u001b[36mscores\u001b[39m matrix and an \u001b[36mInteger\u001b[39m array of correct\n",
       "  \u001b[36manswers\u001b[39m, return the per-instance negative log likelihood. \u001b[36mdims=1\u001b[39m\n",
       "  means instances are in columns, \u001b[36mdims=2\u001b[39m means instances are in rows.\n",
       "  Use \u001b[36maverage=false\u001b[39m to return the sum instead of per-instance average.\n",
       "\n",
       "\u001b[36m  nll(model, data; dims=1, average=true, o...)\u001b[39m\n",
       "\n",
       "  Compute \u001b[36mnll(model(x; o...), y; dims)\u001b[39m for \u001b[36m(x,y)\u001b[39m in \u001b[36mdata\u001b[39m and return\n",
       "  the per-instance average (if average=true) or total (if\n",
       "  average=false) negative log likelihood."
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@doc Knet.nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct itarray\n",
    "    a\n",
    "end\n",
    "function Base.iterate(it::itarray, s...)\n",
    "    if s > length(it)\n",
    "        return nothing\n",
    "    else\n",
    "        return it.a[s]\n",
    "    end\n",
    "end\n",
    "Base.length(it::itarray) = length(it.a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Base.Generator{Base.ReshapedArray{Int64,3,UnitRange{Int64},Tuple{}},getfield(Main, Symbol(\"##37#38\"))}(getfield(Main, Symbol(\"##37#38\"))(), [1 4 7; 2 5 8; 3 6 9]\n",
       "\n",
       "[10 13 16; 11 14 17; 12 15 18]\n",
       "\n",
       "[19 22 25; 20 23 26; 21 24 27])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ita2 = (x for x in array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3×3 Array{Int64,3}:\n",
       "[:, :, 1] =\n",
       " 1  4  7\n",
       " 2  5  8\n",
       " 3  6  9\n",
       "\n",
       "[:, :, 2] =\n",
       " 10  13  16\n",
       " 11  14  17\n",
       " 12  15  18\n",
       "\n",
       "[:, :, 3] =\n",
       " 19  22  25\n",
       " 20  23  26\n",
       " 21  24  27"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a2 = [x for x in array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Base.Generator{Array{Any,1},getfield(Main, Symbol(\"##41#42\"))}(getfield(Main, Symbol(\"##41#42\"))(), Any[Any[\"Al\", \"-\", \"Zaman\", \":\", \"American\", \"forces\", \"killed\", \"Shaikh\", \"Abdullah\", \"al\"  …  \"the\", \"town\", \"of\", \"Qaim\", \",\", \"near\", \"the\", \"Syrian\", \"border\", \".\"], Any[\"[\", \"This\", \"killing\", \"of\", \"a\", \"respected\", \"cleric\", \"will\", \"be\", \"causing\", \"us\", \"trouble\", \"for\", \"years\", \"to\", \"come\", \".\", \"]\"], Any[\"DPA\", \":\", \"Iraqi\", \"authorities\", \"announced\", \"that\", \"they\", \"had\", \"busted\", \"up\", \"3\", \"terrorist\", \"cells\", \"operating\", \"in\", \"Baghdad\", \".\"], Any[\"Two\", \"of\", \"them\", \"were\", \"being\", \"run\", \"by\", \"2\", \"officials\", \"of\", \"the\", \"Ministry\", \"of\", \"the\", \"Interior\", \"!\"], Any[\"The\", \"MoI\", \"in\", \"Iraq\", \"is\", \"equivalent\", \"to\", \"the\", \"US\", \"FBI\"  …  \"members\", \"of\", \"the\", \"Weathermen\", \"bombers\", \"back\", \"in\", \"the\", \"1960s\", \".\"], Any[\"The\", \"third\", \"was\", \"being\", \"run\", \"by\", \"the\", \"head\", \"of\", \"an\", \"investment\", \"firm\", \".\"], Any[\"You\", \"wonder\", \"if\", \"he\", \"was\", \"manipulating\", \"the\", \"market\", \"with\", \"his\", \"bombing\", \"targets\", \".\"], Any[\"The\", \"cells\", \"were\", \"operating\", \"in\", \"the\", \"Ghazaliyah\", \"and\", \"al\", \"-\", \"Jihad\", \"districts\", \"of\", \"the\", \"capital\", \".\"], Any[\"Although\", \"the\", \"announcement\", \"was\", \"probably\", \"made\", \"to\", \"show\", \"progress\", \"in\"  …  \"Baathists\", \"continue\", \"to\", \"penetrate\", \"the\", \"Iraqi\", \"government\", \"very\", \"hopeful\", \".\"], Any[\"It\", \"reminds\", \"me\", \"too\", \"much\", \"of\", \"the\", \"ARVN\", \"officers\", \"who\", \"were\", \"secretly\", \"working\", \"for\", \"the\", \"other\", \"side\", \"in\", \"Vietnam\", \".\"]  …  Any[\"Over\", \"two\", \"hours\", \"later\", \"(\", \"and\", \"ten\", \"minutes\", \"before\", \"they\", \"closed\", \")\", \"my\", \"car\", \"was\", \"finally\", \"finished\", \".\"], Any[\"A\", \"few\", \"minutes\", \"after\", \"I\", \"left\", \",\", \"I\", \"was\", \"called\"  …  \"which\", \"they\", \"should\", \"have\", \"left\", \"in\", \"the\", \"car\", \")\", \".\"], Any[\"Of\", \"course\", \",\", \"they\", \"would\", \"be\", \"closing\", \"in\", \"5\", \"minutes\"  …  \"to\", \"hurry\", \"up\", \"or\", \"get\", \"it\", \"the\", \"next\", \"day\", \".\"], Any[\"Of\", \"course\", \"I\", \"could\", \"n't\", \"make\", \"it\", \"back\", \"in\", \"time\"  …  \"stay\", \"5\", \"extra\", \"minutes\", \"to\", \"wait\", \"for\", \"me\", \")\", \".\"], Any[\"The\", \"next\", \"day\", \",\", \"no\", \"one\", \"could\", \"find\", \"my\", \"wheel\", \"lock\", \"and\", \"that\", \"particular\", \"technician\", \"was\", \"not\", \"in\", \".\"], Any[\"Of\", \"course\", \",\", \"they\", \"could\", \"n't\", \"call\", \"him\", \"either\", \"to\"  …  \"no\", \"wheel\", \"lock\", \"should\", \"I\", \"get\", \"a\", \"flat\", \")\", \".\"], Any[\"On\", \"Monday\", \"I\", \"called\", \"and\", \"again\", \"it\", \"was\", \"a\", \"big\"  …  \"do\", \"to\", \"find\", \"anyone\", \"who\", \"knew\", \"anything\", \"about\", \"it\", \".\"], Any[\"Supposedly\", \"they\", \"will\", \"be\", \"holding\", \"it\", \"for\", \"me\", \"this\", \"evening\"  …  \"'m\", \"sure\", \"that\", \"will\", \"also\", \"be\", \"a\", \"huge\", \"ordeal\", \".\"], Any[\"The\", \"employees\", \"at\", \"this\", \"Sear's\", \"are\", \"completely\", \"apathetic\", \"and\", \"there\"  …  \"be\", \"any\", \"sort\", \"of\", \"management\", \"that\", \"I\", \"could\", \"see\", \".\"], Any[\"I\", \"will\", \"never\", \"return\", \"there\", \"again\", \"(\", \"and\", \"now\", \"have\"  …  \"of\", \"work\", \"they\", \"actually\", \"performed\", \"on\", \"my\", \"car\", \")\", \".\"]])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datxit = (x for x in datax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "minibatch(x, [y], batchsize; shuffle, partial, xtype, ytype, xsize, ysize)\n",
       "\\end{verbatim}\n",
       "Return an iterator of minibatches [(xi,yi)...] given data tensors x, y and batchsize.  \n",
       "\n",
       "The last dimension of x and y give the number of instances and should be equal. \\texttt{y} is optional, if omitted a sequence of \\texttt{xi} will be generated rather than \\texttt{(xi,yi)} tuples.  Use \\texttt{repeat(d,n)} for multiple epochs, \\texttt{Iterators.take(d,n)} for a partial epoch, and \\texttt{Iterators.cycle(d)} to cycle through the data forever (this can be used with \\texttt{converge}). If you need the iterator to continue from its last position when stopped early (e.g. by a break in a for loop), use \\texttt{Iterators.Stateful(d)} (by default the iterator would restart from the beginning).\n",
       "\n",
       "Keyword arguments:\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{shuffle=false}: Shuffle the instances every epoch.\n",
       "\n",
       "\n",
       "\\item \\texttt{partial=false}: If true include the last partial minibatch < batchsize.\n",
       "\n",
       "\n",
       "\\item \\texttt{xtype=typeof(x)}: Convert xi in minibatches to this type.\n",
       "\n",
       "\n",
       "\\item \\texttt{ytype=typeof(y)}: Convert yi in minibatches to this type.\n",
       "\n",
       "\n",
       "\\item \\texttt{xsize=size(x)}: Convert xi in minibatches to this shape.\n",
       "\n",
       "\n",
       "\\item \\texttt{ysize=size(y)}: Convert yi in minibatches to this shape.\n",
       "\n",
       "\\end{itemize}\n"
      ],
      "text/markdown": [
       "```\n",
       "minibatch(x, [y], batchsize; shuffle, partial, xtype, ytype, xsize, ysize)\n",
       "```\n",
       "\n",
       "Return an iterator of minibatches [(xi,yi)...] given data tensors x, y and batchsize.  \n",
       "\n",
       "The last dimension of x and y give the number of instances and should be equal. `y` is optional, if omitted a sequence of `xi` will be generated rather than `(xi,yi)` tuples.  Use `repeat(d,n)` for multiple epochs, `Iterators.take(d,n)` for a partial epoch, and `Iterators.cycle(d)` to cycle through the data forever (this can be used with `converge`). If you need the iterator to continue from its last position when stopped early (e.g. by a break in a for loop), use `Iterators.Stateful(d)` (by default the iterator would restart from the beginning).\n",
       "\n",
       "Keyword arguments:\n",
       "\n",
       "  * `shuffle=false`: Shuffle the instances every epoch.\n",
       "  * `partial=false`: If true include the last partial minibatch < batchsize.\n",
       "  * `xtype=typeof(x)`: Convert xi in minibatches to this type.\n",
       "  * `ytype=typeof(y)`: Convert yi in minibatches to this type.\n",
       "  * `xsize=size(x)`: Convert xi in minibatches to this shape.\n",
       "  * `ysize=size(y)`: Convert yi in minibatches to this shape.\n"
      ],
      "text/plain": [
       "\u001b[36m  minibatch(x, [y], batchsize; shuffle, partial, xtype, ytype, xsize, ysize)\u001b[39m\n",
       "\n",
       "  Return an iterator of minibatches [(xi,yi)...] given data tensors x,\n",
       "  y and batchsize. \n",
       "\n",
       "  The last dimension of x and y give the number of instances and\n",
       "  should be equal. \u001b[36my\u001b[39m is optional, if omitted a sequence of \u001b[36mxi\u001b[39m will be\n",
       "  generated rather than \u001b[36m(xi,yi)\u001b[39m tuples. Use \u001b[36mrepeat(d,n)\u001b[39m for multiple\n",
       "  epochs, \u001b[36mIterators.take(d,n)\u001b[39m for a partial epoch, and\n",
       "  \u001b[36mIterators.cycle(d)\u001b[39m to cycle through the data forever (this can be\n",
       "  used with \u001b[36mconverge\u001b[39m). If you need the iterator to continue from its\n",
       "  last position when stopped early (e.g. by a break in a for loop),\n",
       "  use \u001b[36mIterators.Stateful(d)\u001b[39m (by default the iterator would restart\n",
       "  from the beginning).\n",
       "\n",
       "  Keyword arguments:\n",
       "\n",
       "    •    \u001b[36mshuffle=false\u001b[39m: Shuffle the instances every epoch.\n",
       "\n",
       "    •    \u001b[36mpartial=false\u001b[39m: If true include the last partial minibatch\n",
       "        < batchsize.\n",
       "\n",
       "    •    \u001b[36mxtype=typeof(x)\u001b[39m: Convert xi in minibatches to this type.\n",
       "\n",
       "    •    \u001b[36mytype=typeof(y)\u001b[39m: Convert yi in minibatches to this type.\n",
       "\n",
       "    •    \u001b[36mxsize=size(x)\u001b[39m: Convert xi in minibatches to this shape.\n",
       "\n",
       "    •    \u001b[36mysize=size(y)\u001b[39m: Convert yi in minibatches to this shape."
      ]
     },
     "execution_count": 599,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@doc Knet.Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "repeat(A::AbstractArray, counts::Integer...)\n",
       "\\end{verbatim}\n",
       "Construct an array by repeating array \\texttt{A} a given number of times in each dimension, specified by \\texttt{counts}.\n",
       "\n",
       "\\section{Examples}\n",
       "\\begin{verbatim}\n",
       "julia> repeat([1, 2, 3], 2)\n",
       "6-element Array{Int64,1}:\n",
       " 1\n",
       " 2\n",
       " 3\n",
       " 1\n",
       " 2\n",
       " 3\n",
       "\n",
       "julia> repeat([1, 2, 3], 2, 3)\n",
       "6×3 Array{Int64,2}:\n",
       " 1  1  1\n",
       " 2  2  2\n",
       " 3  3  3\n",
       " 1  1  1\n",
       " 2  2  2\n",
       " 3  3  3\n",
       "\\end{verbatim}\n",
       "\\begin{verbatim}\n",
       "repeat(A::AbstractArray; inner=ntuple(x->1, ndims(A)), outer=ntuple(x->1, ndims(A)))\n",
       "\\end{verbatim}\n",
       "Construct an array by repeating the entries of \\texttt{A}. The i-th element of \\texttt{inner} specifies the number of times that the individual entries of the i-th dimension of \\texttt{A} should be repeated. The i-th element of \\texttt{outer} specifies the number of times that a slice along the i-th dimension of \\texttt{A} should be repeated. If \\texttt{inner} or \\texttt{outer} are omitted, no repetition is performed.\n",
       "\n",
       "\\section{Examples}\n",
       "\\begin{verbatim}\n",
       "julia> repeat(1:2, inner=2)\n",
       "4-element Array{Int64,1}:\n",
       " 1\n",
       " 1\n",
       " 2\n",
       " 2\n",
       "\n",
       "julia> repeat(1:2, outer=2)\n",
       "4-element Array{Int64,1}:\n",
       " 1\n",
       " 2\n",
       " 1\n",
       " 2\n",
       "\n",
       "julia> repeat([1 2; 3 4], inner=(2, 1), outer=(1, 3))\n",
       "4×6 Array{Int64,2}:\n",
       " 1  2  1  2  1  2\n",
       " 1  2  1  2  1  2\n",
       " 3  4  3  4  3  4\n",
       " 3  4  3  4  3  4\n",
       "\\end{verbatim}\n",
       "\\begin{verbatim}\n",
       "repeat(s::AbstractString, r::Integer)\n",
       "\\end{verbatim}\n",
       "Repeat a string \\texttt{r} times. This can be written as \\texttt{s\\^{}r}.\n",
       "\n",
       "See also: \\href{@ref}{\\texttt{\\^{}}}\n",
       "\n",
       "\\section{Examples}\n",
       "\\begin{verbatim}\n",
       "julia> repeat(\"ha\", 3)\n",
       "\"hahaha\"\n",
       "\\end{verbatim}\n",
       "\\begin{verbatim}\n",
       "repeat(c::AbstractChar, r::Integer) -> String\n",
       "\\end{verbatim}\n",
       "Repeat a character \\texttt{r} times. This can equivalently be accomplished by calling \\href{@ref ^}{\\texttt{c\\^{}r}}.\n",
       "\n",
       "\\section{Examples}\n",
       "\\begin{verbatim}\n",
       "julia> repeat('A', 3)\n",
       "\"AAA\"\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "```\n",
       "repeat(A::AbstractArray, counts::Integer...)\n",
       "```\n",
       "\n",
       "Construct an array by repeating array `A` a given number of times in each dimension, specified by `counts`.\n",
       "\n",
       "# Examples\n",
       "\n",
       "```jldoctest\n",
       "julia> repeat([1, 2, 3], 2)\n",
       "6-element Array{Int64,1}:\n",
       " 1\n",
       " 2\n",
       " 3\n",
       " 1\n",
       " 2\n",
       " 3\n",
       "\n",
       "julia> repeat([1, 2, 3], 2, 3)\n",
       "6×3 Array{Int64,2}:\n",
       " 1  1  1\n",
       " 2  2  2\n",
       " 3  3  3\n",
       " 1  1  1\n",
       " 2  2  2\n",
       " 3  3  3\n",
       "```\n",
       "\n",
       "```\n",
       "repeat(A::AbstractArray; inner=ntuple(x->1, ndims(A)), outer=ntuple(x->1, ndims(A)))\n",
       "```\n",
       "\n",
       "Construct an array by repeating the entries of `A`. The i-th element of `inner` specifies the number of times that the individual entries of the i-th dimension of `A` should be repeated. The i-th element of `outer` specifies the number of times that a slice along the i-th dimension of `A` should be repeated. If `inner` or `outer` are omitted, no repetition is performed.\n",
       "\n",
       "# Examples\n",
       "\n",
       "```jldoctest\n",
       "julia> repeat(1:2, inner=2)\n",
       "4-element Array{Int64,1}:\n",
       " 1\n",
       " 1\n",
       " 2\n",
       " 2\n",
       "\n",
       "julia> repeat(1:2, outer=2)\n",
       "4-element Array{Int64,1}:\n",
       " 1\n",
       " 2\n",
       " 1\n",
       " 2\n",
       "\n",
       "julia> repeat([1 2; 3 4], inner=(2, 1), outer=(1, 3))\n",
       "4×6 Array{Int64,2}:\n",
       " 1  2  1  2  1  2\n",
       " 1  2  1  2  1  2\n",
       " 3  4  3  4  3  4\n",
       " 3  4  3  4  3  4\n",
       "```\n",
       "\n",
       "```\n",
       "repeat(s::AbstractString, r::Integer)\n",
       "```\n",
       "\n",
       "Repeat a string `r` times. This can be written as `s^r`.\n",
       "\n",
       "See also: [`^`](@ref)\n",
       "\n",
       "# Examples\n",
       "\n",
       "```jldoctest\n",
       "julia> repeat(\"ha\", 3)\n",
       "\"hahaha\"\n",
       "```\n",
       "\n",
       "```\n",
       "repeat(c::AbstractChar, r::Integer) -> String\n",
       "```\n",
       "\n",
       "Repeat a character `r` times. This can equivalently be accomplished by calling [`c^r`](@ref ^).\n",
       "\n",
       "# Examples\n",
       "\n",
       "```jldoctest\n",
       "julia> repeat('A', 3)\n",
       "\"AAA\"\n",
       "```\n"
      ],
      "text/plain": [
       "\u001b[36m  repeat(A::AbstractArray, counts::Integer...)\u001b[39m\n",
       "\n",
       "  Construct an array by repeating array \u001b[36mA\u001b[39m a given number of times in\n",
       "  each dimension, specified by \u001b[36mcounts\u001b[39m.\n",
       "\n",
       "\u001b[1m  Examples\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "\u001b[36m  julia> repeat([1, 2, 3], 2)\u001b[39m\n",
       "\u001b[36m  6-element Array{Int64,1}:\u001b[39m\n",
       "\u001b[36m   1\u001b[39m\n",
       "\u001b[36m   2\u001b[39m\n",
       "\u001b[36m   3\u001b[39m\n",
       "\u001b[36m   1\u001b[39m\n",
       "\u001b[36m   2\u001b[39m\n",
       "\u001b[36m   3\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> repeat([1, 2, 3], 2, 3)\u001b[39m\n",
       "\u001b[36m  6×3 Array{Int64,2}:\u001b[39m\n",
       "\u001b[36m   1  1  1\u001b[39m\n",
       "\u001b[36m   2  2  2\u001b[39m\n",
       "\u001b[36m   3  3  3\u001b[39m\n",
       "\u001b[36m   1  1  1\u001b[39m\n",
       "\u001b[36m   2  2  2\u001b[39m\n",
       "\u001b[36m   3  3  3\u001b[39m\n",
       "\n",
       "\u001b[36m  repeat(A::AbstractArray; inner=ntuple(x->1, ndims(A)), outer=ntuple(x->1, ndims(A)))\u001b[39m\n",
       "\n",
       "  Construct an array by repeating the entries of \u001b[36mA\u001b[39m. The i-th element\n",
       "  of \u001b[36minner\u001b[39m specifies the number of times that the individual entries\n",
       "  of the i-th dimension of \u001b[36mA\u001b[39m should be repeated. The i-th element of\n",
       "  \u001b[36mouter\u001b[39m specifies the number of times that a slice along the i-th\n",
       "  dimension of \u001b[36mA\u001b[39m should be repeated. If \u001b[36minner\u001b[39m or \u001b[36mouter\u001b[39m are omitted, no\n",
       "  repetition is performed.\n",
       "\n",
       "\u001b[1m  Examples\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "\u001b[36m  julia> repeat(1:2, inner=2)\u001b[39m\n",
       "\u001b[36m  4-element Array{Int64,1}:\u001b[39m\n",
       "\u001b[36m   1\u001b[39m\n",
       "\u001b[36m   1\u001b[39m\n",
       "\u001b[36m   2\u001b[39m\n",
       "\u001b[36m   2\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> repeat(1:2, outer=2)\u001b[39m\n",
       "\u001b[36m  4-element Array{Int64,1}:\u001b[39m\n",
       "\u001b[36m   1\u001b[39m\n",
       "\u001b[36m   2\u001b[39m\n",
       "\u001b[36m   1\u001b[39m\n",
       "\u001b[36m   2\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> repeat([1 2; 3 4], inner=(2, 1), outer=(1, 3))\u001b[39m\n",
       "\u001b[36m  4×6 Array{Int64,2}:\u001b[39m\n",
       "\u001b[36m   1  2  1  2  1  2\u001b[39m\n",
       "\u001b[36m   1  2  1  2  1  2\u001b[39m\n",
       "\u001b[36m   3  4  3  4  3  4\u001b[39m\n",
       "\u001b[36m   3  4  3  4  3  4\u001b[39m\n",
       "\n",
       "\u001b[36m  repeat(s::AbstractString, r::Integer)\u001b[39m\n",
       "\n",
       "  Repeat a string \u001b[36mr\u001b[39m times. This can be written as \u001b[36ms^r\u001b[39m.\n",
       "\n",
       "  See also: \u001b[36m^\u001b[39m\n",
       "\n",
       "\u001b[1m  Examples\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "\u001b[36m  julia> repeat(\"ha\", 3)\u001b[39m\n",
       "\u001b[36m  \"hahaha\"\u001b[39m\n",
       "\n",
       "\u001b[36m  repeat(c::AbstractChar, r::Integer) -> String\u001b[39m\n",
       "\n",
       "  Repeat a character \u001b[36mr\u001b[39m times. This can equivalently be accomplished by\n",
       "  calling \u001b[36mc^r\u001b[39m.\n",
       "\n",
       "\u001b[1m  Examples\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "\u001b[36m  julia> repeat('A', 3)\u001b[39m\n",
       "\u001b[36m  \"AAA\"\u001b[39m"
      ]
     },
     "execution_count": 601,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@doc Knet.repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12543-element Array{Tuple{Array{Float64,3},Array{Int64,1}},1}:\n",
       " ([-0.75898; -0.47426; … ; -0.014116; 0.6448]\n",
       "\n",
       "[0.54214; 1.0302; … ; 0.18317; 0.4338]\n",
       "\n",
       "[-0.16768; 1.2151; … ; 0.30893; 0.11023]\n",
       "\n",
       "...\n",
       "\n",
       "[0.12417; 1.0659; … ; -0.41358; -1.4449]\n",
       "\n",
       "[0.38784; -0.36196; … ; -0.41707; -1.1415]\n",
       "\n",
       "[0.15164; 0.30177; … ; 0.016413; 0.10216], [1, 2, 2, 2, 7, 8, 2, 8, 9, 9  …  22, 19, 24, 22, 22, 29, 29, 29, 22, 2])               \n",
       " ([-0.75898; -0.47426; … ; -0.014116; 0.6448]\n",
       "\n",
       "[-0.61201; 0.98226; … ; 0.30649; 0.81745]\n",
       "\n",
       "[0.53074; 0.40117; … ; 0.1444; 0.23611]\n",
       "\n",
       "...\n",
       "\n",
       "[0.5267; 0.11441; … ; -0.2983; 0.020482]\n",
       "\n",
       "[0.15164; 0.30177; … ; 0.016413; 0.10216]\n",
       "\n",
       "[-0.6297; 0.69044; … ; 0.25744; 1.0729], [11, 4, 11, 8, 8, 8, 4, 11, 11, 1, 11, 11, 15, 11, 17, 15, 11, 11])                      \n",
       " ([-0.75898; -0.47426; … ; -0.014116; 0.6448]\n",
       "\n",
       "[-0.15362; -0.49874; … ; 0.056634; -0.47371]\n",
       "\n",
       "[-0.17587; 1.3508; … ; 0.5665; 0.61385]\n",
       "\n",
       "...\n",
       "\n",
       "[0.33042; 0.24995; … ; -0.027273; -0.53285]\n",
       "\n",
       "[0.83281; -0.22828; … ; -0.41874; -0.25097]\n",
       "\n",
       "[0.15164; 0.30177; … ; 0.016413; 0.10216], [1, 2, 5, 6, 2, 10, 10, 10, 6, 10, 14, 14, 10, 14, 17, 15, 2])                 \n",
       " ([-0.75898; -0.47426; … ; -0.014116; 0.6448]\n",
       "\n",
       "[0.58289; 0.36258; … ; -0.35169; -0.82555]\n",
       "\n",
       "[0.70853; 0.57088; … ; -0.093918; -0.80375]\n",
       "\n",
       "...\n",
       "\n",
       "[0.418; 0.24968; … ; -0.11514; -0.78581]\n",
       "\n",
       "[0.66034; 0.319; … ; 0.84539; -0.57119]\n",
       "\n",
       "[-0.58402; 0.39031; … ; 0.46715; 0.78858], [7, 4, 2, 7, 7, 1, 10, 10, 7, 13, 13, 10, 16, 16, 13, 7])                           \n",
       " ([-0.75898; -0.47426; … ; -0.014116; 0.6448]\n",
       "\n",
       "[0.418; 0.24968; … ; -0.11514; -0.78581]\n",
       "\n",
       "[-0.28555; 0.065832; … ; 1.4055; -1.1979]\n",
       "\n",
       "...\n",
       "\n",
       "[0.418; 0.24968; … ; -0.11514; -0.78581]\n",
       "\n",
       "[-0.3936; 0.025387; … ; -0.93143; -0.2979]\n",
       "\n",
       "[0.15164; 0.30177; … ; 0.016413; 0.10216], [3, 7, 5, 3, 7, 1, 11, 11, 11, 7  …  23, 32, 32, 32, 28, 36, 36, 36, 23, 7])         \n",
       " ([-0.75898; -0.47426; … ; -0.014116; 0.6448]\n",
       "\n",
       "[0.418; 0.24968; … ; -0.11514; -0.78581]\n",
       "\n",
       "[-0.29329; 0.24741; … ; 0.26795; -0.17126]\n",
       "\n",
       "...\n",
       "\n",
       "[0.79548; 0.4639; … ; 0.69828; 0.97692]\n",
       "\n",
       "[0.24036; -0.38171; … ; -0.10046; 0.7096]\n",
       "\n",
       "[0.15164; 0.30177; … ; 0.016413; 0.10216], [3, 6, 6, 6, 1, 9, 9, 6, 13, 13, 13, 9, 6])                                           \n",
       " ([-0.75898; -0.47426; … ; -0.014116; 0.6448]\n",
       "\n",
       "[-0.0010919; 0.33324; … ; -0.048969; 1.1316]\n",
       "\n",
       "[0.0029927; 0.27328; … ; -0.51517; 0.39423]\n",
       "\n",
       "...\n",
       "\n",
       "[1.7527; -0.21281; … ; 0.33416; -0.053689]\n",
       "\n",
       "[1.1528; -0.4252; … ; 0.3907; 0.27316]\n",
       "\n",
       "[0.15164; 0.30177; … ; 0.016413; 0.10216], [3, 1, 7, 7, 7, 3, 9, 7, 13, 13, 13, 7, 3])                                      \n",
       " ([-0.75898; -0.47426; … ; -0.014116; 0.6448]\n",
       "\n",
       "[0.418; 0.24968; … ; -0.11514; -0.78581]\n",
       "\n",
       "[2.0865; 0.10589; … ; -0.86881; -0.30734]\n",
       "\n",
       "...\n",
       "\n",
       "[0.418; 0.24968; … ; -0.11514; -0.78581]\n",
       "\n",
       "[0.9671; 0.015075; … ; 0.37563; -0.69832]\n",
       "\n",
       "[0.15164; 0.30177; … ; 0.016413; 0.10216], [3, 5, 5, 1, 13, 13, 13, 12, 12, 12, 8, 5, 16, 16, 13, 5])                            \n",
       " ([-0.75898; -0.47426; … ; -0.014116; 0.6448]\n",
       "\n",
       "[0.44634; -0.11361; … ; -0.12799; -0.06593]\n",
       "\n",
       "[0.418; 0.24968; … ; -0.11514; -0.78581]\n",
       "\n",
       "...\n",
       "\n",
       "[0.57049; -0.0077854; … ; 0.12573; 0.83939]\n",
       "\n",
       "[0.20952; 0.37266; … ; 0.20278; 0.60385]\n",
       "\n",
       "[0.15164; 0.30177; … ; 0.016413; 0.10216], [7, 4, 7, 7, 7, 22, 9, 7, 9, 12  …  28, 24, 30, 28, 33, 33, 30, 35, 22, 22])      \n",
       " ([-0.75898; -0.47426; … ; -0.014116; 0.6448]\n",
       "\n",
       "[0.61183; -0.22072; … ; -0.097922; 0.16806]\n",
       "\n",
       "[-0.083451; 0.26305; … ; -0.19264; 0.59856]\n",
       "\n",
       "...\n",
       "\n",
       "[0.33042; 0.24995; … ; -0.027273; -0.53285]\n",
       "\n",
       "[0.015116; -0.10943; … ; -0.30163; 0.0054462]\n",
       "\n",
       "[0.15164; 0.30177; … ; 0.016413; 0.10216], [3, 1, 3, 6, 3, 10, 10, 10, 3, 14, 14, 14, 10, 18, 18, 18, 14, 20, 14, 3])\n",
       " ([-0.75898; -0.47426; … ; -0.014116; 0.6448]\n",
       "\n",
       "[0.54214; 1.0302; … ; 0.18317; 0.4338]\n",
       "\n",
       "[-0.16768; 1.2151; … ; 0.30893; 0.11023]\n",
       "\n",
       "...\n",
       "\n",
       "[0.33042; 0.24995; … ; -0.027273; -0.53285]\n",
       "\n",
       "[1.2157; -0.5335; … ; -0.066592; -0.50836]\n",
       "\n",
       "[0.15164; 0.30177; … ; 0.016413; 0.10216], [1, 2, 2, 2, 7, 2, 9, 7, 14, 14, 14, 14, 9, 16, 7, 16, 19, 16, 2])                   \n",
       " ([-0.75898; -0.47426; … ; -0.014116; 0.6448]\n",
       "\n",
       "[0.418; 0.24968; … ; -0.11514; -0.78581]\n",
       "\n",
       "[0.49725; -1.1949; … ; 0.85397; -0.65035]\n",
       "\n",
       "...\n",
       "\n",
       "[0.70853; 0.57088; … ; -0.093918; -0.80375]\n",
       "\n",
       "[1.5226; -0.75782; … ; 0.076369; -0.59173]\n",
       "\n",
       "[0.15164; 0.30177; … ; 0.016413; 0.10216], [4, 4, 8, 7, 7, 4, 1, 12, 12, 12  …  12, 24, 24, 21, 27, 27, 24, 29, 27, 8])      \n",
       " ([-0.75898; -0.47426; … ; -0.014116; 0.6448]\n",
       "\n",
       "[0.30045; 0.25006; … ; 0.23052; -0.51939]\n",
       "\n",
       "[0.33028; 0.0068584; … ; 0.41093; -0.38788]\n",
       "\n",
       "...\n",
       "\n",
       "[0.53662; -0.01189; … ; 0.063342; 0.17774]\n",
       "\n",
       "[0.23302; 0.45885; … ; 0.2041; -0.19532]\n",
       "\n",
       "[0.15164; 0.30177; … ; 0.016413; 0.10216], [3, 6, 6, 6, 1, 9, 9, 6, 12, 12, 9, 18, 18, 18, 18, 18, 6, 18, 6])                \n",
       " ⋮                                                                                                                                                                                                                                                                                                                                                             \n",
       " ([-0.75898; -0.47426; … ; -0.014116; 0.6448]\n",
       "\n",
       "[0.60308; -0.32024; … ; -0.26879; 0.36657]\n",
       "\n",
       "[0.11891; 0.15255; … ; -0.26671; 0.92121]\n",
       "\n",
       "...\n",
       "\n",
       "[-0.57057; -0.52453; … ; 0.19937; -0.47439]\n",
       "\n",
       "[-0.16911; -0.17595; … ; -0.1365; 0.0054371]\n",
       "\n",
       "[0.15164; 0.30177; … ; 0.016413; 0.10216], [4, 4, 1, 4, 9, 9, 9, 4, 4, 4])                                               \n",
       " ([-0.75898; -0.47426; … ; -0.014116; 0.6448]\n",
       "\n",
       "[0.70835; -0.57361; … ; -0.31217; -0.3049]\n",
       "\n",
       "[0.54968; 0.028416; … ; -0.19113; 0.1844]\n",
       "\n",
       "...\n",
       "\n",
       "[0.418; 0.24968; … ; -0.11514; -0.78581]\n",
       "\n",
       "[-0.00034635; 0.17365; … ; -0.13533; -0.14868]\n",
       "\n",
       "[0.15164; 0.30177; … ; 0.016413; 0.10216], [3, 1, 7, 7, 7, 3, 10, 10, 7, 3, 14, 14, 3, 17, 17, 14, 3])                    \n",
       " ([-0.75898; -0.47426; … ; -0.014116; 0.6448]\n",
       "\n",
       "[0.12972; 0.088073; … ; -0.60515; -0.9827]\n",
       "\n",
       "[0.58289; 0.36258; … ; -0.35169; -0.82555]\n",
       "\n",
       "...\n",
       "\n",
       "[0.16265; -0.37274; … ; -0.2768; -0.36693]\n",
       "\n",
       "[-1.1949; 0.053218; … ; 0.41548; -0.34975]\n",
       "\n",
       "[0.15164; 0.30177; … ; 0.016413; 0.10216], [3, 4, 5, 18, 5, 12, 9, 10, 12, 12, 5, 5, 15, 18, 18, 18, 1, 18])               \n",
       " ([-0.75898; -0.47426; … ; -0.014116; 0.6448]\n",
       "\n",
       "[0.21705; 0.46515; … ; 0.41013; 0.1796]\n",
       "\n",
       "[0.6008; 0.18044; … ; -0.65372; -0.38255]\n",
       "\n",
       "...\n",
       "\n",
       "[0.47685; -0.084552; … ; 1.1177; -0.36595]\n",
       "\n",
       "[-0.28314; 1.0028; … ; 0.14553; 0.37339]\n",
       "\n",
       "[0.15164; 0.30177; … ; 0.016413; 0.10216], [4, 4, 7, 7, 7, 11, 11, 11, 11, 1  …  27, 27, 27, 27, 21, 30, 30, 27, 27, 11])        \n",
       " ([-0.75898; -0.47426; … ; -0.014116; 0.6448]\n",
       "\n",
       "[0.70853; 0.57088; … ; -0.093918; -0.80375]\n",
       "\n",
       "[0.078276; 0.79227; … ; -0.041225; 0.42042]\n",
       "\n",
       "...\n",
       "\n",
       "[0.39119; 0.34992; … ; -0.0059552; 0.1746]\n",
       "\n",
       "[0.11626; 0.53897; … ; -0.10885; 0.084513]\n",
       "\n",
       "[0.15164; 0.30177; … ; 0.016413; 0.10216], [8, 2, 8, 8, 8, 8, 1, 11, 11, 8  …  18, 16, 18, 21, 18, 21, 25, 25, 21, 8])   \n",
       " ([-0.75898; -0.47426; … ; -0.014116; 0.6448]\n",
       "\n",
       "[0.70853; 0.57088; … ; -0.093918; -0.80375]\n",
       "\n",
       "[0.078276; 0.79227; … ; -0.041225; 0.42042]\n",
       "\n",
       "...\n",
       "\n",
       "[-0.14525; 0.31265; … ; 0.14322; 1.0118]\n",
       "\n",
       "[-0.28314; 1.0028; … ; 0.14553; 0.37339]\n",
       "\n",
       "[0.15164; 0.30177; … ; 0.016413; 0.10216], [7, 2, 7, 7, 7, 1, 7, 7, 11, 7  …  7, 21, 21, 18, 23, 18, 25, 23, 7, 7])          \n",
       " ([-0.75898; -0.47426; … ; -0.014116; 0.6448]\n",
       "\n",
       "[0.418; 0.24968; … ; -0.11514; -0.78581]\n",
       "\n",
       "[0.39119; 0.34992; … ; -0.0059552; 0.1746]\n",
       "\n",
       "...\n",
       "\n",
       "[0.55025; -0.24942; … ; 0.050139; 0.33465]\n",
       "\n",
       "[0.33042; 0.24995; … ; -0.027273; -0.53285]\n",
       "\n",
       "[0.15164; 0.30177; … ; 0.016413; 0.10216], [4, 4, 9, 9, 7, 9, 9, 1, 12, 12, 9, 19, 16, 16, 19, 19, 19, 9, 9])               \n",
       " ([-0.75898; -0.47426; … ; -0.014116; 0.6448]\n",
       "\n",
       "[0.70853; 0.57088; … ; -0.093918; -0.80375]\n",
       "\n",
       "[0.078276; 0.79227; … ; -0.041225; 0.42042]\n",
       "\n",
       "...\n",
       "\n",
       "[-0.093384; -0.08952; … ; 0.43841; -0.33986]\n",
       "\n",
       "[-0.28314; 1.0028; … ; 0.14553; 0.37339]\n",
       "\n",
       "[0.15164; 0.30177; … ; 0.016413; 0.10216], [8, 2, 8, 8, 8, 8, 1, 8, 8, 12  …  47, 47, 39, 50, 50, 39, 52, 50, 39, 8])    \n",
       " ([-0.75898; -0.47426; … ; -0.014116; 0.6448]\n",
       "\n",
       "[0.30045; 0.25006; … ; 0.23052; -0.51939]\n",
       "\n",
       "[0.32801; -0.00015885; … ; 0.27383; -0.34373]\n",
       "\n",
       "...\n",
       "\n",
       "[0.89466; 0.36604; … ; -0.66593; 0.12177]\n",
       "\n",
       "[0.61183; -0.22072; … ; -0.097922; 0.16806]\n",
       "\n",
       "[0.15164; 0.30177; … ; 0.016413; 0.10216], [3, 5, 5, 1, 14, 14, 14, 14, 14, 14  …  5, 16, 14, 16, 19, 17, 19, 22, 20, 5])\n",
       " ([-0.75898; -0.47426; … ; -0.014116; 0.6448]\n",
       "\n",
       "[0.53511; -0.22559; … ; 0.069407; -0.35229]\n",
       "\n",
       "[0.70835; -0.57361; … ; -0.31217; -0.3049]\n",
       "\n",
       "...\n",
       "\n",
       "[0.80196; 0.36491; … ; -0.091248; -0.91374]\n",
       "\n",
       "[1.3429; -0.076521; … ; -0.54888; -0.067072]\n",
       "\n",
       "[0.15164; 0.30177; … ; 0.016413; 0.10216], [6, 6, 6, 6, 1, 6, 9, 6, 11, 6  …  16, 6, 23, 23, 23, 23, 23, 23, 16, 6])   \n",
       " ([-0.75898; -0.47426; … ; -0.014116; 0.6448]\n",
       "\n",
       "[0.418; 0.24968; … ; -0.11514; -0.78581]\n",
       "\n",
       "[0.72931; -0.45016; … ; -1.0209; 0.80056]\n",
       "\n",
       "...\n",
       "\n",
       "[0.90754; -0.38322; … ; -0.1844; 0.071147]\n",
       "\n",
       "[0.31369; 0.73732; … ; 0.1355; 0.24894]\n",
       "\n",
       "[0.15164; 0.30177; … ; 0.016413; 0.10216], [3, 9, 6, 6, 3, 9, 9, 1, 14, 14  …  14, 18, 16, 20, 18, 24, 24, 24, 20, 9])           \n",
       " ([-0.75898; -0.47426; … ; -0.014116; 0.6448]\n",
       "\n",
       "[0.11891; 0.15255; … ; -0.26671; 0.92121]\n",
       "\n",
       "[0.81544; 0.30171; … ; -0.34741; 0.41672]\n",
       "\n",
       "...\n",
       "\n",
       "[0.47685; -0.084552; … ; 1.1177; -0.36595]\n",
       "\n",
       "[-0.28314; 1.0028; … ; 0.14553; 0.37339]\n",
       "\n",
       "[0.15164; 0.30177; … ; 0.016413; 0.10216], [5, 5, 5, 1, 5, 5, 5, 11, 11, 5  …  19, 17, 22, 22, 19, 25, 25, 22, 5, 5])          "
      ]
     },
     "execution_count": 677,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clcdata5 = collect(data5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12543"
      ]
     },
     "execution_count": 680,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length(clcdata5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000-element Array{Tuple{Array{Float64,3},Array{Int64,1}},1}:\n",
       " ([-0.75898; -0.47426; … ; -0.014116; 0.6448]\n",
       "\n",
       "[0.68491; 0.32385; … ; -0.1693; 0.062375]\n",
       "\n",
       "[0.96193; 0.012516; … ; -0.38468; -0.38712]\n",
       "\n",
       "...\n",
       "\n",
       "[0.26818; 0.14346; … ; -0.25028; -0.38097]\n",
       "\n",
       "[0.43433; 0.72169; … ; 0.14362; -0.93825]\n",
       "\n",
       "[0.013441; 0.23682; … ; 0.044691; 0.30392], [3, 1, 6, 6, 3, 6, 11, 11, 11, 6  …  17, 17, 13, 19, 13, 21, 19, 23, 19, 3])                                 \n",
       " ([-0.75898; -0.47426; … ; -0.014116; 0.6448]\n",
       "\n",
       "[0.16788; 0.10003; … ; 0.57603; 0.039515]\n",
       "\n",
       "[-0.25472; 0.13607; … ; 0.25924; 0.68466]\n",
       "\n",
       "...\n",
       "\n",
       "[0.013441; 0.23682; … ; 0.044691; 0.30392]\n",
       "\n",
       "[-0.32005; 0.1444; … ; -0.33944; -0.089125]\n",
       "\n",
       "[0.3205; -0.26331; … ; 0.084629; -0.69395], [1, 4, 2, 7, 7, 4, 10, 10, 2, 14  …  45, 37, 48, 48, 37, 50, 37, 53, 53, 37])                                \n",
       " ([-0.75898; -0.47426; … ; -0.014116; 0.6448]\n",
       "\n",
       "[-0.61554; 0.47275; … ; -0.19855; -0.39879]\n",
       "\n",
       "[0.8052; 0.37121; … ; -0.20902; 0.47612]\n",
       "\n",
       "...\n",
       "\n",
       "[1.2487; 0.087988; … ; -0.56704; -0.14627]\n",
       "\n",
       "[0.58827; -0.0063354; … ; -0.95651; -0.83804]\n",
       "\n",
       "[0.013441; 0.23682; … ; 0.044691; 0.30392], [4, 4, 1, 7, 7, 4, 10, 10, 7, 15  …  4, 32, 30, 35, 35, 32, 38, 38, 30, 4])                               \n",
       " ([-0.75898; -0.47426; … ; -0.014116; 0.6448]\n",
       "\n",
       "[0.072617; -0.51393; … ; -0.59021; 0.55559], [1])                                                                                                                                                                                                                                                                                            \n",
       " ([-0.75898; -0.47426; … ; -0.014116; 0.6448]\n",
       "\n",
       "[0.072617; -0.51393; … ; -0.59021; 0.55559], [1])                                                                                                                                                                                                                                                                                            \n",
       " ([-0.75898; -0.47426; … ; -0.014116; 0.6448]\n",
       "\n",
       "[0.072617; -0.51393; … ; -0.59021; 0.55559], [1])                                                                                                                                                                                                                                                                                            \n",
       " ([-0.75898; -0.47426; … ; -0.014116; 0.6448]\n",
       "\n",
       "[0.072617; -0.51393; … ; -0.59021; 0.55559], [1])                                                                                                                                                                                                                                                                                            \n",
       " ([-0.75898; -0.47426; … ; -0.014116; 0.6448]\n",
       "\n",
       "[0.072617; -0.51393; … ; -0.59021; 0.55559], [1])                                                                                                                                                                                                                                                                                            \n",
       " ([-0.75898; -0.47426; … ; -0.014116; 0.6448]\n",
       "\n",
       "[0.072617; -0.51393; … ; -0.59021; 0.55559], [1])                                                                                                                                                                                                                                                                                            \n",
       " ([-0.75898; -0.47426; … ; -0.014116; 0.6448]\n",
       "\n",
       "[0.072617; -0.51393; … ; -0.59021; 0.55559], [1])                                                                                                                                                                                                                                                                                            \n",
       " ([-0.75898; -0.47426; … ; -0.014116; 0.6448]\n",
       "\n",
       "[0.68938; -0.10644; … ; -0.22504; 0.61412]\n",
       "\n",
       "[0.8052; 0.37121; … ; -0.20902; 0.47612]\n",
       "\n",
       "...\n",
       "\n",
       "[-0.20092; -0.060271; … ; -0.041659; -0.013171]\n",
       "\n",
       "[-0.68975; 0.6703; … ; -0.25186; 0.6161]\n",
       "\n",
       "[-0.14578; 0.50459; … ; 0.11054; 1.1681], [5, 5, 5, 1, 7, 5, 9, 5, 9, 13, 13, 10, 5])                                                                  \n",
       " ([-0.75898; -0.47426; … ; -0.014116; 0.6448]\n",
       "\n",
       "[0.11891; 0.15255; … ; -0.26671; 0.92121]\n",
       "\n",
       "[0.94911; -0.34968; … ; -0.72512; -0.6089]\n",
       "\n",
       "...\n",
       "\n",
       "[0.66834; -0.21732; … ; -0.475; -0.082577]\n",
       "\n",
       "[0.00027751; 0.42673; … ; 0.018857; 0.17536]\n",
       "\n",
       "[0.15164; 0.30177; … ; 0.016413; 0.10216], [3, 1, 5, 3, 8, 8, 5, 10, 8, 8, 3])                                                                         \n",
       " ([-0.75898; -0.47426; … ; -0.014116; 0.6448]\n",
       "\n",
       "[-0.20092; -0.060271; … ; -0.041659; -0.013171]\n",
       "\n",
       "[0.23727; 0.40478; … ; -0.007901; -0.030231]\n",
       "\n",
       "...\n",
       "\n",
       "[0.0016675; -0.16376; … ; -0.069572; 1.1343]\n",
       "\n",
       "[-0.53079; -0.21965; … ; -0.24725; -0.030552]\n",
       "\n",
       "[0.15164; 0.30177; … ; 0.016413; 0.10216], [6, 6, 6, 6, 1, 6, 13, 11, 11, 13, 13, 6, 15, 13, 6])                                            \n",
       " ⋮                                                                                                                                                                                                                                                                                                                                                                                          \n",
       " ([-0.75898; -0.47426; … ; -0.014116; 0.6448]\n",
       "\n",
       "[0.35934; -0.2657; … ; -0.11503; 0.074678]\n",
       "\n",
       "[0.047246; 0.042534; … ; -0.10124; 0.7584]\n",
       "\n",
       "[0.54822; 0.038847; … ; -0.40267; -0.17111]\n",
       "\n",
       "[0.68491; 0.32385; … ; -0.1693; 0.062375]\n",
       "\n",
       "[0.27848; 0.079579; … ; -0.071975; -0.22772]\n",
       "\n",
       "[0.53662; -0.01189; … ; 0.063342; 0.17774]\n",
       "\n",
       "[0.072617; -0.51393; … ; -0.59021; 0.55559], [4, 4, 1, 7, 7, 4, 4])\n",
       " ([-0.75898; -0.47426; … ; -0.014116; 0.6448]\n",
       "\n",
       "[0.5788; -0.52068; … ; 0.70262; 0.22582]\n",
       "\n",
       "[1.3787; -0.23076; … ; 0.91915; 0.18906]\n",
       "\n",
       "[-0.87948; -0.19223; … ; -0.84482; 0.44037]\n",
       "\n",
       "[-0.58402; 0.39031; … ; 0.46715; 0.78858], [3, 1, 3, 3])                                                                                                                                                    \n",
       " ([-0.75898; -0.47426; … ; -0.014116; 0.6448]\n",
       "\n",
       "[0.41519; 0.13167; … ; -0.34499; -0.86949]\n",
       "\n",
       "[0.5788; -0.52068; … ; 0.70262; 0.22582]\n",
       "\n",
       "...\n",
       "\n",
       "[0.62143; -0.49645; … ; 0.16453; 0.65013]\n",
       "\n",
       "[0.043327; -0.49887; … ; -0.19529; -0.41575]\n",
       "\n",
       "[0.15164; 0.30177; … ; 0.016413; 0.10216], [4, 4, 7, 7, 7, 1, 11, 11, 11, 7, 7])                                                                         \n",
       " ([-0.75898; -0.47426; … ; -0.014116; 0.6448]\n",
       "\n",
       "[0.61183; -0.22072; … ; -0.097922; 0.16806]\n",
       "\n",
       "[-0.15234; 0.98085; … ; 0.71744; 0.32435]\n",
       "\n",
       "...\n",
       "\n",
       "[-0.053903; -0.30871; … ; 0.68161; 0.066202]\n",
       "\n",
       "[0.70604; 0.11939; … ; -0.24049; -1.1034]\n",
       "\n",
       "[0.15164; 0.30177; … ; 0.016413; 0.10216], [7, 7, 7, 7, 7, 1, 12, 12, 12, 12, 7, 12, 15, 13, 12, 7])                                                   \n",
       " ([-0.75898; -0.47426; … ; -0.014116; 0.6448]\n",
       "\n",
       "[0.61183; -0.22072; … ; -0.097922; 0.16806]\n",
       "\n",
       "[0.60225; -0.036286; … ; 0.10712; 0.69641]\n",
       "\n",
       "...\n",
       "\n",
       "[0.072617; -0.51393; … ; -0.59021; 0.55559]\n",
       "\n",
       "[0.55025; -0.24942; … ; 0.050139; 0.33465]\n",
       "\n",
       "[0.15164; 0.30177; … ; 0.016413; 0.10216], [3, 1, 7, 7, 7, 3, 3, 13, 13, 13, 13, 3, 13, 3])                                                           \n",
       " ([-0.75898; -0.47426; … ; -0.014116; 0.6448]\n",
       "\n",
       "[0.352; 0.25323; … ; -0.011489; -0.025882]\n",
       "\n",
       "[0.013441; 0.23682; … ; 0.044691; 0.30392]\n",
       "\n",
       "...\n",
       "\n",
       "[0.44506; 0.088891; … ; -0.33489; 0.17979]\n",
       "\n",
       "[-0.83203; 1.3531; … ; 1.0293; 0.23714]\n",
       "\n",
       "[0.15164; 0.30177; … ; 0.016413; 0.10216], [6, 6, 6, 6, 1, 6, 6, 10, 8, 14  …  16, 19, 16, 22, 22, 14, 22, 22, 6, 6])                                      \n",
       " ([-0.75898; -0.47426; … ; -0.014116; 0.6448]\n",
       "\n",
       "[0.88387; -0.14199; … ; -0.20148; 0.0095952]\n",
       "\n",
       "[0.43191; -0.31311; … ; -0.45787; 0.64811]\n",
       "\n",
       "...\n",
       "\n",
       "[-0.43277; 0.75213; … ; -0.74908; 2.6744]\n",
       "\n",
       "[0.78954; 0.10954; … ; -0.064436; -0.091983]\n",
       "\n",
       "[0.15164; 0.30177; … ; 0.016413; 0.10216], [3, 1, 3, 8, 8, 8, 3, 3, 13, 13, 13, 3, 15, 13, 19, 19, 19, 13, 3])                                       \n",
       " ([-0.75898; -0.47426; … ; -0.014116; 0.6448]\n",
       "\n",
       "[0.61183; -0.22072; … ; -0.097922; 0.16806]\n",
       "\n",
       "[0.47939; 0.30759; … ; -0.12417; 0.13525]\n",
       "\n",
       "...\n",
       "\n",
       "[0.418; 0.24968; … ; -0.11514; -0.78581]\n",
       "\n",
       "[-0.605; -0.030167; … ; -0.019283; 0.2439]\n",
       "\n",
       "[0.15164; 0.30177; … ; 0.016413; 0.10216], [4, 4, 1, 6, 4, 10, 10, 10, 4, 4  …  4, 16, 14, 19, 19, 16, 22, 22, 16, 4])                                    \n",
       " ([-0.75898; -0.47426; … ; -0.014116; 0.6448]\n",
       "\n",
       "[0.41519; 0.13167; … ; -0.34499; -0.86949]\n",
       "\n",
       "[0.62143; -0.49645; … ; 0.16453; 0.65013]\n",
       "\n",
       "...\n",
       "\n",
       "[0.47689; -0.076447; … ; -0.38907; -0.37151]\n",
       "\n",
       "[-0.35586; 0.5213; … ; 0.31305; 0.92771]\n",
       "\n",
       "[0.15164; 0.30177; … ; 0.016413; 0.10216], [3, 7, 7, 6, 7, 1, 9, 7, 15, 15, 15, 15, 15, 7, 7])                                                           \n",
       " ([-0.75898; -0.47426; … ; -0.014116; 0.6448]\n",
       "\n",
       "[0.70835; -0.57361; … ; -0.31217; -0.3049]\n",
       "\n",
       "[0.29605; -0.13841; … ; 0.13854; 0.96954]\n",
       "\n",
       "...\n",
       "\n",
       "[-0.44621; 0.42797; … ; 0.28996; 0.027486]\n",
       "\n",
       "[0.66698; 0.06417; … ; 0.19996; 0.64148]\n",
       "\n",
       "[0.15164; 0.30177; … ; 0.016413; 0.10216], [3, 1, 6, 6, 3, 9, 9, 6, 11, 9, 13, 9, 15, 13, 18, 18, 9, 18, 3])                                               \n",
       " ([-0.75898; -0.47426; … ; -0.014116; 0.6448]\n",
       "\n",
       "[0.25067; -1.3541; … ; 0.34604; 0.65977]\n",
       "\n",
       "[0.013441; 0.23682; … ; 0.044691; 0.30392]\n",
       "\n",
       "...\n",
       "\n",
       "[0.38204; -0.50218; … ; 0.24452; -0.027145]\n",
       "\n",
       "[0.61183; -0.22072; … ; -0.097922; 0.16806]\n",
       "\n",
       "[0.15164; 0.30177; … ; 0.016413; 0.10216], [6, 6, 6, 6, 11, 6, 10, 10, 1, 10, 6])                                                                       \n",
       " ([-0.75898; -0.47426; … ; -0.014116; 0.6448]\n",
       "\n",
       "[0.61183; -0.22072; … ; -0.097922; 0.16806]\n",
       "\n",
       "[-0.11001; 0.55259; … ; 0.24473; 0.14655]\n",
       "\n",
       "...\n",
       "\n",
       "[0.68047; -0.039263; … ; -0.064699; -0.26044]\n",
       "\n",
       "[0.47498; 0.1999; … ; -0.45965; -0.20069]\n",
       "\n",
       "[0.15164; 0.30177; … ; 0.016413; 0.10216], [3, 1, 3, 6, 7, 3, 10, 10, 3, 12, 10, 14, 10, 3])                                                          "
      ]
     },
     "execution_count": 682,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtrn = clcdata5[1:8000]\n",
    "dtst = clcdata5[8001:12000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "metadata": {},
   "outputs": [
    {
     "ename": "MethodError",
     "evalue": "MethodError: Cannot `convert` an object of type Array{Tuple{Array{Float64,3},Array{Int64,1}},1} to an object of type Data\nClosest candidates are:\n  convert(::Type{T}, !Matched::T) where T at essentials.jl:154",
     "output_type": "error",
     "traceback": [
      "MethodError: Cannot `convert` an object of type Array{Tuple{Array{Float64,3},Array{Int64,1}},1} to an object of type Data\nClosest candidates are:\n  convert(::Type{T}, !Matched::T) where T at essentials.jl:154",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[732]:1"
     ]
    }
   ],
   "source": [
    "dtrn = convert(Knet.Data, dtrn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Array{Tuple{Array{Float64,3},Array{Int64,1}},1}:\n",
       " ([-0.75898; -0.47426; … ; -0.014116; 0.6448]\n",
       "\n",
       "[0.54214; 1.0302; … ; 0.18317; 0.4338]\n",
       "\n",
       "[-0.16768; 1.2151; … ; 0.30893; 0.11023]\n",
       "\n",
       "...\n",
       "\n",
       "[0.12417; 1.0659; … ; -0.41358; -1.4449]\n",
       "\n",
       "[0.38784; -0.36196; … ; -0.41707; -1.1415]\n",
       "\n",
       "[0.15164; 0.30177; … ; 0.016413; 0.10216], [1, 2, 2, 2, 7, 8, 2, 8, 9, 9  …  22, 19, 24, 22, 22, 29, 29, 29, 22, 2])"
      ]
     },
     "execution_count": 615,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[iterate(data5)[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainresults (generic function with 1 method)"
      ]
     },
     "execution_count": 683,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For running experiments\n",
    "function trainresults(file,model; o...)\n",
    "    if (print(\"Train from scratch? \"); readline()[1]=='y')\n",
    "        takeevery(n,itr) = (x for (i,x) in enumerate(itr) if i % n == 1)\n",
    "        r = ((model(dtrn), model(dtst), zeroone(model,dtrn), zeroone(model,dtst))\n",
    "             for x in takeevery(length(dtrn), progress(sgd(model,repeat(dtrn,2)))))\n",
    "        r = reshape(collect(Float32,flatten(r)),(4,:))\n",
    "        Knet.save(file,\"results\",r)\n",
    "        Knet.gc() # To save gpu memory\n",
    "    else\n",
    "        isfile(file) || download(\"http://people.csail.mit.edu/deniz/models/tutorial/$file\",file)\n",
    "        r = Knet.load(file,\"results\")\n",
    "    end\n",
    "    println(minimum(r,dims=2))\n",
    "    return r\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainresults (generic function with 1 method)"
      ]
     },
     "execution_count": 735,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For running experiments\n",
    "function trainresults(file,model; o...)\n",
    "    if (print(\"Train from scratch? \"); readline()[1]=='y')\n",
    "        takeevery(n,itr) = (x for (i,x) in enumerate(itr) if i % n == 1)\n",
    "        r = ((model(dtrn,1,1), model(dtst,1,1), zeroone(model,dtrn), zeroone(model,dtst))\n",
    "             for x in takeevery(length(dtrn), progress(adam(model,repeat(dtrn,25)))))\n",
    "        r = reshape(collect(Float32,flatten(r)),(4,:))\n",
    "        Knet.save(file,\"results\",r)\n",
    "        Knet.gc() # To save gpu memory\n",
    "    else\n",
    "        isfile(file) || download(\"http://people.csail.mit.edu/deniz/models/tutorial/$file\",file)\n",
    "        r = Knet.load(file,\"results\")\n",
    "    end\n",
    "    println(minimum(r,dims=2))\n",
    "    return r\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44383983333462895"
      ]
     },
     "execution_count": 686,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zeroone(mymodel1,dtrn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3700570167371713"
      ]
     },
     "execution_count": 688,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zeroone(mymodel1,dtst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9543924565263019"
      ]
     },
     "execution_count": 689,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zeroone(mymodel2,dtrn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9578352032370793"
      ]
     },
     "execution_count": 690,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zeroone(mymodel2,dtst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8000-element Array{Float64,1}:\n",
       " 3.4013262946080154\n",
       " 2.9445139923910455\n",
       " 2.890014048476892 \n",
       " 2.833665594836151 \n",
       " 3.610792494901225 \n",
       " 2.6392085490833916\n",
       " 2.6389081854716823\n",
       " 2.8330951977016197\n",
       " 3.5836647209932364\n",
       " 3.044733973843193 \n",
       " 2.9953588011890853\n",
       " 3.401196953372921 \n",
       " 2.9959895680252835\n",
       " ⋮                 \n",
       " 2.197129255745929 \n",
       " 2.397617231689629 \n",
       " 3.044770177687923 \n",
       " 3.135344376546868 \n",
       " 2.485110164167745 \n",
       " 2.94452519933547  \n",
       " 1.6092654086523668\n",
       " 2.995813691311004 \n",
       " 2.079744283345211 \n",
       " 3.0907028855436933\n",
       " 3.044398750582876 \n",
       " 2.943909500454036 "
      ]
     },
     "execution_count": 721,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collect(mean(mymodel2(x,y)) for (x,y) in dtrn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "mean(itr)\n",
       "\\end{verbatim}\n",
       "Compute the mean of all elements in a collection.\n",
       "\n",
       "\\begin{quote}\n",
       "\\textbf{note}\n",
       "\n",
       "Note\n",
       "\n",
       "If \\texttt{itr} contains \\texttt{NaN} or \\href{@ref}{\\texttt{missing}} values, the result is also \\texttt{NaN} or \\texttt{missing} (\\texttt{missing} takes precedence if array contains both). Use the \\href{@ref}{\\texttt{skipmissing}} function to omit \\texttt{missing} entries and compute the mean of non-missing values.\n",
       "\n",
       "\\end{quote}\n",
       "\\section{Examples}\n",
       "\\begin{verbatim}\n",
       "julia> mean(1:20)\n",
       "10.5\n",
       "\n",
       "julia> mean([1, missing, 3])\n",
       "missing\n",
       "\n",
       "julia> mean(skipmissing([1, missing, 3]))\n",
       "2.0\n",
       "\\end{verbatim}\n",
       "\\begin{verbatim}\n",
       "mean(f::Function, itr)\n",
       "\\end{verbatim}\n",
       "Apply the function \\texttt{f} to each element of collection \\texttt{itr} and take the mean.\n",
       "\n",
       "\\begin{verbatim}\n",
       "julia> mean(√, [1, 2, 3])\n",
       "1.3820881233139908\n",
       "\n",
       "julia> mean([√1, √2, √3])\n",
       "1.3820881233139908\n",
       "\\end{verbatim}\n",
       "\\begin{verbatim}\n",
       "mean(A::AbstractArray; dims)\n",
       "\\end{verbatim}\n",
       "Compute the mean of an array over the given dimensions.\n",
       "\n",
       "\\begin{quote}\n",
       "\\textbf{compat}\n",
       "\n",
       "Julia 1.1\n",
       "\n",
       "\\texttt{mean} for empty arrays requires at least Julia 1.1.\n",
       "\n",
       "\\end{quote}\n",
       "\\section{Examples}\n",
       "\\begin{verbatim}\n",
       "julia> A = [1 2; 3 4]\n",
       "2×2 Array{Int64,2}:\n",
       " 1  2\n",
       " 3  4\n",
       "\n",
       "julia> mean(A, dims=1)\n",
       "1×2 Array{Float64,2}:\n",
       " 2.0  3.0\n",
       "\n",
       "julia> mean(A, dims=2)\n",
       "2×1 Array{Float64,2}:\n",
       " 1.5\n",
       " 3.5\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "```\n",
       "mean(itr)\n",
       "```\n",
       "\n",
       "Compute the mean of all elements in a collection.\n",
       "\n",
       "!!! note\n",
       "    If `itr` contains `NaN` or [`missing`](@ref) values, the result is also `NaN` or `missing` (`missing` takes precedence if array contains both). Use the [`skipmissing`](@ref) function to omit `missing` entries and compute the mean of non-missing values.\n",
       "\n",
       "\n",
       "# Examples\n",
       "\n",
       "```jldoctest\n",
       "julia> mean(1:20)\n",
       "10.5\n",
       "\n",
       "julia> mean([1, missing, 3])\n",
       "missing\n",
       "\n",
       "julia> mean(skipmissing([1, missing, 3]))\n",
       "2.0\n",
       "```\n",
       "\n",
       "```\n",
       "mean(f::Function, itr)\n",
       "```\n",
       "\n",
       "Apply the function `f` to each element of collection `itr` and take the mean.\n",
       "\n",
       "```jldoctest\n",
       "julia> mean(√, [1, 2, 3])\n",
       "1.3820881233139908\n",
       "\n",
       "julia> mean([√1, √2, √3])\n",
       "1.3820881233139908\n",
       "```\n",
       "\n",
       "```\n",
       "mean(A::AbstractArray; dims)\n",
       "```\n",
       "\n",
       "Compute the mean of an array over the given dimensions.\n",
       "\n",
       "!!! compat \"Julia 1.1\"\n",
       "    `mean` for empty arrays requires at least Julia 1.1.\n",
       "\n",
       "\n",
       "# Examples\n",
       "\n",
       "```jldoctest\n",
       "julia> A = [1 2; 3 4]\n",
       "2×2 Array{Int64,2}:\n",
       " 1  2\n",
       " 3  4\n",
       "\n",
       "julia> mean(A, dims=1)\n",
       "1×2 Array{Float64,2}:\n",
       " 2.0  3.0\n",
       "\n",
       "julia> mean(A, dims=2)\n",
       "2×1 Array{Float64,2}:\n",
       " 1.5\n",
       " 3.5\n",
       "```\n"
      ],
      "text/plain": [
       "\u001b[36m  mean(itr)\u001b[39m\n",
       "\n",
       "  Compute the mean of all elements in a collection.\n",
       "\n",
       "\u001b[36m\u001b[1m  │ \u001b[22m\u001b[39m\u001b[36m\u001b[1mNote\u001b[22m\u001b[39m\n",
       "\u001b[36m\u001b[1m  │\u001b[22m\u001b[39m\n",
       "\u001b[36m\u001b[1m  │\u001b[22m\u001b[39m  If \u001b[36mitr\u001b[39m contains \u001b[36mNaN\u001b[39m or \u001b[36mmissing\u001b[39m values, the result is also\n",
       "\u001b[36m\u001b[1m  │\u001b[22m\u001b[39m  \u001b[36mNaN\u001b[39m or \u001b[36mmissing\u001b[39m (\u001b[36mmissing\u001b[39m takes precedence if array contains\n",
       "\u001b[36m\u001b[1m  │\u001b[22m\u001b[39m  both). Use the \u001b[36mskipmissing\u001b[39m function to omit \u001b[36mmissing\u001b[39m\n",
       "\u001b[36m\u001b[1m  │\u001b[22m\u001b[39m  entries and compute the mean of non-missing values.\n",
       "\n",
       "\u001b[1m  Examples\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "\u001b[36m  julia> mean(1:20)\u001b[39m\n",
       "\u001b[36m  10.5\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> mean([1, missing, 3])\u001b[39m\n",
       "\u001b[36m  missing\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> mean(skipmissing([1, missing, 3]))\u001b[39m\n",
       "\u001b[36m  2.0\u001b[39m\n",
       "\n",
       "\u001b[36m  mean(f::Function, itr)\u001b[39m\n",
       "\n",
       "  Apply the function \u001b[36mf\u001b[39m to each element of collection \u001b[36mitr\u001b[39m and take the\n",
       "  mean.\n",
       "\n",
       "\u001b[36m  julia> mean(√, [1, 2, 3])\u001b[39m\n",
       "\u001b[36m  1.3820881233139908\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> mean([√1, √2, √3])\u001b[39m\n",
       "\u001b[36m  1.3820881233139908\u001b[39m\n",
       "\n",
       "\u001b[36m  mean(A::AbstractArray; dims)\u001b[39m\n",
       "\n",
       "  Compute the mean of an array over the given dimensions.\n",
       "\n",
       "\u001b[39m\u001b[1m  │ \u001b[22m\u001b[39m\u001b[1mJulia 1.1\u001b[22m\n",
       "\u001b[39m\u001b[1m  │\u001b[22m\n",
       "\u001b[39m\u001b[1m  │\u001b[22m  \u001b[36mmean\u001b[39m for empty arrays requires at least Julia 1.1.\n",
       "\n",
       "\u001b[1m  Examples\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "\u001b[36m  julia> A = [1 2; 3 4]\u001b[39m\n",
       "\u001b[36m  2×2 Array{Int64,2}:\u001b[39m\n",
       "\u001b[36m   1  2\u001b[39m\n",
       "\u001b[36m   3  4\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> mean(A, dims=1)\u001b[39m\n",
       "\u001b[36m  1×2 Array{Float64,2}:\u001b[39m\n",
       "\u001b[36m   2.0  3.0\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> mean(A, dims=2)\u001b[39m\n",
       "\u001b[36m  2×1 Array{Float64,2}:\u001b[39m\n",
       "\u001b[36m   1.5\u001b[39m\n",
       "\u001b[36m   3.5\u001b[39m"
      ]
     },
     "execution_count": 717,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@doc Knet.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50×1×30 Array{Float64,3}:\n",
       "[:, :, 1] =\n",
       " -0.7589799761772156  \n",
       " -0.47426000237464905 \n",
       "  0.47369998693466187 \n",
       "  0.7724999785423279  \n",
       " -0.7806400060653687  \n",
       "  0.23232999444007874 \n",
       "  0.0461140014231205  \n",
       "  0.8401399850845337  \n",
       "  0.243709996342659   \n",
       "  0.022978000342845917\n",
       "  0.5396400094032288  \n",
       " -0.36100998520851135 \n",
       "  0.9419800043106079  \n",
       "  ⋮                   \n",
       "  0.035413000732660294\n",
       "  0.5883399844169617  \n",
       "  0.4543899893760681  \n",
       " -0.8425400257110596  \n",
       "  0.10649999976158142 \n",
       " -0.059397000819444656\n",
       "  0.09044899791479111 \n",
       "  0.30581000447273254 \n",
       " -0.6142399907112122  \n",
       "  0.7895399928092957  \n",
       " -0.014116000384092331\n",
       "  0.6448000073432922  \n",
       "\n",
       "[:, :, 2] =\n",
       "  0.542140007019043  \n",
       "  1.0302000045776367 \n",
       "  0.8689600229263306 \n",
       "  0.5001400113105774 \n",
       "  0.9518200159072876 \n",
       " -1.3366999626159668 \n",
       " -0.4010699987411499 \n",
       "  0.3922699987888336 \n",
       "  0.536620020866394  \n",
       "  0.48791998624801636\n",
       " -0.8468700051307678 \n",
       " -0.6293799877166748 \n",
       " -1.3402999639511108 \n",
       "  ⋮                  \n",
       " -0.7340899705886841 \n",
       "  1.3209999799728394 \n",
       "  0.5159100294113159 \n",
       "  0.5730599761009216 \n",
       " -0.8462100028991699 \n",
       " -0.2014700025320053 \n",
       "  1.2488000392913818 \n",
       " -0.7575899958610535 \n",
       " -2.0058999061584473 \n",
       "  0.4759100079536438 \n",
       "  0.18317000567913055\n",
       "  0.43380001187324524\n",
       "\n",
       "[:, :, 3] =\n",
       " -0.16767999529838562\n",
       "  1.2151000499725342 \n",
       "  0.49514999985694885\n",
       "  0.2683599889278412 \n",
       " -0.4584999978542328 \n",
       " -0.2331099957227707 \n",
       " -0.528219997882843  \n",
       " -1.3557000160217285 \n",
       "  0.1609800010919571 \n",
       "  0.376910001039505  \n",
       " -0.9270200133323669 \n",
       " -0.43904000520706177\n",
       " -1.0634000301361084 \n",
       "  ⋮                  \n",
       "  0.6995599865913391 \n",
       "  0.1488499939441681 \n",
       "  0.02945300005376339\n",
       "  1.488800048828125  \n",
       "  0.52360999584198   \n",
       "  0.09935399889945984\n",
       "  1.2515000104904175 \n",
       "  0.09938099980354309\n",
       " -0.07926099747419357\n",
       " -0.30862000584602356\n",
       "  0.30893000960350037\n",
       "  0.11022999882698059\n",
       "\n",
       "...\n",
       "\n",
       "[:, :, 28] =\n",
       "  0.12416999787092209 \n",
       "  1.0658999681472778  \n",
       "  0.33647000789642334 \n",
       " -0.9356300234794617  \n",
       "  0.04676799848675728 \n",
       " -0.3745900094509125  \n",
       " -0.26107001304626465 \n",
       "  0.7559199929237366  \n",
       " -0.8221700191497803  \n",
       " -0.30507999658584595 \n",
       "  0.2481199949979782  \n",
       " -0.9628099799156189  \n",
       " -0.2593500018119812  \n",
       "  ⋮                   \n",
       " -1.1033999919891357  \n",
       "  1.426300048828125   \n",
       "  0.5391299724578857  \n",
       "  0.6975299715995789  \n",
       " -0.023492999374866486\n",
       " -0.8098599910736084  \n",
       "  0.13798999786376953 \n",
       " -0.7403200268745422  \n",
       " -0.9150000214576721  \n",
       "  1.7891000509262085  \n",
       " -0.4135800004005432  \n",
       " -1.4449000358581543  \n",
       "\n",
       "[:, :, 29] =\n",
       "  0.38784000277519226 \n",
       " -0.3619599938392639  \n",
       " -0.04096100106835365 \n",
       "  0.2936899960041046  \n",
       " -0.4400100111961365  \n",
       " -0.426829993724823   \n",
       " -0.32728999853134155 \n",
       "  0.3984200060367584  \n",
       "  0.5217900276184082  \n",
       " -1.8260999917984009  \n",
       " -0.2205599993467331  \n",
       " -1.1518000364303589  \n",
       "  0.047251999378204346\n",
       "  ⋮                   \n",
       " -0.971310019493103   \n",
       "  0.9911999702453613  \n",
       " -0.304639995098114   \n",
       "  0.76569002866745    \n",
       "  1.4150999784469604  \n",
       " -0.8635600209236145  \n",
       " -0.3028700053691864  \n",
       " -0.8349900245666504  \n",
       "  0.39190998673439026 \n",
       "  0.41297000646591187 \n",
       " -0.4170700013637543  \n",
       " -1.1414999961853027  \n",
       "\n",
       "[:, :, 30] =\n",
       "  0.15163999795913696 \n",
       "  0.30177000164985657 \n",
       " -0.16763000190258026 \n",
       "  0.17684000730514526 \n",
       "  0.3171899914741516  \n",
       "  0.33972999453544617 \n",
       " -0.4347800016403198  \n",
       " -0.3108600080013275  \n",
       " -0.44999000430107117 \n",
       " -0.29486000537872314 \n",
       "  0.16607999801635742 \n",
       "  0.11963000148534775 \n",
       " -0.41328001022338867 \n",
       "  ⋮                   \n",
       "  0.41705000400543213 \n",
       "  0.056763000786304474\n",
       " -6.368100002873689e-5\n",
       "  0.06898699700832367 \n",
       "  0.08793900161981583 \n",
       " -0.10284999758005142 \n",
       " -0.13931000232696533 \n",
       "  0.22314000129699707 \n",
       " -0.08080299943685532 \n",
       " -0.35651999711990356 \n",
       "  0.016412999480962753\n",
       "  0.1021599993109703  "
      ]
     },
     "execution_count": 704,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtrn[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 702,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mymodel2.layers[1].inputSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "No documentation found.\n",
       "\n",
       "Binding \\texttt{Knet.cget} does not exist.\n",
       "\n"
      ],
      "text/markdown": [
       "No documentation found.\n",
       "\n",
       "Binding `Knet.cget` does not exist.\n"
      ],
      "text/plain": [
       "  No documentation found.\n",
       "\n",
       "  Binding \u001b[36mKnet.cget\u001b[39m does not exist."
      ]
     },
     "execution_count": 709,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@doc Knet.cget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Base.Generator{Array{Tuple{Array{Float64,3},Array{Int64,1}},1},getfield(Main, Symbol(\"##245#246\"))}(getfield(Main, Symbol(\"##245#246\"))(), Tuple{Array{Float64,3},Array{Int64,1}}[([-0.75898; -0.47426; … ; -0.014116; 0.6448]\n",
       "\n",
       "[0.54214; 1.0302; … ; 0.18317; 0.4338]\n",
       "\n",
       "[-0.16768; 1.2151; … ; 0.30893; 0.11023]\n",
       "\n",
       "...\n",
       "\n",
       "[0.12417; 1.0659; … ; -0.41358; -1.4449]\n",
       "\n",
       "[0.38784; -0.36196; … ; -0.41707; -1.1415]\n",
       "\n",
       "[0.15164; 0.30177; … ; 0.016413; 0.10216], [1, 2, 2, 2, 7, 8, 2, 8, 9, 9  …  22, 19, 24, 22, 22, 29, 29, 29, 22, 2]), ([-0.75898; -0.47426; … ; -0.014116; 0.6448]\n",
       "\n",
       "[-0.61201; 0.98226; … ; 0.30649; 0.81745]\n",
       "\n",
       "[0.53074; 0.40117; … ; 0.1444; 0.23611]\n",
       "\n",
       "...\n",
       "\n",
       "[0.5267; 0.11441; … ; -0.2983; 0.020482]\n",
       "\n",
       "[0.15164; 0.30177; … ; 0.016413; 0.10216]\n",
       "\n",
       "[-0.6297; 0.69044; … ; 0.25744; 1.0729], [11, 4, 11, 8, 8, 8, 4, 11, 11, 1, 11, 11, 15, 11, 17, 15, 11, 11]), ([-0.75898; -0.47426; … ; -0.014116; 0.6448]\n",
       "\n",
       "[-0.15362; -0.49874; … ; 0.056634; -0.47371]\n",
       "\n",
       "[-0.17587; 1.3508; … ; 0.5665; 0.61385]\n",
       "\n",
       "...\n",
       "\n",
       "[0.33042; 0.24995; … ; -0.027273; -0.53285]\n",
       "\n",
       "[0.83281; -0.22828; … ; -0.41874; -0.25097]\n",
       "\n",
       "[0.15164; 0.30177; … ; 0.016413; 0.10216], [1, 2, 5, 6, 2, 10, 10, 10, 6, 10, 14, 14, 10, 14, 17, 15, 2]), ([-0.75898; -0.47426; … ; -0.014116; 0.6448]\n",
       "\n",
       "[0.58289; 0.36258; … ; -0.35169; -0.82555]\n",
       "\n",
       "[0.70853; 0.57088; … ; -0.093918; -0.80375]\n",
       "\n",
       "...\n",
       "\n",
       "[0.418; 0.24968; … ; -0.11514; -0.78581]\n",
       "\n",
       "[0.66034; 0.319; … ; 0.84539; -0.57119]\n",
       "\n",
       "[-0.58402; 0.39031; … ; 0.46715; 0.78858], [7, 4, 2, 7, 7, 1, 10, 10, 7, 13, 13, 10, 16, 16, 13, 7]), ([-0.75898; -0.47426; … ; -0.014116; 0.6448]\n",
       "\n",
       "[0.418; 0.24968; … ; -0.11514; -0.78581]\n",
       "\n",
       "[-0.28555; 0.065832; … ; 1.4055; -1.1979]\n",
       "\n",
       "...\n",
       "\n",
       "[0.418; 0.24968; … ; -0.11514; -0.78581]\n",
       "\n",
       "[-0.3936; 0.025387; … ; -0.93143; -0.2979]\n",
       "\n",
       "[0.15164; 0.30177; … ; 0.016413; 0.10216], [3, 7, 5, 3, 7, 1, 11, 11, 11, 7  …  23, 32, 32, 32, 28, 36, 36, 36, 23, 7]), ([-0.75898; -0.47426; … ; -0.014116; 0.6448]\n",
       "\n",
       "[0.418; 0.24968; … ; -0.11514; -0.78581]\n",
       "\n",
       "[-0.29329; 0.24741; … ; 0.26795; -0.17126]\n",
       "\n",
       "...\n",
       "\n",
       "[0.79548; 0.4639; … ; 0.69828; 0.97692]\n",
       "\n",
       "[0.24036; -0.38171; … ; -0.10046; 0.7096]\n",
       "\n",
       "[0.15164; 0.30177; … ; 0.016413; 0.10216], [3, 6, 6, 6, 1, 9, 9, 6, 13, 13, 13, 9, 6]), ([-0.75898; -0.47426; … ; -0.014116; 0.6448]\n",
       "\n",
       "[-0.0010919; 0.33324; … ; -0.048969; 1.1316]\n",
       "\n",
       "[0.0029927; 0.27328; … ; -0.51517; 0.39423]\n",
       "\n",
       "...\n",
       "\n",
       "[1.7527; -0.21281; … ; 0.33416; -0.053689]\n",
       "\n",
       "[1.1528; -0.4252; … ; 0.3907; 0.27316]\n",
       "\n",
       "[0.15164; 0.30177; … ; 0.016413; 0.10216], [3, 1, 7, 7, 7, 3, 9, 7, 13, 13, 13, 7, 3]), ([-0.75898; -0.47426; … ; -0.014116; 0.6448]\n",
       "\n",
       "[0.418; 0.24968; … ; -0.11514; -0.78581]\n",
       "\n",
       "[2.0865; 0.10589; … ; -0.86881; -0.30734]\n",
       "\n",
       "...\n",
       "\n",
       "[0.418; 0.24968; … ; -0.11514; -0.78581]\n",
       "\n",
       "[0.9671; 0.015075; … ; 0.37563; -0.69832]\n",
       "\n",
       "[0.15164; 0.30177; … ; 0.016413; 0.10216], [3, 5, 5, 1, 13, 13, 13, 12, 12, 12, 8, 5, 16, 16, 13, 5]), ([-0.75898; -0.47426; … ; -0.014116; 0.6448]\n",
       "\n",
       "[0.44634; -0.11361; … ; -0.12799; -0.06593]\n",
       "\n",
       "[0.418; 0.24968; … ; -0.11514; -0.78581]\n",
       "\n",
       "...\n",
       "\n",
       "[0.57049; -0.0077854; … ; 0.12573; 0.83939]\n",
       "\n",
       "[0.20952; 0.37266; … ; 0.20278; 0.60385]\n",
       "\n",
       "[0.15164; 0.30177; … ; 0.016413; 0.10216], [7, 4, 7, 7, 7, 22, 9, 7, 9, 12  …  28, 24, 30, 28, 33, 33, 30, 35, 22, 22]), ([-0.75898; -0.47426; … ; -0.014116; 0.6448]\n",
       "\n",
       "[0.61183; -0.22072; … ; -0.097922; 0.16806]\n",
       "\n",
       "[-0.083451; 0.26305; … ; -0.19264; 0.59856]\n",
       "\n",
       "...\n",
       "\n",
       "[0.33042; 0.24995; … ; -0.027273; -0.53285]\n",
       "\n",
       "[0.015116; -0.10943; … ; -0.30163; 0.0054462]\n",
       "\n",
       "[0.15164; 0.30177; … ; 0.016413; 0.10216], [3, 1, 3, 6, 3, 10, 10, 10, 3, 14, 14, 14, 10, 18, 18, 18, 14, 20, 14, 3])  …  ([-0.75898; -0.47426; … ; -0.014116; 0.6448]\n",
       "\n",
       "[-0.27279; 0.77515; … ; 0.040383; 0.26657]\n",
       "\n",
       "[-1.5837; 1.0394; … ; -0.49982; -0.26356]\n",
       "\n",
       "...\n",
       "\n",
       "[0.55494; -0.048714; … ; -0.086564; 0.40418]\n",
       "\n",
       "[0.64642; -0.556; … ; -0.10995; -0.447]\n",
       "\n",
       "[0.15164; 0.30177; … ; 0.016413; 0.10216], [3, 4, 1, 4, 7, 5, 4, 4, 12, 12, 4, 12, 15, 13, 19, 19, 19, 13, 19, 4]), ([-0.75898; -0.47426; … ; -0.014116; 0.6448]\n",
       "\n",
       "[0.11891; 0.15255; … ; -0.26671; 0.92121]\n",
       "\n",
       "[0.7619; -0.29773; … ; -0.22768; 0.4026]\n",
       "\n",
       "...\n",
       "\n",
       "[0.8052; 0.37121; … ; -0.20902; 0.47612]\n",
       "\n",
       "[0.19525; -0.17856; … ; 0.029149; 0.5399]\n",
       "\n",
       "[0.15164; 0.30177; … ; 0.016413; 0.10216], [4, 4, 1, 6, 7, 4, 9, 7, 11, 4  …  18, 18, 18, 18, 12, 22, 22, 22, 18, 4]), ([-0.75898; -0.47426; … ; -0.014116; 0.6448]\n",
       "\n",
       "[0.26818; 0.14346; … ; -0.25028; -0.38097]\n",
       "\n",
       "[0.11891; 0.15255; … ; -0.26671; 0.92121]\n",
       "\n",
       "...\n",
       "\n",
       "[0.418; 0.24968; … ; -0.11514; -0.78581]\n",
       "\n",
       "[-0.19212; -0.12816; … ; -0.034036; -0.65241]\n",
       "\n",
       "[0.15164; 0.30177; … ; 0.016413; 0.10216], [4, 4, 1, 6, 4, 9, 9, 6, 11, 9, 4]), ([-0.75898; -0.47426; … ; -0.014116; 0.6448]\n",
       "\n",
       "[-0.43396; 0.73992; … ; 0.3273; 1.1465]\n",
       "\n",
       "[0.55494; -0.048714; … ; -0.086564; 0.40418]\n",
       "\n",
       "...\n",
       "\n",
       "[0.88387; -0.14199; … ; -0.20148; 0.0095952]\n",
       "\n",
       "[-0.35586; 0.5213; … ; 0.31305; 0.92771]\n",
       "\n",
       "[0.15164; 0.30177; … ; 0.016413; 0.10216], [3, 1, 3, 6, 3, 9, 9, 5, 13, 12, 13, 3, 13, 13, 18, 18, 13, 3]), ([-0.75898; -0.47426; … ; -0.014116; 0.6448]\n",
       "\n",
       "[0.6196; 0.34757; … ; -0.32997; -0.54469]\n",
       "\n",
       "[0.7619; -0.29773; … ; -0.22768; 0.4026]\n",
       "\n",
       "[0.88189; -0.07336; … ; -0.15583; 0.11012]\n",
       "\n",
       "[-0.58402; 0.39031; … ; 0.46715; 0.78858], [4, 4, 1, 4]), ([-0.75898; -0.47426; … ; -0.014116; 0.6448]\n",
       "\n",
       "[-0.55154; 0.99557; … ; 0.48167; 0.21643]\n",
       "\n",
       "[-0.17587; 1.3508; … ; 0.5665; 0.61385]\n",
       "\n",
       "...\n",
       "\n",
       "[0.68491; 0.32385; … ; -0.1693; 0.062375]\n",
       "\n",
       "[0.26818; 0.14346; … ; -0.25028; -0.38097]\n",
       "\n",
       "[0.19565; -0.32773; … ; -0.088545; -0.31418], [1, 2, 6, 6, 14, 8, 6, 11, 11, 8, 14, 14, 2, 16, 14, 18, 14, 20, 18]), ([-0.75898; -0.47426; … ; -0.014116; 0.6448]\n",
       "\n",
       "[-0.20218; -0.20935; … ; -0.80171; 1.1608]\n",
       "\n",
       "[-0.20454; 0.23321; … ; 0.65984; -0.3198]\n",
       "\n",
       "[0.013441; 0.23682; … ; 0.044691; 0.30392]\n",
       "\n",
       "[0.25968; 0.22696; … ; 0.067308; -1.2239]\n",
       "\n",
       "[-0.49935; 1.4759; … ; 0.67729; 0.71499]\n",
       "\n",
       "[0.35093; 0.67075; … ; 0.22914; 0.62689]\n",
       "\n",
       "[0.31675; 0.63482; … ; -0.19717; -0.8948], [3, 1, 3, 8, 7, 5, 3]), ([-0.75898; -0.47426; … ; -0.014116; 0.6448]\n",
       "\n",
       "[0.31967; -0.18114; … ; 0.41304; 0.59379]\n",
       "\n",
       "[0.32112; -0.69306; … ; -0.1108; -0.585]\n",
       "\n",
       "...\n",
       "\n",
       "[-0.14356; 0.48219; … ; 0.23032; 0.60243]\n",
       "\n",
       "[0.33896; 1.0616; … ; 0.080315; -0.45889]\n",
       "\n",
       "[0.013441; 0.23682; … ; 0.044691; 0.30392], [1, 2, 5, 2, 7, 2, 7, 10, 2, 12  …  15, 15, 12, 21, 21, 21, 21, 21, 12, 2]), ([-0.75898; -0.47426; … ; -0.014116; 0.6448]\n",
       "\n",
       "[-0.27028; 0.72918; … ; 0.22614; -0.32425]\n",
       "\n",
       "[0.32112; -0.69306; … ; -0.1108; -0.585]\n",
       "\n",
       "...\n",
       "\n",
       "[0.70853; 0.57088; … ; -0.093918; -0.80375]\n",
       "\n",
       "[-0.20454; 0.23321; … ; 0.65984; -0.3198]\n",
       "\n",
       "[0.013441; 0.23682; … ; 0.044691; 0.30392], [1, 2, 5, 6, 8, 8, 2, 2, 11, 2, 13, 14, 11, 16, 11, 18, 16, 20, 16, 2]), ([-0.75898; -0.47426; … ; -0.014116; 0.6448]\n",
       "\n",
       "[-0.031767; -0.30366; … ; 0.27918; 0.8917]\n",
       "\n",
       "[0.42496; 0.62256; … ; 0.11955; 0.071263]\n",
       "\n",
       "...\n",
       "\n",
       "[0.26818; 0.14346; … ; -0.25028; -0.38097]\n",
       "\n",
       "[-0.15016; 1.5301; … ; -0.28969; -0.46711]\n",
       "\n",
       "[0.013441; 0.23682; … ; 0.044691; 0.30392], [1, 2, 5, 2, 7, 2, 7, 10, 7, 10, 13, 2, 13, 16, 13, 18, 16, 2])])"
      ]
     },
     "execution_count": 693,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asd = ((x,y) for (x,y) in dtrn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50×1×30 Array{Float64,3}:\n",
       "[:, :, 1] =\n",
       " -0.7589799761772156  \n",
       " -0.47426000237464905 \n",
       "  0.47369998693466187 \n",
       "  0.7724999785423279  \n",
       " -0.7806400060653687  \n",
       "  0.23232999444007874 \n",
       "  0.0461140014231205  \n",
       "  0.8401399850845337  \n",
       "  0.243709996342659   \n",
       "  0.022978000342845917\n",
       "  0.5396400094032288  \n",
       " -0.36100998520851135 \n",
       "  0.9419800043106079  \n",
       "  ⋮                   \n",
       "  0.035413000732660294\n",
       "  0.5883399844169617  \n",
       "  0.4543899893760681  \n",
       " -0.8425400257110596  \n",
       "  0.10649999976158142 \n",
       " -0.059397000819444656\n",
       "  0.09044899791479111 \n",
       "  0.30581000447273254 \n",
       " -0.6142399907112122  \n",
       "  0.7895399928092957  \n",
       " -0.014116000384092331\n",
       "  0.6448000073432922  \n",
       "\n",
       "[:, :, 2] =\n",
       "  0.542140007019043  \n",
       "  1.0302000045776367 \n",
       "  0.8689600229263306 \n",
       "  0.5001400113105774 \n",
       "  0.9518200159072876 \n",
       " -1.3366999626159668 \n",
       " -0.4010699987411499 \n",
       "  0.3922699987888336 \n",
       "  0.536620020866394  \n",
       "  0.48791998624801636\n",
       " -0.8468700051307678 \n",
       " -0.6293799877166748 \n",
       " -1.3402999639511108 \n",
       "  ⋮                  \n",
       " -0.7340899705886841 \n",
       "  1.3209999799728394 \n",
       "  0.5159100294113159 \n",
       "  0.5730599761009216 \n",
       " -0.8462100028991699 \n",
       " -0.2014700025320053 \n",
       "  1.2488000392913818 \n",
       " -0.7575899958610535 \n",
       " -2.0058999061584473 \n",
       "  0.4759100079536438 \n",
       "  0.18317000567913055\n",
       "  0.43380001187324524\n",
       "\n",
       "[:, :, 3] =\n",
       " -0.16767999529838562\n",
       "  1.2151000499725342 \n",
       "  0.49514999985694885\n",
       "  0.2683599889278412 \n",
       " -0.4584999978542328 \n",
       " -0.2331099957227707 \n",
       " -0.528219997882843  \n",
       " -1.3557000160217285 \n",
       "  0.1609800010919571 \n",
       "  0.376910001039505  \n",
       " -0.9270200133323669 \n",
       " -0.43904000520706177\n",
       " -1.0634000301361084 \n",
       "  ⋮                  \n",
       "  0.6995599865913391 \n",
       "  0.1488499939441681 \n",
       "  0.02945300005376339\n",
       "  1.488800048828125  \n",
       "  0.52360999584198   \n",
       "  0.09935399889945984\n",
       "  1.2515000104904175 \n",
       "  0.09938099980354309\n",
       " -0.07926099747419357\n",
       " -0.30862000584602356\n",
       "  0.30893000960350037\n",
       "  0.11022999882698059\n",
       "\n",
       "...\n",
       "\n",
       "[:, :, 28] =\n",
       "  0.12416999787092209 \n",
       "  1.0658999681472778  \n",
       "  0.33647000789642334 \n",
       " -0.9356300234794617  \n",
       "  0.04676799848675728 \n",
       " -0.3745900094509125  \n",
       " -0.26107001304626465 \n",
       "  0.7559199929237366  \n",
       " -0.8221700191497803  \n",
       " -0.30507999658584595 \n",
       "  0.2481199949979782  \n",
       " -0.9628099799156189  \n",
       " -0.2593500018119812  \n",
       "  ⋮                   \n",
       " -1.1033999919891357  \n",
       "  1.426300048828125   \n",
       "  0.5391299724578857  \n",
       "  0.6975299715995789  \n",
       " -0.023492999374866486\n",
       " -0.8098599910736084  \n",
       "  0.13798999786376953 \n",
       " -0.7403200268745422  \n",
       " -0.9150000214576721  \n",
       "  1.7891000509262085  \n",
       " -0.4135800004005432  \n",
       " -1.4449000358581543  \n",
       "\n",
       "[:, :, 29] =\n",
       "  0.38784000277519226 \n",
       " -0.3619599938392639  \n",
       " -0.04096100106835365 \n",
       "  0.2936899960041046  \n",
       " -0.4400100111961365  \n",
       " -0.426829993724823   \n",
       " -0.32728999853134155 \n",
       "  0.3984200060367584  \n",
       "  0.5217900276184082  \n",
       " -1.8260999917984009  \n",
       " -0.2205599993467331  \n",
       " -1.1518000364303589  \n",
       "  0.047251999378204346\n",
       "  ⋮                   \n",
       " -0.971310019493103   \n",
       "  0.9911999702453613  \n",
       " -0.304639995098114   \n",
       "  0.76569002866745    \n",
       "  1.4150999784469604  \n",
       " -0.8635600209236145  \n",
       " -0.3028700053691864  \n",
       " -0.8349900245666504  \n",
       "  0.39190998673439026 \n",
       "  0.41297000646591187 \n",
       " -0.4170700013637543  \n",
       " -1.1414999961853027  \n",
       "\n",
       "[:, :, 30] =\n",
       "  0.15163999795913696 \n",
       "  0.30177000164985657 \n",
       " -0.16763000190258026 \n",
       "  0.17684000730514526 \n",
       "  0.3171899914741516  \n",
       "  0.33972999453544617 \n",
       " -0.4347800016403198  \n",
       " -0.3108600080013275  \n",
       " -0.44999000430107117 \n",
       " -0.29486000537872314 \n",
       "  0.16607999801635742 \n",
       "  0.11963000148534775 \n",
       " -0.41328001022338867 \n",
       "  ⋮                   \n",
       "  0.41705000400543213 \n",
       "  0.056763000786304474\n",
       " -6.368100002873689e-5\n",
       "  0.06898699700832367 \n",
       "  0.08793900161981583 \n",
       " -0.10284999758005142 \n",
       " -0.13931000232696533 \n",
       "  0.22314000129699707 \n",
       " -0.08080299943685532 \n",
       " -0.35651999711990356 \n",
       "  0.016412999480962753\n",
       "  0.1021599993109703  "
      ]
     },
     "execution_count": 695,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterate(asd)[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "AssertionError: X == r.inputSize",
     "output_type": "error",
     "traceback": [
      "AssertionError: X == r.inputSize",
      "",
      "Stacktrace:",
      " [1] #rnntest#636(::Nothing, ::Bool, ::Bool, ::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}, ::Function, ::RNN{Array{Float32,3}}, ::Param{Array{Float32,3}}, ::Array{Tuple{Array{Float64,3},Array{Int64,1}},1}, ::Nothing, ::Nothing) at C:\\Users\\dfhdhsd\\.julia\\packages\\Knet\\05UDD\\src\\rnn.jl:727",
      " [2] (::getfield(Knet, Symbol(\"#kw##rnntest\")))(::NamedTuple{(:hy, :cy, :batchSizes),Tuple{Bool,Bool,Nothing}}, ::typeof(Knet.rnntest), ::RNN{Array{Float32,3}}, ::Param{Array{Float32,3}}, ::Array{Tuple{Array{Float64,3},Array{Int64,1}},1}, ::Nothing, ::Nothing) at .\\none:0",
      " [3] #rnnforw#635(::Base.Iterators.Pairs{Symbol,Union{Nothing, Bool},Tuple{Symbol,Symbol,Symbol},NamedTuple{(:hy, :cy, :batchSizes),Tuple{Bool,Bool,Nothing}}}, ::Function, ::RNN{Array{Float32,3}}, ::Param{Array{Float32,3}}, ::Array{Tuple{Array{Float64,3},Array{Int64,1}},1}, ::Vararg{Any,N} where N) at C:\\Users\\dfhdhsd\\.julia\\packages\\Knet\\05UDD\\src\\rnn.jl:712",
      " [4] (::getfield(Knet, Symbol(\"#kw##rnnforw\")))(::NamedTuple{(:hy, :cy, :batchSizes),Tuple{Bool,Bool,Nothing}}, ::typeof(Knet.rnnforw), ::RNN{Array{Float32,3}}, ::Param{Array{Float32,3}}, ::Array{Tuple{Array{Float64,3},Array{Int64,1}},1}, ::Nothing, ::Nothing) at .\\none:0",
      " [5] #call#571(::Nothing, ::RNN{Array{Float32,3}}, ::Array{Tuple{Array{Float64,3},Array{Int64,1}},1}) at C:\\Users\\dfhdhsd\\.julia\\packages\\Knet\\05UDD\\src\\rnn.jl:192",
      " [6] (::RNN{Array{Float32,3}})(::Array{Tuple{Array{Float64,3},Array{Int64,1}},1}) at C:\\Users\\dfhdhsd\\.julia\\packages\\Knet\\05UDD\\src\\rnn.jl:184",
      " [7] (::Chain)(::Array{Tuple{Array{Float64,3},Array{Int64,1}},1}) at .\\In[116]:5",
      " [8] top-level scope at In[692]:1"
     ]
    }
   ],
   "source": [
    "mymodel1(dtst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.514376439046779"
      ]
     },
     "execution_count": 734,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mymodel2(dtrn,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel2(dtst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.02e+00  68.22%┣█████████████▋      ┫ 8557/12543 [10:14/14:59, 13.95i/s]33/12543 [00:05/31:35, 6.62i/s]46/12543 [00:06/27:19, 7.65i/s]┫ 51/12543 [00:07/29:04, 7.19i/s]54/12543 [00:08/32:39, 6.40i/s]67/12543 [00:12/37:02, 5.65i/s]79/12543 [00:14/37:56, 5.51i/s]121/12543 [00:16/28:22, 7.37i/s]137/12543 [00:19/28:58, 7.22i/s]147/12543 [00:20/28:29, 7.34i/s]┫ 156/12543 [00:21/28:12, 7.41i/s]163/12543 [00:22/28:34, 7.32i/s]185/12543 [00:24/27:27, 7.62i/s]202/12543 [00:27/27:37, 7.57i/s]213/12543 [00:28/27:11, 7.69i/s]239/12543 [00:31/27:11, 7.69i/s]┫ 249/12543 [00:32/26:57, 7.76i/s]265/12543 [00:34/27:10, 7.69i/s]283/12543 [00:38/28:17, 7.39i/s]326/12543 [00:48/31:01, 6.74i/s]┫ 366/12543 [00:50/28:49, 7.25i/s]392/12543 [00:53/28:10, 7.42i/s]412/12543 [00:55/27:50, 7.51i/s]443/12543 [00:59/28:02, 7.46i/s]492/12543 [01:04/27:06, 7.71i/s]501/12543 [01:05/27:03, 7.73i/s]507/12543 [01:06/27:09, 7.70i/s]534/12543 [01:08/26:34, 7.87i/s]556/12543 [01:09/25:56, 8.06i/s]577/12543 [01:12/26:15, 7.96i/s]587/12543 [01:13/26:10, 7.99i/s]591/12543 [01:15/26:25, 7.91i/s]┫ 609/12543 [01:17/26:19, 7.94i/s]639/12543 [01:20/26:18, 7.95i/s]647/12543 [01:21/26:18, 7.95i/s]664/12543 [01:24/26:22, 7.93i/s]671/12543 [01:25/26:28, 7.90i/s]┫ 687/12543 [01:27/26:33, 7.88i/s]694/12543 [01:28/26:35, 7.86i/s]748/12543 [01:34/26:16, 7.96i/s]789/12543 [01:37/25:47, 8.11i/s]800/12543 [01:38/25:42, 8.14i/s]818/12543 [01:39/25:26, 8.22i/s]837/12543 [01:42/25:31, 8.19i/s]┫ 859/12543 [01:45/25:29, 8.20i/s]867/12543 [01:46/25:35, 8.17i/s]894/12543 [01:47/25:03, 8.35i/s]919/12543 [01:50/24:55, 8.39i/s]1059/12543 [01:58/23:20, 8.96i/s]1077/12543 [01:59/23:08, 9.03i/s]┫ 1107/12543 [02:03/23:09, 9.03i/s]1152/12543 [02:09/23:21, 8.95i/s]                   ┫ 1200/12543 [02:13/23:08, 9.04i/s]1228/12543 [02:16/23:08, 9.03i/s]                   ┫ 1239/12543 [02:17/23:06, 9.05i/s]1247/12543 [02:18/23:11, 9.02i/s]1253/12543 [02:19/23:15, 8.99i/s]1262/12543 [02:21/23:18, 8.97i/s]1285/12543 [02:22/23:07, 9.04i/s]┫ 1290/12543 [02:23/23:13, 9.00i/s]                  ┫ 1301/12543 [02:24/23:11, 9.02i/s]▏                  ┫ 1315/12543 [02:26/23:17, 8.98i/s]1336/12543 [02:29/23:16, 8.99i/s]▏                  ┫ 1343/12543 [02:30/23:20, 8.96i/s]1373/12543 [02:35/23:34, 8.87i/s]1388/12543 [02:37/23:39, 8.84i/s]                  ┫ 1404/12543 [02:40/23:47, 8.79i/s]1428/12543 [02:43/23:54, 8.75i/s]1437/12543 [02:44/23:54, 8.75i/s]1449/12543 [02:45/23:51, 8.76i/s]1458/12543 [02:46/23:51, 8.76i/s]1484/12543 [02:48/23:44, 8.81i/s]1500/12543 [02:51/23:48, 8.78i/s]1563/12543 [02:59/23:58, 8.73i/s]1577/12543 [03:00/23:54, 8.75i/s]1698/12543 [03:08/23:11, 9.02i/s]1869/12543 [03:13/21:38, 9.66i/s]1987/12543 [03:19/20:57, 9.98i/s]2095/12543 [03:22/20:11, 10.36i/s]2118/12543 [03:24/20:10, 10.37i/s]2123/12543 [03:25/20:14, 10.33i/s]2162/12543 [03:28/20:05, 10.41i/s]2421/12543 [03:36/18:40, 11.20i/s]2458/12543 [03:37/18:29, 11.31i/s]2479/12543 [03:38/18:24, 11.36i/s]2504/12543 [03:39/18:19, 11.41i/s]2522/12543 [03:40/18:16, 11.44i/s]2614/12543 [03:45/18:00, 11.62i/s]2675/12543 [03:47/17:45, 11.78i/s]2727/12543 [03:50/17:36, 11.88i/s]2776/12543 [03:52/17:27, 11.98i/s]2990/12543 [03:57/16:36, 12.59i/s]3061/12543 [04:01/16:27, 12.71i/s]3102/12543 [04:04/16:28, 12.69i/s]3129/12543 [04:07/16:29, 12.69i/s]3142/12543 [04:08/16:29, 12.69i/s]3227/12543 [04:11/16:16, 12.85i/s]3290/12543 [04:13/16:06, 12.99i/s]3315/12543 [04:14/16:03, 13.03i/s]┫ 3350/12543 [04:16/15:58, 13.10i/s]3367/12543 [04:17/15:57, 13.11i/s]3374/12543 [04:18/15:59, 13.08i/s]3395/12543 [04:19/15:58, 13.10i/s]3420/12543 [04:21/15:58, 13.09i/s]3435/12543 [04:22/15:58, 13.10i/s]┫ 3459/12543 [04:23/15:55, 13.13i/s]3541/12543 [04:28/15:48, 13.22i/s]3619/12543 [04:32/15:44, 13.29i/s]┫ 3719/12543 [04:39/15:41, 13.33i/s]3735/12543 [04:40/15:41, 13.34i/s]3741/12543 [04:41/15:43, 13.30i/s]3806/12543 [04:47/15:45, 13.27i/s]             ┫ 3837/12543 [04:50/15:48, 13.23i/s]3977/12543 [04:56/15:33, 13.44i/s]3989/12543 [04:57/15:34, 13.43i/s]4045/12543 [04:59/15:28, 13.51i/s]┫ 4072/12543 [05:00/15:25, 13.55i/s]4235/12543 [05:07/15:10, 13.79i/s]4291/12543 [05:08/15:01, 13.92i/s]4583/12543 [05:18/14:32, 14.39i/s]4649/12543 [05:23/14:32, 14.38i/s]4708/12543 [05:27/14:30, 14.41i/s]4722/12543 [05:28/14:31, 14.40i/s]4787/12543 [05:30/14:25, 14.50i/s]4836/12543 [05:32/14:22, 14.55i/s]4872/12543 [05:35/14:21, 14.56i/s]4941/12543 [05:37/14:15, 14.68i/s]┫ 4972/12543 [05:39/14:15, 14.66i/s]4991/12543 [05:40/14:15, 14.67i/s]┫ 5006/12543 [05:41/14:15, 14.67i/s]5021/12543 [05:42/14:15, 14.67i/s]5081/12543 [05:45/14:12, 14.71i/s]5120/12543 [05:49/14:14, 14.68i/s]5149/12543 [05:52/14:17, 14.63i/s]5169/12543 [05:53/14:17, 14.64i/s]5188/12543 [05:54/14:16, 14.65i/s]┫ 5198/12543 [05:55/14:17, 14.63i/s]5219/12543 [05:56/14:16, 14.65i/s]5347/12543 [06:02/14:10, 14.76i/s]5363/12543 [06:03/14:10, 14.76i/s]5416/12543 [06:09/14:14, 14.68i/s]5468/12543 [06:11/14:12, 14.73i/s]5563/12543 [06:17/14:09, 14.77i/s]5584/12543 [06:18/14:09, 14.78i/s]5602/12543 [06:19/14:08, 14.78i/s]5655/12543 [06:22/14:08, 14.79i/s]5733/12543 [06:28/14:08, 14.78i/s]5808/12543 [06:33/14:10, 14.76i/s]5881/12543 [06:38/14:09, 14.78i/s]5915/12543 [06:40/14:08, 14.79i/s]5941/12543 [06:43/14:12, 14.73i/s]5950/12543 [06:44/14:12, 14.71i/s]5975/12543 [06:46/14:13, 14.70i/s]6005/12543 [06:47/14:11, 14.74i/s]6029/12543 [06:50/14:13, 14.71i/s]┫ 6050/12543 [06:52/14:15, 14.68i/s]6061/12543 [06:53/14:15, 14.67i/s]6073/12543 [06:54/14:16, 14.66i/s]6091/12543 [06:57/14:18, 14.62i/s]6096/12543 [06:58/14:20, 14.59i/s]6121/12543 [07:00/14:21, 14.57i/s]6134/12543 [07:01/14:21, 14.56i/s]6175/12543 [07:07/14:28, 14.46i/s]6208/12543 [07:10/14:29, 14.43i/s]6221/12543 [07:11/14:29, 14.43i/s]6337/12543 [07:19/14:29, 14.43i/s]6346/12543 [07:21/14:31, 14.41i/s]6401/12543 [07:25/14:32, 14.39i/s]┣██████████▏         ┫ 6412/12543 [07:26/14:32, 14.38i/s]6420/12543 [07:27/14:33, 14.36i/s]6427/12543 [07:28/14:34, 14.35i/s]6435/12543 [07:29/14:36, 14.33i/s]6448/12543 [07:30/14:36, 14.32i/s]6554/12543 [07:39/14:39, 14.28i/s]6566/12543 [07:40/14:39, 14.27i/s]6595/12543 [07:42/14:39, 14.27i/s]┫ 6636/12543 [07:46/14:40, 14.25i/s]6680/12543 [07:48/14:38, 14.28i/s]6731/12543 [07:52/14:40, 14.25i/s]┫ 6735/12543 [07:54/14:42, 14.22i/s]6763/12543 [07:57/14:45, 14.17i/s]6767/12543 [07:58/14:47, 14.15i/s]6779/12543 [08:01/14:50, 14.10i/s]6819/12543 [08:12/15:05, 13.86i/s]6834/12543 [08:13/15:06, 13.85i/s]6843/12543 [08:14/15:06, 13.84i/s]6865/12543 [08:18/15:10, 13.79i/s]6879/12543 [08:19/15:10, 13.79i/s]6888/12543 [08:20/15:11, 13.78i/s]6899/12543 [08:21/15:11, 13.77i/s]6932/12543 [08:25/15:13, 13.74i/s]6949/12543 [08:27/15:15, 13.71i/s]6969/12543 [08:29/15:16, 13.69i/s]6998/12543 [08:32/15:17, 13.68i/s]┫ 7007/12543 [08:33/15:18, 13.67i/s]┫ 7016/12543 [08:34/15:18, 13.66i/s]7065/12543 [08:38/15:20, 13.64i/s]7099/12543 [08:40/15:19, 13.65i/s]7123/12543 [08:41/15:18, 13.67i/s]7185/12543 [08:46/15:18, 13.67i/s]7209/12543 [08:47/15:16, 13.69i/s]┫ 7215/12543 [08:48/15:18, 13.67i/s]7259/12543 [08:51/15:18, 13.67i/s]7291/12543 [08:53/15:17, 13.68i/s]7315/12543 [08:54/15:16, 13.70i/s]7345/12543 [08:55/15:14, 13.73i/s]▊        ┫ 7417/12543 [08:59/15:12, 13.75i/s]┫ 7448/12543 [09:00/15:10, 13.79i/s]7475/12543 [09:02/15:10, 13.78i/s]7510/12543 [09:06/15:12, 13.76i/s]7529/12543 [09:08/15:13, 13.74i/s]7576/12543 [09:10/15:11, 13.77i/s]7582/12543 [09:11/15:12, 13.75i/s]7722/12543 [09:18/15:07, 13.83i/s]7793/12543 [09:21/15:04, 13.88i/s]7810/12543 [09:22/15:03, 13.89i/s]7854/12543 [09:24/15:02, 13.91i/s]7952/12543 [09:30/14:59, 13.95i/s]7966/12543 [09:31/14:59, 13.95i/s]7980/12543 [09:32/14:59, 13.94i/s]8057/12543 [09:37/14:58, 13.97i/s]8116/12543 [09:40/14:56, 13.99i/s]8193/12543 [09:44/14:54, 14.03i/s]8202/12543 [09:45/14:55, 14.01i/s]8276/12543 [09:51/14:55, 14.01i/s]8292/12543 [09:52/14:55, 14.01i/s]8330/12543 [09:54/14:55, 14.02i/s]8347/12543 [09:55/14:55, 14.02i/s]┫ 8367/12543 [09:57/14:54, 14.03i/s]8372/12543 [09:58/14:56, 14.00i/s]8394/12543 [10:00/14:57, 13.99i/s]8419/12543 [10:01/14:56, 14.00i/s]8437/12543 [10:03/14:56, 14.00i/s]8476/12543 [10:07/14:59, 13.96i/s]8490/12543 [10:08/14:59, 13.96i/s]8502/12543 [10:09/14:59, 13.95i/s]▌      ┫ 8542/12543 [10:12/14:59, 13.95i/s]4.99e-01  100.00%┣█████████████████▉┫ 12543/12543 [14:49/14:49, 14.12i/s]8609/12543 [10:19/15:02, 13.90i/s]8628/12543 [10:20/15:02, 13.91i/s]┫ 8682/12543 [10:25/15:03, 13.89i/s]8697/12543 [10:26/15:03, 13.89i/s]8727/12543 [10:28/15:03, 13.89i/s]8823/12543 [10:33/15:00, 13.94i/s]8856/12543 [10:35/15:00, 13.94i/s]9046/12543 [10:45/14:54, 14.02i/s]9057/12543 [10:46/14:55, 14.02i/s]9064/12543 [10:47/14:56, 14.00i/s]9075/12543 [10:48/14:56, 14.00i/s]9146/12543 [10:52/14:54, 14.03i/s]9172/12543 [10:54/14:54, 14.02i/s]9198/12543 [10:55/14:53, 14.04i/s]9271/12543 [11:00/14:53, 14.05i/s]9284/12543 [11:01/14:53, 14.05i/s]9314/12543 [11:03/14:53, 14.05i/s]9324/12543 [11:04/14:53, 14.04i/s]┫ 9366/12543 [11:08/14:55, 14.01i/s]9376/12543 [11:09/14:56, 14.01i/s]9383/12543 [11:10/14:56, 14.00i/s]9397/12543 [11:11/14:56, 14.00i/s]9427/12543 [11:12/14:55, 14.02i/s]9450/12543 [11:13/14:54, 14.03i/s]9467/12543 [11:15/14:54, 14.04i/s]9475/12543 [11:16/14:54, 14.03i/s]9490/12543 [11:18/14:56, 14.00i/s]9506/12543 [11:20/14:57, 13.98i/s]9535/12543 [11:21/14:56, 14.00i/s]9555/12543 [11:24/14:57, 13.98i/s]9562/12543 [11:25/14:58, 13.97i/s]9578/12543 [11:26/14:58, 13.97i/s]9655/12543 [11:30/14:57, 13.99i/s]    ┫ 9664/12543 [11:31/14:57, 13.98i/s]9700/12543 [11:34/14:57, 13.98i/s]9724/12543 [11:37/14:59, 13.95i/s]9789/12543 [11:42/14:59, 13.95i/s]9803/12543 [11:43/14:59, 13.95i/s]9874/12543 [11:46/14:57, 13.99i/s]9928/12543 [11:48/14:55, 14.02i/s]9941/12543 [11:49/14:55, 14.02i/s]10004/12543 [11:52/14:53, 14.04i/s]10107/12543 [11:57/14:49, 14.10i/s]┫ 10227/12543 [12:02/14:45, 14.17i/s]10256/12543 [12:03/14:44, 14.19i/s]10277/12543 [12:04/14:44, 14.19i/s]10292/12543 [12:05/14:44, 14.19i/s]10408/12543 [12:11/14:41, 14.24i/s]10431/12543 [12:12/14:40, 14.26i/s]10444/12543 [12:13/14:40, 14.25i/s]10510/12543 [12:16/14:38, 14.28i/s]10649/12543 [12:23/14:35, 14.34i/s]10687/12543 [12:25/14:35, 14.34i/s]10702/12543 [12:26/14:35, 14.34i/s]10774/12543 [12:30/14:33, 14.37i/s]10829/12543 [12:33/14:32, 14.39i/s]10841/12543 [12:34/14:32, 14.38i/s]10943/12543 [12:39/14:30, 14.41i/s]10993/12543 [12:42/14:29, 14.43i/s]11045/12543 [12:45/14:29, 14.44i/s]11069/12543 [12:47/14:29, 14.43i/s]11096/12543 [12:49/14:30, 14.42i/s]11134/12543 [12:51/14:29, 14.44i/s]11201/12543 [12:56/14:29, 14.44i/s]11220/12543 [12:57/14:29, 14.44i/s]11247/12543 [12:59/14:29, 14.44i/s]11269/12543 [13:00/14:29, 14.44i/s]┫ 11286/12543 [13:01/14:29, 14.44i/s]11313/12543 [13:04/14:29, 14.44i/s]11337/12543 [13:06/14:29, 14.43i/s]┫ 11380/12543 [13:09/14:30, 14.42i/s]11393/12543 [13:10/14:30, 14.42i/s]11442/12543 [13:13/14:30, 14.42i/s]11471/12543 [13:16/14:30, 14.42i/s]11480/12543 [13:17/14:30, 14.41i/s]11531/12543 [13:20/14:30, 14.42i/s]11626/12543 [13:25/14:29, 14.44i/s]11643/12543 [13:26/14:29, 14.44i/s]11705/12543 [13:31/14:29, 14.43i/s]11714/12543 [13:32/14:30, 14.42i/s] ┫ 11735/12543 [13:34/14:31, 14.41i/s]11800/12543 [13:39/14:31, 14.40i/s]11821/12543 [13:41/14:31, 14.39i/s]11830/12543 [13:42/14:32, 14.39i/s]11881/12543 [13:48/14:34, 14.35i/s]11885/12543 [13:49/14:35, 14.34i/s]11901/12543 [13:50/14:35, 14.34i/s]11949/12543 [13:54/14:35, 14.33i/s]11968/12543 [13:55/14:35, 14.34i/s]11988/12543 [13:56/14:35, 14.34i/s]12041/12543 [13:58/14:33, 14.36i/s]┫ 12088/12543 [14:04/14:36, 14.32i/s]12097/12543 [14:05/14:36, 14.31i/s]12108/12543 [14:06/14:37, 14.31i/s]12200/12543 [14:14/14:38, 14.28i/s]12241/12543 [14:18/14:39, 14.27i/s]12264/12543 [14:19/14:38, 14.28i/s]12283/12543 [14:21/14:39, 14.27i/s]┫ 12300/12543 [14:22/14:39, 14.27i/s]12312/12543 [14:23/14:39, 14.27i/s]┫ 12319/12543 [14:24/14:40, 14.26i/s]12328/12543 [14:25/14:40, 14.25i/s]12363/12543 [14:27/14:40, 14.25i/s]12375/12543 [14:29/14:40, 14.25i/s]12425/12543 [14:34/14:43, 14.21i/s]12488/12543 [14:40/14:44, 14.19i/s]12494/12543 [14:41/14:44, 14.18i/s]12514/12543 [14:44/14:46, 14.15i/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12543-element Array{Float64,1}:\n",
       " 3.3972679682479403 \n",
       " 2.944077958326243  \n",
       " 2.8880662027033206 \n",
       " 2.8314236571895517 \n",
       " 3.610724608512998  \n",
       " 2.6377387327405724 \n",
       " 2.6357795900340952 \n",
       " 2.831147497143261  \n",
       " 3.582933695115336  \n",
       " 3.04301685864481   \n",
       " 2.994654805826147  \n",
       " 3.4008567352251506 \n",
       " 2.9949588454527585 \n",
       " ⋮                  \n",
       " 0.35260356557272277\n",
       " 0.3669984439431611 \n",
       " 2.0905570706705277 \n",
       " 1.3207603488167554 \n",
       " 1.3575671494623547 \n",
       " 0.9675048087907924 \n",
       " 0.7501239384432881 \n",
       " 1.6698215053740249 \n",
       " 1.2444769867806043 \n",
       " 0.8196766186693428 \n",
       " 1.2821853223097497 \n",
       " 0.4985160145118432 "
      ]
     },
     "execution_count": 662,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collect(flatten(Knet.progress(Knet.sgd(mymodel1,dtrn))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train from scratch? stdin> y\n",
      "2.05e+00  1.70%┣▎                 ┫ 3395/200000 [11:04/10:52:17, 5.11i/s]1.40e+00  4.01%┣▋                 ┫ 7992/200000 [21:14/08:51:24, 6.27i/s]3408/200000 [11:06/10:51:52, 5.11i/s]3419/200000 [11:09/10:51:49, 5.11i/s]3458/200000 [11:13/10:48:48, 5.14i/s]3474/200000 [11:14/10:47:07, 5.15i/s]3515/200000 [11:19/10:43:50, 5.18i/s]3537/200000 [11:22/10:42:54, 5.18i/s]3563/200000 [11:26/10:41:22, 5.20i/s]3612/200000 [11:31/10:37:55, 5.23i/s]3623/200000 [11:33/10:37:55, 5.23i/s]3628/200000 [11:34/10:38:01, 5.22i/s]3641/200000 [11:36/10:36:46, 5.23i/s]3678/200000 [11:39/10:33:08, 5.26i/s]3705/200000 [11:44/10:33:18, 5.26i/s]3709/200000 [11:45/10:33:31, 5.26i/s]                 ┫ 3726/200000 [11:49/10:34:18, 5.26i/s]3737/200000 [11:51/10:34:20, 5.25i/s]3753/200000 [11:54/10:34:29, 5.25i/s]3769/200000 [11:58/10:34:35, 5.25i/s]3798/200000 [12:02/10:33:28, 5.26i/s]3802/200000 [12:03/10:33:41, 5.26i/s]3809/200000 [12:05/10:34:24, 5.25i/s]3812/200000 [12:06/10:34:56, 5.25i/s]3836/200000 [12:11/10:35:11, 5.25i/s]3848/200000 [12:12/10:34:17, 5.26i/s]3871/200000 [12:14/10:31:46, 5.28i/s]3892/200000 [12:16/10:30:27, 5.29i/s]3895/200000 [12:17/10:30:52, 5.28i/s]3907/200000 [12:18/10:29:56, 5.29i/s]3917/200000 [12:20/10:30:09, 5.29i/s]▎                 ┫ 3942/200000 [12:23/10:28:11, 5.31i/s]3956/200000 [12:25/10:27:45, 5.31i/s]┫ 4035/200000 [12:36/10:24:22, 5.34i/s]4067/200000 [12:39/10:22:10, 5.36i/s]4084/200000 [12:41/10:21:23, 5.36i/s]4106/200000 [12:44/10:19:57, 5.38i/s]4162/200000 [12:47/10:14:23, 5.43i/s]4192/200000 [12:51/10:12:41, 5.44i/s]4199/200000 [12:52/10:12:28, 5.44i/s]4234/200000 [12:56/10:10:56, 5.46i/s]4315/200000 [13:01/10:03:24, 5.52i/s]┫ 4362/200000 [13:04/09:59:29, 5.56i/s]4415/200000 [13:09/09:55:34, 5.60i/s]4470/200000 [13:12/09:50:46, 5.64i/s]4496/200000 [13:14/09:49:01, 5.66i/s]4522/200000 [13:17/09:47:38, 5.67i/s]4550/200000 [13:20/09:45:43, 5.69i/s]4578/200000 [13:22/09:43:37, 5.71i/s]4583/200000 [13:23/09:43:49, 5.71i/s]4589/200000 [13:24/09:44:00, 5.71i/s]4594/200000 [13:25/09:44:14, 5.71i/s]4604/200000 [13:26/09:43:52, 5.71i/s]4629/200000 [13:28/09:42:11, 5.73i/s]4647/200000 [13:31/09:41:45, 5.73i/s]4664/200000 [13:34/09:41:26, 5.73i/s]4699/200000 [13:36/09:38:40, 5.76i/s]4754/200000 [13:42/09:36:07, 5.79i/s]4835/200000 [13:50/09:32:18, 5.82i/s]4839/200000 [13:51/09:32:31, 5.82i/s]4845/200000 [13:52/09:32:39, 5.82i/s]4872/200000 [13:55/09:31:03, 5.84i/s]4896/200000 [13:57/09:29:48, 5.85i/s]┫ 4929/200000 [13:58/09:26:40, 5.88i/s]4945/200000 [13:59/09:25:32, 5.89i/s]4952/200000 [14:00/09:25:33, 5.89i/s]4983/200000 [14:04/09:24:40, 5.90i/s]5031/200000 [14:09/09:22:22, 5.93i/s]5074/200000 [14:14/09:20:44, 5.94i/s]5104/200000 [14:17/09:19:33, 5.96i/s]5114/200000 [14:19/09:19:50, 5.95i/s]5120/200000 [14:20/09:19:56, 5.95i/s]5126/200000 [14:21/09:20:08, 5.95i/s]5141/200000 [14:24/09:20:11, 5.95i/s]5148/200000 [14:25/09:20:12, 5.95i/s]5171/200000 [14:29/09:20:12, 5.95i/s]5187/200000 [14:30/09:19:07, 5.96i/s]5193/200000 [14:31/09:19:17, 5.96i/s]5240/200000 [14:36/09:17:09, 5.98i/s]5292/200000 [14:39/09:13:39, 6.02i/s]5349/200000 [14:45/09:11:26, 6.04i/s]5368/200000 [14:47/09:11:05, 6.05i/s]5384/200000 [14:50/09:10:48, 6.05i/s]5389/200000 [14:51/09:11:04, 6.05i/s]5395/200000 [14:52/09:11:14, 6.05i/s]5412/200000 [14:55/09:11:10, 6.05i/s]5418/200000 [14:56/09:11:18, 6.05i/s]┫ 5441/200000 [14:58/09:10:25, 6.06i/s]5463/200000 [15:01/09:09:34, 6.07i/s]5584/200000 [15:14/09:05:35, 6.11i/s]5596/200000 [15:15/09:05:01, 6.12i/s]▌                 ┫ 5606/200000 [15:16/09:04:48, 6.12i/s]5623/200000 [15:18/09:04:27, 6.12i/s]5643/200000 [15:21/09:03:50, 6.13i/s]5681/200000 [15:26/09:03:12, 6.14i/s]5700/200000 [15:28/09:02:46, 6.14i/s]5727/200000 [15:32/09:02:13, 6.15i/s]5735/200000 [15:34/09:02:44, 6.14i/s]5777/200000 [15:40/09:02:20, 6.15i/s]5823/200000 [15:45/09:00:53, 6.16i/s]                 ┫ 5852/200000 [15:48/09:00:13, 6.17i/s]5891/200000 [15:54/09:00:00, 6.17i/s]5903/200000 [15:55/08:59:29, 6.18i/s]5936/200000 [16:01/08:59:49, 6.17i/s]5943/200000 [16:03/08:59:55, 6.17i/s]5949/200000 [16:04/09:00:06, 6.17i/s]5954/200000 [16:05/09:00:21, 6.17i/s]5973/200000 [16:08/09:00:05, 6.17i/s]5987/200000 [16:09/08:59:25, 6.18i/s]5993/200000 [16:10/08:59:30, 6.18i/s]▌                 ┫ 6033/200000 [16:16/08:59:16, 6.18i/s]6039/200000 [16:17/08:59:24, 6.18i/s]6055/200000 [16:20/08:59:25, 6.18i/s]6061/200000 [16:21/08:59:35, 6.18i/s]6078/200000 [16:24/08:59:31, 6.18i/s]6086/200000 [16:25/08:59:22, 6.18i/s]6091/200000 [16:26/08:59:49, 6.17i/s]6100/200000 [16:29/09:00:25, 6.17i/s]6112/200000 [16:30/08:59:55, 6.17i/s]6136/200000 [16:34/08:59:44, 6.18i/s]6149/200000 [16:36/08:59:58, 6.17i/s]6174/200000 [16:42/09:00:46, 6.16i/s]6188/200000 [16:44/09:00:43, 6.16i/s]6196/200000 [16:45/09:00:35, 6.17i/s]6221/200000 [16:48/09:00:17, 6.17i/s]6245/200000 [16:51/08:59:33, 6.18i/s]6266/200000 [16:53/08:58:57, 6.18i/s]┫ 6282/200000 [16:55/08:58:42, 6.19i/s]6290/200000 [16:57/08:58:42, 6.19i/s]6307/200000 [16:59/08:58:35, 6.19i/s]6321/200000 [17:00/08:58:07, 6.19i/s]┫ 6329/200000 [17:02/08:58:08, 6.19i/s]6335/200000 [17:03/08:58:17, 6.19i/s]6340/200000 [17:04/08:58:34, 6.19i/s]6379/200000 [17:10/08:58:23, 6.19i/s]6398/200000 [17:13/08:58:09, 6.19i/s]6403/200000 [17:14/08:58:24, 6.19i/s]6426/200000 [17:19/08:58:54, 6.19i/s]6432/200000 [17:20/08:59:04, 6.18i/s]6463/200000 [17:25/08:58:48, 6.19i/s]6497/200000 [17:30/08:58:50, 6.19i/s]6505/200000 [17:31/08:58:48, 6.19i/s]6530/200000 [17:35/08:58:42, 6.19i/s]┫ 6545/200000 [17:38/08:58:50, 6.19i/s]6553/200000 [17:39/08:58:51, 6.19i/s]6568/200000 [17:42/08:58:47, 6.19i/s]6592/200000 [17:45/08:58:35, 6.19i/s]6621/200000 [17:50/08:58:34, 6.19i/s]6635/200000 [17:52/08:58:42, 6.19i/s]6654/200000 [17:53/08:57:41, 6.20i/s]6676/200000 [17:55/08:56:56, 6.21i/s]6681/200000 [17:57/08:57:08, 6.21i/s]6709/200000 [18:00/08:56:40, 6.21i/s]┫ 6724/200000 [18:01/08:55:58, 6.22i/s]6740/200000 [18:06/08:56:56, 6.21i/s]6763/200000 [18:10/08:57:23, 6.20i/s]6767/200000 [18:12/08:57:43, 6.20i/s]6780/200000 [18:16/08:58:46, 6.19i/s]┫ 6795/200000 [18:21/09:00:07, 6.17i/s]6810/200000 [18:25/09:00:51, 6.16i/s]6819/200000 [18:28/09:01:35, 6.15i/s]6843/200000 [18:31/09:01:26, 6.16i/s]▌                 ┫ 6854/200000 [18:34/09:01:48, 6.15i/s]6859/200000 [18:35/09:02:01, 6.15i/s]6864/200000 [18:36/09:02:06, 6.15i/s]6892/200000 [18:41/09:02:19, 6.15i/s]6927/200000 [18:48/09:02:42, 6.14i/s]6967/200000 [18:55/09:03:06, 6.14i/s]6972/200000 [18:56/09:03:15, 6.14i/s]6985/200000 [18:59/09:03:28, 6.13i/s]6994/200000 [19:00/09:03:22, 6.13i/s]┫ 7001/200000 [19:01/09:03:26, 6.13i/s]7035/200000 [19:08/09:03:54, 6.13i/s]▋                 ┫ 7047/200000 [19:10/09:04:07, 6.13i/s]┫ 7069/200000 [19:14/09:04:02, 6.13i/s]7085/200000 [19:16/09:04:00, 6.13i/s]7109/200000 [19:20/09:03:58, 6.13i/s]7173/200000 [19:30/09:03:43, 6.13i/s]7213/200000 [19:36/09:03:31, 6.13i/s]7220/200000 [19:37/09:03:28, 6.13i/s]7255/200000 [19:42/09:03:00, 6.14i/s]7273/200000 [19:44/09:02:50, 6.14i/s]7283/200000 [19:45/09:02:33, 6.14i/s]7289/200000 [19:47/09:02:37, 6.14i/s]7309/200000 [19:49/09:02:09, 6.15i/s]┫ 7316/200000 [19:50/09:02:12, 6.15i/s]7328/200000 [19:51/09:01:55, 6.15i/s]7386/200000 [19:57/09:00:11, 6.17i/s]7396/200000 [19:58/09:00:02, 6.17i/s]7416/200000 [20:01/08:59:37, 6.18i/s]7453/200000 [20:04/08:58:31, 6.19i/s]7462/200000 [20:05/08:58:26, 6.19i/s]7474/200000 [20:06/08:58:01, 6.20i/s]┫ 7478/200000 [20:08/08:58:16, 6.19i/s]7501/200000 [20:11/08:58:10, 6.19i/s]7517/200000 [20:14/08:58:16, 6.19i/s]7544/200000 [20:17/08:57:51, 6.20i/s]7587/200000 [20:23/08:57:25, 6.20i/s]▋                 ┫ 7619/200000 [20:28/08:57:17, 6.20i/s]7637/200000 [20:30/08:57:02, 6.21i/s]▋                 ┫ 7646/200000 [20:31/08:56:51, 6.21i/s]7688/200000 [20:35/08:55:30, 6.22i/s]7763/200000 [20:43/08:53:56, 6.24i/s]7769/200000 [20:45/08:54:01, 6.24i/s]7780/200000 [20:46/08:53:49, 6.24i/s]┫ 7798/200000 [20:49/08:53:41, 6.25i/s]┫ 7838/200000 [20:54/08:53:09, 6.25i/s]7881/200000 [20:58/08:52:11, 6.26i/s]7906/200000 [21:02/08:52:05, 6.26i/s]7919/200000 [21:03/08:51:46, 6.27i/s]7935/200000 [21:05/08:51:36, 6.27i/s]7950/200000 [21:08/08:51:31, 6.27i/s]7973/200000 [21:11/08:51:34, 6.27i/s]8001/200000 [21:15/08:51:20, 6.27i/s]8002/200000 [22:38/09:25:39, 5.89i/s]8009/200000 [22:39/09:25:35, 5.89i/s]1.33e+00  6.50%┣█                ┫ 12994/200000 [32:57/08:27:08, 6.57i/s]8034/200000 [22:44/09:25:53, 5.89i/s]8057/200000 [22:49/09:26:23, 5.89i/s]8083/200000 [22:56/09:27:15, 5.88i/s]8140/200000 [23:02/09:25:48, 5.89i/s]8158/200000 [23:06/09:26:08, 5.89i/s]8172/200000 [23:08/09:26:06, 5.89i/s]8184/200000 [23:10/09:26:10, 5.89i/s]8188/200000 [23:11/09:26:24, 5.89i/s]8194/200000 [23:12/09:26:24, 5.89i/s]▋                 ┫ 8206/200000 [23:15/09:26:34, 5.88i/s]8214/200000 [23:16/09:26:33, 5.88i/s]8244/200000 [23:22/09:26:56, 5.88i/s]                 ┫ 8250/200000 [23:23/09:27:02, 5.88i/s]8257/200000 [23:25/09:27:05, 5.88i/s]8268/200000 [23:27/09:27:23, 5.87i/s]8278/200000 [23:30/09:27:45, 5.87i/s]8286/200000 [23:32/09:28:12, 5.87i/s]8291/200000 [23:33/09:28:16, 5.87i/s]8297/200000 [23:35/09:28:20, 5.87i/s]┫ 8315/200000 [23:39/09:29:01, 5.86i/s]8395/200000 [23:53/09:29:02, 5.86i/s]8400/200000 [23:54/09:29:12, 5.86i/s]8416/200000 [23:57/09:29:09, 5.86i/s]8421/200000 [23:58/09:29:19, 5.85i/s]8446/200000 [24:03/09:29:40, 5.85i/s]8449/200000 [24:05/09:29:58, 5.85i/s]8474/200000 [24:09/09:29:50, 5.85i/s]8505/200000 [24:14/09:29:55, 5.85i/s]8520/200000 [24:17/09:29:54, 5.85i/s]8526/200000 [24:18/09:30:00, 5.85i/s]8554/200000 [24:20/09:28:56, 5.86i/s]8566/200000 [24:23/09:29:07, 5.86i/s]8594/200000 [24:29/09:29:47, 5.85i/s]8601/200000 [24:30/09:29:51, 5.85i/s]8621/200000 [24:34/09:29:59, 5.85i/s]8636/200000 [24:37/09:30:00, 5.85i/s]┫ 8648/200000 [24:39/09:30:13, 5.85i/s]8660/200000 [24:42/09:30:21, 5.84i/s]8666/200000 [24:43/09:30:27, 5.84i/s]8672/200000 [24:44/09:30:34, 5.84i/s]8679/200000 [24:45/09:30:30, 5.84i/s]8689/200000 [24:48/09:30:47, 5.84i/s]┫ 8694/200000 [24:49/09:30:56, 5.84i/s]┫ 8700/200000 [24:50/09:31:01, 5.84i/s]8707/200000 [24:52/09:31:03, 5.84i/s]8752/200000 [24:59/09:30:47, 5.84i/s]                 ┫ 8761/200000 [25:00/09:30:35, 5.84i/s]8823/200000 [25:11/09:30:45, 5.84i/s]┫ 8827/200000 [25:12/09:30:58, 5.84i/s]┫ 8858/200000 [25:16/09:30:40, 5.84i/s]8867/200000 [25:19/09:31:02, 5.84i/s]8885/200000 [25:21/09:30:41, 5.84i/s]┫ 8923/200000 [25:27/09:30:31, 5.84i/s]8966/200000 [25:32/09:29:38, 5.85i/s]8986/200000 [25:35/09:29:21, 5.85i/s]9005/200000 [25:37/09:29:07, 5.86i/s]▊                 ┫ 9010/200000 [25:39/09:29:15, 5.86i/s]9060/200000 [25:45/09:28:15, 5.87i/s]9080/200000 [25:47/09:27:55, 5.87i/s]9089/200000 [25:48/09:27:50, 5.87i/s]┫ 9107/200000 [25:52/09:28:07, 5.87i/s]9120/200000 [25:55/09:28:14, 5.87i/s]┫ 9132/200000 [25:57/09:28:26, 5.86i/s]9137/200000 [25:59/09:28:35, 5.86i/s]9141/200000 [26:00/09:28:42, 5.86i/s]9154/200000 [26:02/09:28:41, 5.86i/s]9162/200000 [26:03/09:28:36, 5.86i/s]9209/200000 [26:10/09:28:15, 5.87i/s]9214/200000 [26:11/09:28:22, 5.86i/s]9227/200000 [26:14/09:28:30, 5.86i/s]9246/200000 [26:17/09:28:42, 5.86i/s]9257/200000 [26:20/09:28:51, 5.86i/s]9261/200000 [26:21/09:29:01, 5.86i/s]9294/200000 [26:27/09:29:11, 5.86i/s]9304/200000 [26:28/09:29:03, 5.86i/s]┫ 9310/200000 [26:30/09:29:08, 5.86i/s]9317/200000 [26:31/09:29:10, 5.86i/s]9326/200000 [26:32/09:29:04, 5.86i/s]9350/200000 [26:37/09:29:28, 5.85i/s]9396/200000 [26:48/09:30:21, 5.84i/s]9400/200000 [26:49/09:30:33, 5.84i/s]9414/200000 [26:52/09:30:37, 5.84i/s]9419/200000 [26:53/09:30:46, 5.84i/s]9429/200000 [26:55/09:31:04, 5.84i/s]9456/200000 [27:00/09:31:14, 5.84i/s]9486/200000 [27:06/09:31:14, 5.84i/s]9499/200000 [27:08/09:31:21, 5.83i/s]9514/200000 [27:12/09:31:47, 5.83i/s]┫ 9536/200000 [27:16/09:31:49, 5.83i/s]9543/200000 [27:17/09:31:51, 5.83i/s]9581/200000 [27:25/09:32:14, 5.83i/s]┫ 9601/200000 [27:27/09:31:50, 5.83i/s]9617/200000 [27:28/09:31:14, 5.84i/s]9624/200000 [27:29/09:31:15, 5.84i/s]9640/200000 [27:30/09:30:39, 5.84i/s]9666/200000 [27:32/09:29:49, 5.85i/s]9691/200000 [27:36/09:29:39, 5.85i/s]┫ 9735/200000 [27:40/09:28:15, 5.87i/s]9773/200000 [27:43/09:27:11, 5.88i/s]▉                 ┫ 9816/200000 [27:47/09:25:58, 5.89i/s]9845/200000 [27:49/09:24:59, 5.90i/s]▉                 ┫ 9874/200000 [27:51/09:24:06, 5.91i/s]9908/200000 [27:53/09:22:52, 5.92i/s]9932/200000 [27:55/09:22:17, 5.93i/s]9973/200000 [28:00/09:21:35, 5.94i/s]9994/200000 [28:03/09:21:15, 5.94i/s]10050/200000 [28:07/09:19:33, 5.96i/s]10112/200000 [28:12/09:17:43, 5.98i/s]10132/200000 [28:16/09:17:52, 5.98i/s]10142/200000 [28:17/09:17:39, 5.98i/s]┫ 10168/200000 [28:19/09:16:58, 5.98i/s]10202/200000 [28:22/09:16:11, 5.99i/s]10218/200000 [28:23/09:15:40, 6.00i/s]10241/200000 [28:26/09:15:08, 6.00i/s]10279/200000 [28:29/09:14:15, 6.01i/s]10312/200000 [28:32/09:13:30, 6.02i/s]10330/200000 [28:35/09:13:21, 6.02i/s]10349/200000 [28:37/09:13:04, 6.03i/s]10421/200000 [28:43/09:10:59, 6.05i/s]10459/200000 [28:45/09:09:49, 6.06i/s]10486/200000 [28:47/09:09:03, 6.07i/s]10531/200000 [28:52/09:08:07, 6.08i/s]┫ 10576/200000 [28:57/09:07:22, 6.09i/s]10588/200000 [28:58/09:07:09, 6.09i/s]10643/200000 [29:01/09:05:19, 6.11i/s]10663/200000 [29:02/09:04:39, 6.12i/s]10674/200000 [29:03/09:04:25, 6.12i/s]10679/200000 [29:04/09:04:31, 6.12i/s]┫ 10738/200000 [29:10/09:03:23, 6.13i/s]10759/200000 [29:13/09:03:00, 6.14i/s]10771/200000 [29:14/09:02:42, 6.14i/s]10780/200000 [29:15/09:02:37, 6.14i/s]10792/200000 [29:16/09:02:20, 6.15i/s]10829/200000 [29:19/09:01:34, 6.15i/s]10844/200000 [29:20/09:01:08, 6.16i/s]10871/200000 [29:22/09:00:09, 6.17i/s]10886/200000 [29:23/08:59:43, 6.18i/s]10913/200000 [29:25/08:59:04, 6.18i/s]11053/200000 [29:37/08:56:01, 6.22i/s]11062/200000 [29:39/08:56:05, 6.22i/s]11074/200000 [29:40/08:55:49, 6.22i/s]11088/200000 [29:43/08:55:54, 6.22i/s]11126/200000 [29:48/08:55:32, 6.22i/s]11144/200000 [29:50/08:55:20, 6.23i/s]┫ 11157/200000 [29:51/08:55:00, 6.23i/s]┫ 11166/200000 [29:52/08:54:57, 6.23i/s]11184/200000 [29:53/08:54:25, 6.24i/s]11225/200000 [29:58/08:53:47, 6.24i/s]┫ 11238/200000 [29:59/08:53:33, 6.25i/s]11255/200000 [30:01/08:53:22, 6.25i/s]11359/200000 [30:11/08:51:19, 6.27i/s]11372/200000 [30:12/08:51:02, 6.28i/s]11395/200000 [30:15/08:51:02, 6.28i/s]11403/200000 [30:16/08:50:58, 6.28i/s]11484/200000 [30:24/08:49:34, 6.29i/s]11525/200000 [30:29/08:49:06, 6.30i/s]11540/200000 [30:30/08:48:42, 6.30i/s]11580/200000 [30:34/08:47:51, 6.31i/s]11599/200000 [30:36/08:47:40, 6.32i/s]11612/200000 [30:37/08:47:26, 6.32i/s]┫ 11621/200000 [30:38/08:47:19, 6.32i/s]11626/200000 [30:40/08:47:26, 6.32i/s]11681/200000 [30:44/08:46:11, 6.33i/s]11687/200000 [30:45/08:46:18, 6.33i/s]11715/200000 [30:50/08:46:29, 6.33i/s]11722/200000 [30:52/08:46:32, 6.33i/s]11755/200000 [30:57/08:46:32, 6.33i/s]11765/200000 [30:58/08:46:28, 6.33i/s]11771/200000 [30:59/08:46:33, 6.33i/s]11785/200000 [31:00/08:46:13, 6.33i/s]11800/200000 [31:03/08:46:16, 6.33i/s]11827/200000 [31:08/08:46:30, 6.33i/s]11834/200000 [31:09/08:46:29, 6.33i/s]11848/200000 [31:10/08:46:13, 6.33i/s]11869/200000 [31:11/08:45:34, 6.34i/s]11893/200000 [31:14/08:45:10, 6.35i/s]11911/200000 [31:16/08:44:59, 6.35i/s]11935/200000 [31:18/08:44:34, 6.35i/s]11947/200000 [31:19/08:44:24, 6.36i/s]11976/200000 [31:23/08:44:13, 6.36i/s]11983/200000 [31:25/08:44:15, 6.36i/s]12047/200000 [31:30/08:43:04, 6.37i/s]12061/200000 [31:31/08:42:44, 6.38i/s]12083/200000 [31:34/08:42:24, 6.38i/s]12142/200000 [31:38/08:41:04, 6.40i/s]12216/200000 [31:45/08:39:53, 6.41i/s]12263/200000 [31:49/08:38:49, 6.42i/s]12282/200000 [31:50/08:38:18, 6.43i/s]12323/200000 [31:53/08:37:23, 6.44i/s]12392/200000 [31:57/08:35:41, 6.46i/s]12425/200000 [32:01/08:35:16, 6.47i/s]┫ 12470/200000 [32:03/08:33:59, 6.49i/s]12484/200000 [32:04/08:33:41, 6.49i/s]12495/200000 [32:05/08:33:33, 6.49i/s]12505/200000 [32:06/08:33:31, 6.49i/s]12522/200000 [32:08/08:33:11, 6.50i/s]12559/200000 [32:11/08:32:35, 6.50i/s]12585/200000 [32:14/08:32:21, 6.51i/s]12606/200000 [32:18/08:32:24, 6.51i/s]12616/200000 [32:19/08:32:16, 6.51i/s]12624/200000 [32:20/08:32:16, 6.51i/s]12651/200000 [32:23/08:32:03, 6.51i/s]12658/200000 [32:24/08:32:01, 6.51i/s]12700/200000 [32:28/08:31:13, 6.52i/s]12707/200000 [32:29/08:31:17, 6.52i/s]12792/200000 [32:37/08:29:56, 6.54i/s]12817/200000 [32:39/08:29:32, 6.54i/s]12839/200000 [32:43/08:29:36, 6.54i/s]12845/200000 [32:44/08:29:40, 6.54i/s]12858/200000 [32:45/08:29:29, 6.54i/s]12888/200000 [32:47/08:28:50, 6.55i/s]12973/200000 [32:55/08:27:22, 6.57i/s]12983/200000 [32:56/08:27:18, 6.57i/s]1.41e+00  8.47%┣█▍               ┫ 16913/200000 [44:17/08:43:42, 6.36i/s]┫ 13010/200000 [32:59/08:27:03, 6.57i/s]┫ 13027/200000 [33:00/08:26:39, 6.58i/s]13039/200000 [33:01/08:26:31, 6.58i/s]13053/200000 [33:02/08:26:14, 6.58i/s]13072/200000 [33:05/08:26:08, 6.59i/s]13112/200000 [33:11/08:26:02, 6.59i/s]13155/200000 [33:18/08:26:20, 6.58i/s]13192/200000 [33:23/08:26:10, 6.59i/s]13214/200000 [33:25/08:25:52, 6.59i/s]13237/200000 [33:28/08:25:33, 6.59i/s]13251/200000 [33:29/08:25:17, 6.60i/s]13287/200000 [33:31/08:24:26, 6.61i/s]13296/200000 [33:32/08:24:25, 6.61i/s]13308/200000 [33:33/08:24:12, 6.61i/s]13323/200000 [33:35/08:24:11, 6.61i/s]13361/200000 [33:39/08:23:40, 6.62i/s]13369/200000 [33:40/08:23:42, 6.62i/s]13379/200000 [33:41/08:23:39, 6.62i/s]13387/200000 [33:43/08:23:36, 6.62i/s]13391/200000 [33:44/08:23:44, 6.62i/s]13398/200000 [33:45/08:23:48, 6.62i/s]13421/200000 [33:49/08:23:54, 6.62i/s]13466/200000 [33:53/08:23:17, 6.62i/s]13481/200000 [33:54/08:22:59, 6.63i/s]┫ 13516/200000 [33:57/08:22:17, 6.64i/s]13537/200000 [33:59/08:22:04, 6.64i/s]13585/200000 [34:06/08:22:03, 6.64i/s]13604/200000 [34:08/08:21:53, 6.64i/s]13623/200000 [34:11/08:21:48, 6.64i/s]13653/200000 [34:14/08:21:33, 6.65i/s]13660/200000 [34:16/08:21:36, 6.65i/s]13692/200000 [34:20/08:21:24, 6.65i/s]13708/200000 [34:22/08:21:20, 6.65i/s]13727/200000 [34:24/08:21:12, 6.65i/s]┫ 13739/200000 [34:27/08:21:36, 6.65i/s]13746/200000 [34:28/08:21:35, 6.65i/s]13767/200000 [34:31/08:21:25, 6.65i/s]13792/200000 [34:35/08:21:23, 6.65i/s]13799/200000 [34:36/08:21:25, 6.65i/s]13849/200000 [34:40/08:20:39, 6.66i/s]┫ 13854/200000 [34:41/08:20:46, 6.66i/s]13863/200000 [34:42/08:20:41, 6.66i/s]13908/200000 [34:48/08:20:30, 6.66i/s]13916/200000 [34:50/08:20:31, 6.66i/s]13943/200000 [34:55/08:20:48, 6.66i/s]13963/200000 [34:58/08:20:56, 6.65i/s]13970/200000 [34:59/08:20:57, 6.65i/s]14010/200000 [35:04/08:20:40, 6.66i/s]14027/200000 [35:07/08:20:41, 6.66i/s]14048/200000 [35:11/08:20:51, 6.66i/s]┫ 14055/200000 [35:12/08:20:54, 6.65i/s]14071/200000 [35:15/08:20:58, 6.65i/s]14086/200000 [35:17/08:20:58, 6.65i/s]14091/200000 [35:19/08:21:11, 6.65i/s]┫ 14096/200000 [35:20/08:21:19, 6.65i/s]14100/200000 [35:21/08:21:28, 6.65i/s]14143/200000 [35:27/08:21:19, 6.65i/s]14155/200000 [35:30/08:21:30, 6.65i/s]14182/200000 [35:35/08:21:48, 6.64i/s]14196/200000 [35:37/08:21:49, 6.64i/s]14210/200000 [35:40/08:21:54, 6.64i/s]14258/200000 [35:44/08:21:17, 6.65i/s]14266/200000 [35:45/08:21:18, 6.65i/s]14276/200000 [35:46/08:21:11, 6.65i/s]14290/200000 [35:49/08:21:15, 6.65i/s]14335/200000 [35:55/08:21:12, 6.65i/s]14358/200000 [35:59/08:21:14, 6.65i/s]14363/200000 [36:00/08:21:19, 6.65i/s]14390/200000 [36:04/08:21:18, 6.65i/s]14398/200000 [36:05/08:21:19, 6.65i/s]14410/200000 [36:08/08:21:29, 6.65i/s]14426/200000 [36:11/08:21:43, 6.64i/s]14439/200000 [36:14/08:21:51, 6.64i/s]14447/200000 [36:15/08:21:49, 6.64i/s]14453/200000 [36:16/08:21:52, 6.64i/s]14469/200000 [36:18/08:21:48, 6.64i/s]14475/200000 [36:19/08:21:50, 6.64i/s]14498/200000 [36:23/08:21:54, 6.64i/s]14545/200000 [36:30/08:21:58, 6.64i/s]┫ 14560/200000 [36:33/08:22:02, 6.64i/s]14568/200000 [36:34/08:21:59, 6.64i/s]14574/200000 [36:35/08:22:03, 6.64i/s]14611/200000 [36:40/08:21:57, 6.64i/s]14618/200000 [36:41/08:21:57, 6.64i/s]14658/200000 [36:46/08:21:35, 6.65i/s]14717/200000 [36:53/08:21:09, 6.65i/s]14725/200000 [36:54/08:21:10, 6.65i/s]┫ 14733/200000 [36:57/08:21:29, 6.65i/s]14737/200000 [36:58/08:21:37, 6.65i/s]14767/200000 [37:04/08:22:03, 6.64i/s]14774/200000 [37:05/08:22:05, 6.64i/s]14788/200000 [37:11/08:22:53, 6.63i/s]14810/200000 [37:17/08:23:36, 6.62i/s]14837/200000 [37:22/08:23:48, 6.62i/s]14849/200000 [37:25/08:23:58, 6.61i/s]14855/200000 [37:26/08:24:03, 6.61i/s]14901/200000 [37:35/08:24:29, 6.61i/s]14921/200000 [37:39/08:24:40, 6.61i/s]14938/200000 [37:43/08:24:54, 6.60i/s]14945/200000 [37:44/08:24:57, 6.60i/s]14974/200000 [37:49/08:25:07, 6.60i/s]14979/200000 [37:50/08:25:14, 6.60i/s]14987/200000 [37:52/08:25:14, 6.60i/s]▎               ┫ 15002/200000 [37:54/08:25:19, 6.60i/s]15009/200000 [37:56/08:25:22, 6.60i/s]15016/200000 [37:57/08:25:25, 6.60i/s]15029/200000 [37:59/08:25:32, 6.59i/s]15048/200000 [38:03/08:25:40, 6.59i/s]┫ 15071/200000 [38:06/08:25:40, 6.59i/s]15078/200000 [38:08/08:25:43, 6.59i/s]15101/200000 [38:11/08:25:46, 6.59i/s]15110/200000 [38:13/08:25:45, 6.59i/s]15120/200000 [38:14/08:25:43, 6.59i/s]15139/200000 [38:17/08:25:39, 6.59i/s]15146/200000 [38:18/08:25:42, 6.59i/s]15154/200000 [38:19/08:25:43, 6.59i/s]15161/200000 [38:20/08:25:46, 6.59i/s]15213/200000 [38:29/08:25:54, 6.59i/s]15221/200000 [38:30/08:25:52, 6.59i/s]15250/200000 [38:33/08:25:40, 6.59i/s]15264/200000 [38:36/08:25:45, 6.59i/s]15305/200000 [38:41/08:25:24, 6.60i/s]15311/200000 [38:42/08:25:28, 6.59i/s]15340/200000 [38:45/08:25:16, 6.60i/s]15353/200000 [38:46/08:25:03, 6.60i/s]15361/200000 [38:47/08:25:03, 6.60i/s]15402/200000 [38:52/08:24:41, 6.60i/s]15436/200000 [38:54/08:24:01, 6.61i/s]15453/200000 [38:57/08:24:01, 6.61i/s]15462/200000 [38:58/08:23:57, 6.61i/s]15472/200000 [38:59/08:23:53, 6.62i/s]15478/200000 [39:00/08:23:58, 6.61i/s]15502/200000 [39:04/08:23:57, 6.61i/s]┫ 15508/200000 [39:05/08:23:58, 6.61i/s]15517/200000 [39:06/08:24:02, 6.61i/s]15524/200000 [39:08/08:24:05, 6.61i/s]15532/200000 [39:09/08:24:03, 6.61i/s]15570/200000 [39:12/08:23:35, 6.62i/s]15578/200000 [39:13/08:23:33, 6.62i/s]15603/200000 [39:17/08:23:29, 6.62i/s]15703/200000 [39:28/08:22:43, 6.63i/s]15715/200000 [39:29/08:22:34, 6.63i/s]15723/200000 [39:31/08:22:34, 6.63i/s]15733/200000 [39:32/08:22:27, 6.63i/s]15763/200000 [39:35/08:22:18, 6.64i/s]┫ 15798/200000 [39:40/08:22:11, 6.64i/s]15820/200000 [39:42/08:21:59, 6.64i/s]15849/200000 [39:46/08:21:48, 6.64i/s]15869/200000 [39:48/08:21:38, 6.64i/s]15884/200000 [39:50/08:21:37, 6.65i/s]15911/200000 [39:54/08:21:26, 6.65i/s]15931/200000 [39:56/08:21:20, 6.65i/s]15964/200000 [40:01/08:21:22, 6.65i/s]16002/200000 [41:30/08:38:35, 6.43i/s]16009/200000 [41:31/08:38:37, 6.43i/s]┫ 16023/200000 [41:33/08:38:38, 6.43i/s]16033/200000 [41:35/08:38:49, 6.42i/s]16048/200000 [41:38/08:38:51, 6.42i/s]16061/200000 [41:42/08:39:15, 6.42i/s]16103/200000 [41:48/08:39:11, 6.42i/s]16121/200000 [41:49/08:38:48, 6.42i/s]16151/200000 [41:55/08:39:09, 6.42i/s]┫ 16172/200000 [41:59/08:39:17, 6.42i/s]┫ 16181/200000 [42:01/08:39:16, 6.42i/s]16192/200000 [42:03/08:39:27, 6.42i/s]16199/200000 [42:05/08:39:30, 6.42i/s]16204/200000 [42:06/08:39:36, 6.42i/s]16213/200000 [42:07/08:39:31, 6.42i/s]16242/200000 [42:13/08:39:52, 6.41i/s]16261/200000 [42:17/08:40:04, 6.41i/s]16266/200000 [42:18/08:40:09, 6.41i/s]16290/200000 [42:25/08:40:43, 6.40i/s]16323/200000 [42:34/08:41:31, 6.39i/s]16330/200000 [42:35/08:41:30, 6.39i/s]16381/200000 [42:42/08:41:19, 6.39i/s]16428/200000 [42:51/08:41:40, 6.39i/s]16435/200000 [42:52/08:41:42, 6.39i/s]16456/200000 [42:57/08:42:04, 6.38i/s]16465/200000 [42:58/08:41:59, 6.39i/s]16495/200000 [43:03/08:42:03, 6.38i/s]16501/200000 [43:05/08:42:07, 6.38i/s]16507/200000 [43:06/08:42:12, 6.38i/s]┫ 16535/200000 [43:10/08:42:03, 6.39i/s]16556/200000 [43:12/08:41:52, 6.39i/s]16569/200000 [43:15/08:42:00, 6.39i/s]16579/200000 [43:17/08:42:11, 6.38i/s]16590/200000 [43:20/08:42:21, 6.38i/s]┫ 16603/200000 [43:22/08:42:28, 6.38i/s]16623/200000 [43:26/08:42:36, 6.38i/s]16637/200000 [43:29/08:42:41, 6.38i/s]16649/200000 [43:31/08:42:49, 6.38i/s]16656/200000 [43:33/08:42:51, 6.38i/s]16662/200000 [43:34/08:42:55, 6.37i/s]16674/200000 [43:36/08:43:04, 6.37i/s]16686/200000 [43:39/08:43:11, 6.37i/s]16692/200000 [43:40/08:43:17, 6.37i/s]16756/200000 [43:51/08:43:21, 6.37i/s]16769/200000 [43:53/08:43:27, 6.37i/s]16776/200000 [43:55/08:43:29, 6.37i/s]┫ 16783/200000 [43:56/08:43:28, 6.37i/s]┫ 16793/200000 [43:58/08:43:38, 6.37i/s]16804/200000 [43:59/08:43:33, 6.37i/s]16812/200000 [44:01/08:43:33, 6.37i/s]16825/200000 [44:03/08:43:40, 6.37i/s]16837/200000 [44:06/08:43:48, 6.36i/s]16845/200000 [44:07/08:43:49, 6.36i/s]16858/200000 [44:09/08:43:41, 6.37i/s]16867/200000 [44:11/08:43:55, 6.36i/s]16880/200000 [44:12/08:43:43, 6.36i/s]16895/200000 [44:15/08:43:44, 6.36i/s]16905/200000 [44:16/08:43:42, 6.37i/s]16917/200000 [44:18/08:43:49, 6.36i/s]16924/200000 [44:20/08:43:51, 6.36i/s]1.36e-01  11.22%┣█▊              ┫ 22450/200000 [55:07/08:11:03, 6.79i/s]16966/200000 [44:25/08:43:31, 6.37i/s]16985/200000 [44:27/08:43:22, 6.37i/s]16993/200000 [44:28/08:43:23, 6.37i/s]17001/200000 [44:29/08:43:23, 6.37i/s]17041/200000 [44:34/08:43:07, 6.37i/s]17072/200000 [44:38/08:42:51, 6.38i/s]17100/200000 [44:43/08:43:00, 6.37i/s]17120/200000 [44:47/08:43:08, 6.37i/s]17141/200000 [44:52/08:43:29, 6.37i/s]17223/200000 [45:05/08:43:35, 6.37i/s]17235/200000 [45:08/08:43:42, 6.36i/s]┫ 17244/200000 [45:09/08:43:41, 6.37i/s]17252/200000 [45:11/08:43:51, 6.36i/s]17263/200000 [45:14/08:44:01, 6.36i/s]17273/200000 [45:15/08:43:58, 6.36i/s]17288/200000 [45:18/08:44:01, 6.36i/s]17292/200000 [45:19/08:44:08, 6.36i/s]17316/200000 [45:23/08:44:10, 6.36i/s]17326/200000 [45:24/08:44:07, 6.36i/s]17330/200000 [45:26/08:44:14, 6.36i/s]17336/200000 [45:27/08:44:18, 6.36i/s]┫ 17353/200000 [45:30/08:44:27, 6.36i/s]17379/200000 [45:37/08:44:54, 6.35i/s]17397/200000 [45:41/08:45:07, 6.35i/s]17414/200000 [45:44/08:45:15, 6.35i/s]17419/200000 [45:45/08:45:20, 6.35i/s]17424/200000 [45:47/08:45:26, 6.34i/s]17451/200000 [45:52/08:45:37, 6.34i/s]17457/200000 [45:53/08:45:40, 6.34i/s]17465/200000 [45:54/08:45:41, 6.34i/s]17509/200000 [46:03/08:46:03, 6.34i/s]17526/200000 [46:07/08:46:17, 6.33i/s]17536/200000 [46:08/08:46:13, 6.33i/s]17555/200000 [46:12/08:46:23, 6.33i/s]17561/200000 [46:13/08:46:27, 6.33i/s]17576/200000 [46:16/08:46:29, 6.33i/s]17581/200000 [46:17/08:46:34, 6.33i/s]17655/200000 [46:24/08:45:40, 6.34i/s]17668/200000 [46:25/08:45:28, 6.34i/s]17676/200000 [46:26/08:45:27, 6.34i/s]17687/200000 [46:28/08:45:23, 6.34i/s]17693/200000 [46:29/08:45:27, 6.34i/s]17720/200000 [46:31/08:45:01, 6.35i/s]17735/200000 [46:32/08:44:46, 6.35i/s]17747/200000 [46:33/08:44:37, 6.35i/s]17773/200000 [46:35/08:44:17, 6.36i/s]17812/200000 [46:39/08:43:44, 6.36i/s]17824/200000 [46:40/08:43:35, 6.37i/s]17836/200000 [46:41/08:43:27, 6.37i/s]17880/200000 [46:44/08:42:48, 6.38i/s]17931/200000 [46:48/08:41:55, 6.39i/s]┫ 17938/200000 [46:49/08:41:56, 6.39i/s]17948/200000 [46:50/08:41:50, 6.39i/s]17981/200000 [46:54/08:41:35, 6.39i/s]18081/200000 [47:01/08:40:07, 6.41i/s]18095/200000 [47:02/08:39:54, 6.41i/s]┫ 18118/200000 [47:06/08:39:53, 6.41i/s]18155/200000 [47:10/08:39:39, 6.41i/s]18185/200000 [47:14/08:39:26, 6.42i/s]18241/200000 [47:18/08:38:38, 6.43i/s]18265/200000 [47:20/08:38:22, 6.43i/s]18278/200000 [47:21/08:38:11, 6.43i/s]18304/200000 [47:24/08:37:52, 6.44i/s]18350/200000 [47:30/08:37:40, 6.44i/s]18386/200000 [47:33/08:37:14, 6.44i/s]18486/200000 [47:40/08:35:37, 6.46i/s]               ┫ 18492/200000 [47:41/08:35:39, 6.46i/s]18522/200000 [47:43/08:35:12, 6.47i/s]18565/200000 [47:48/08:34:53, 6.47i/s]18575/200000 [47:49/08:34:47, 6.48i/s]18603/200000 [47:51/08:34:26, 6.48i/s]18655/200000 [47:54/08:33:33, 6.49i/s]18668/200000 [47:55/08:33:22, 6.49i/s]18677/200000 [47:56/08:33:21, 6.49i/s]18700/200000 [47:59/08:33:11, 6.50i/s]┫ 18711/200000 [48:00/08:33:04, 6.50i/s]18724/200000 [48:01/08:32:56, 6.50i/s]18753/200000 [48:05/08:32:47, 6.50i/s]18829/200000 [48:12/08:31:55, 6.51i/s]┫ 18915/200000 [48:17/08:30:32, 6.53i/s]18928/200000 [48:18/08:30:24, 6.53i/s]18972/200000 [48:22/08:29:48, 6.54i/s]┫ 18990/200000 [48:23/08:29:34, 6.54i/s]19062/200000 [48:31/08:29:02, 6.55i/s]19082/200000 [48:33/08:28:54, 6.55i/s]┣█▋               ┫ 19137/200000 [48:40/08:28:42, 6.55i/s]┫ 19143/200000 [48:41/08:28:43, 6.55i/s]19207/200000 [48:48/08:28:11, 6.56i/s]┫ 19224/200000 [48:50/08:27:58, 6.56i/s]19237/200000 [48:51/08:27:49, 6.56i/s]19246/200000 [48:52/08:27:46, 6.56i/s]19269/200000 [48:54/08:27:34, 6.57i/s]19320/200000 [48:59/08:27:01, 6.57i/s]19372/200000 [49:04/08:26:32, 6.58i/s]┫ 19377/200000 [49:05/08:26:36, 6.58i/s]19388/200000 [49:06/08:26:32, 6.58i/s]19403/200000 [49:08/08:26:32, 6.58i/s]19411/200000 [49:10/08:26:31, 6.58i/s]19419/200000 [49:11/08:26:32, 6.58i/s]19432/200000 [49:12/08:26:22, 6.58i/s]19551/200000 [49:24/08:25:18, 6.60i/s]19582/200000 [49:26/08:24:51, 6.60i/s]19623/200000 [49:31/08:24:38, 6.61i/s]19634/200000 [49:32/08:24:32, 6.61i/s]19703/200000 [49:39/08:24:00, 6.61i/s]19709/200000 [49:40/08:24:04, 6.61i/s]19731/200000 [49:44/08:24:09, 6.61i/s]19737/200000 [49:46/08:24:13, 6.61i/s]19763/200000 [49:49/08:24:13, 6.61i/s]               ┫ 19797/200000 [49:54/08:24:09, 6.61i/s]19809/200000 [49:57/08:24:17, 6.61i/s]19818/200000 [49:58/08:24:17, 6.61i/s]19832/200000 [50:01/08:24:22, 6.61i/s]19847/200000 [50:02/08:24:09, 6.61i/s]19867/200000 [50:03/08:23:49, 6.62i/s]19902/200000 [50:06/08:23:29, 6.62i/s]19917/200000 [50:09/08:23:31, 6.62i/s]19934/200000 [50:10/08:23:16, 6.62i/s]19945/200000 [50:11/08:23:12, 6.62i/s]19956/200000 [50:12/08:23:08, 6.63i/s]19976/200000 [50:15/08:23:04, 6.63i/s]20048/200000 [50:22/08:22:26, 6.63i/s]20065/200000 [50:23/08:22:12, 6.64i/s]20103/200000 [50:27/08:21:51, 6.64i/s]20155/200000 [50:30/08:21:04, 6.65i/s]20229/200000 [50:38/08:20:34, 6.66i/s]20315/200000 [50:43/08:19:17, 6.68i/s]20324/200000 [50:44/08:19:16, 6.68i/s]20394/200000 [50:49/08:18:17, 6.69i/s]20420/200000 [50:51/08:18:02, 6.69i/s]20435/200000 [50:52/08:17:50, 6.70i/s]20539/200000 [51:00/08:16:37, 6.71i/s]20550/200000 [51:01/08:16:33, 6.71i/s]20578/200000 [51:03/08:16:12, 6.72i/s]20594/200000 [51:07/08:16:25, 6.71i/s]20617/200000 [51:09/08:16:14, 6.72i/s]┫ 20680/200000 [51:16/08:15:52, 6.72i/s]20699/200000 [51:17/08:15:36, 6.73i/s]20706/200000 [51:19/08:15:38, 6.73i/s]20718/200000 [51:20/08:15:33, 6.73i/s]20727/200000 [51:21/08:15:33, 6.73i/s]20745/200000 [51:22/08:15:17, 6.73i/s]20792/200000 [51:27/08:14:54, 6.74i/s]20810/200000 [51:28/08:14:38, 6.74i/s]20825/200000 [51:31/08:14:41, 6.74i/s]20839/200000 [51:33/08:14:43, 6.74i/s]┫ 20858/200000 [51:35/08:14:40, 6.74i/s]20889/200000 [51:37/08:14:17, 6.74i/s]20896/200000 [51:39/08:14:18, 6.74i/s]20960/200000 [51:43/08:13:32, 6.75i/s]20986/200000 [51:46/08:13:18, 6.76i/s]20994/200000 [51:47/08:13:19, 6.76i/s]21007/200000 [51:48/08:13:10, 6.76i/s]21012/200000 [51:49/08:13:14, 6.76i/s]21066/200000 [51:54/08:12:43, 6.77i/s]21105/200000 [51:59/08:12:34, 6.77i/s]21111/200000 [52:00/08:12:38, 6.77i/s]21138/200000 [52:05/08:12:48, 6.76i/s]21145/200000 [52:06/08:12:51, 6.76i/s]┫ 21153/200000 [52:08/08:12:52, 6.76i/s]21167/200000 [52:10/08:12:56, 6.76i/s]21207/200000 [52:15/08:12:45, 6.76i/s]21263/200000 [52:19/08:12:08, 6.77i/s]21276/200000 [52:20/08:11:59, 6.78i/s]21313/200000 [52:24/08:11:40, 6.78i/s]┣█▋              ┫ 21350/200000 [52:28/08:11:31, 6.78i/s]┫ 21391/200000 [52:35/08:11:35, 6.78i/s]21405/200000 [52:37/08:11:40, 6.78i/s]21412/200000 [52:39/08:11:43, 6.78i/s]21457/200000 [52:44/08:11:28, 6.78i/s]21500/200000 [52:47/08:11:01, 6.79i/s]21517/200000 [52:48/08:10:47, 6.79i/s]21537/200000 [52:50/08:10:41, 6.79i/s]21542/200000 [52:52/08:10:46, 6.79i/s]21585/200000 [52:57/08:10:41, 6.79i/s]21594/200000 [52:59/08:10:40, 6.79i/s]21604/200000 [53:00/08:10:38, 6.79i/s]21623/200000 [53:02/08:10:33, 6.80i/s]21643/200000 [53:04/08:10:26, 6.80i/s]21653/200000 [53:06/08:10:25, 6.80i/s]21660/200000 [53:07/08:10:27, 6.80i/s]21690/200000 [53:10/08:10:19, 6.80i/s]21706/200000 [53:13/08:10:21, 6.80i/s]┫ 21727/200000 [53:15/08:10:14, 6.80i/s]21730/200000 [53:17/08:10:20, 6.80i/s]┫ 21767/200000 [53:23/08:10:26, 6.80i/s]21785/200000 [53:25/08:10:26, 6.80i/s]21828/200000 [53:30/08:10:08, 6.80i/s]21835/200000 [53:31/08:10:09, 6.80i/s]21854/200000 [53:33/08:10:04, 6.80i/s]21868/200000 [53:35/08:10:05, 6.80i/s]21898/200000 [53:39/08:09:58, 6.80i/s]21908/200000 [53:40/08:09:56, 6.80i/s]21942/200000 [53:46/08:10:07, 6.80i/s]21969/200000 [53:51/08:10:14, 6.80i/s]21980/200000 [53:52/08:10:11, 6.80i/s]┫ 22027/200000 [53:59/08:10:06, 6.80i/s]22100/200000 [54:13/08:10:39, 6.79i/s]22171/200000 [54:25/08:10:51, 6.79i/s]22195/200000 [54:29/08:10:55, 6.79i/s]22237/200000 [54:34/08:10:43, 6.79i/s]┫ 22245/200000 [54:35/08:10:44, 6.79i/s]22259/200000 [54:36/08:10:35, 6.79i/s]22266/200000 [54:37/08:10:37, 6.79i/s]22335/200000 [54:47/08:10:35, 6.79i/s]22359/200000 [54:51/08:10:39, 6.79i/s]22384/200000 [54:55/08:10:40, 6.79i/s]22408/200000 [55:00/08:10:50, 6.79i/s]22416/200000 [55:01/08:10:51, 6.79i/s]┫ 22427/200000 [55:03/08:10:59, 6.79i/s]22440/200000 [55:06/08:11:05, 6.79i/s]3.35e-06  13.02%┣█▋           ┫ 26043/200000 [01:05:53/08:25:59, 6.59i/s]22470/200000 [55:10/08:10:58, 6.79i/s]22485/200000 [55:12/08:11:01, 6.79i/s]22513/200000 [55:17/08:11:09, 6.79i/s]22532/200000 [55:20/08:11:08, 6.79i/s]┫ 22576/200000 [55:27/08:11:15, 6.79i/s]22618/200000 [55:33/08:11:14, 6.79i/s]22623/200000 [55:34/08:11:18, 6.78i/s]22631/200000 [55:36/08:11:19, 6.78i/s]22659/200000 [55:38/08:11:03, 6.79i/s]┫ 22678/200000 [55:41/08:11:01, 6.79i/s]22722/200000 [55:45/08:10:45, 6.79i/s]22729/200000 [55:48/08:11:00, 6.79i/s]22740/200000 [55:50/08:11:05, 6.79i/s]22744/200000 [55:51/08:11:10, 6.79i/s]22753/200000 [55:53/08:11:10, 6.79i/s]22764/200000 [55:56/08:11:21, 6.78i/s]22770/200000 [55:57/08:11:26, 6.78i/s]22779/200000 [55:59/08:11:36, 6.78i/s]22780/200000 [56:01/08:11:46, 6.78i/s]22788/200000 [56:03/08:11:59, 6.78i/s]22790/200000 [56:05/08:12:07, 6.77i/s]22804/200000 [56:08/08:12:23, 6.77i/s]┫ 22810/200000 [56:10/08:12:27, 6.77i/s]22833/200000 [56:14/08:12:36, 6.77i/s]22839/200000 [56:15/08:12:39, 6.77i/s]22850/200000 [56:18/08:12:47, 6.76i/s]22860/200000 [56:20/08:12:53, 6.76i/s]22875/200000 [56:23/08:12:56, 6.76i/s]22901/200000 [56:28/08:13:07, 6.76i/s]22908/200000 [56:29/08:13:09, 6.76i/s]22915/200000 [56:30/08:13:11, 6.76i/s]22935/200000 [56:34/08:13:19, 6.76i/s]23060/200000 [56:57/08:13:51, 6.75i/s]23067/200000 [56:58/08:13:53, 6.75i/s]23075/200000 [56:59/08:13:52, 6.75i/s]23115/200000 [57:05/08:13:52, 6.75i/s]23154/200000 [57:11/08:13:54, 6.75i/s]23190/200000 [57:17/08:14:02, 6.75i/s]23209/200000 [57:19/08:13:57, 6.75i/s]23222/200000 [57:22/08:14:02, 6.75i/s]23231/200000 [57:23/08:14:02, 6.75i/s]23280/200000 [57:29/08:13:54, 6.75i/s]23308/200000 [57:33/08:13:48, 6.75i/s]23312/200000 [57:34/08:13:53, 6.75i/s]23331/200000 [57:36/08:13:47, 6.75i/s]23354/200000 [57:38/08:13:38, 6.75i/s]23363/200000 [57:40/08:13:37, 6.75i/s]23389/200000 [57:42/08:13:24, 6.76i/s]23414/200000 [57:45/08:13:20, 6.76i/s]23436/200000 [57:46/08:13:02, 6.76i/s]23444/200000 [57:48/08:13:02, 6.76i/s]23453/200000 [57:49/08:13:02, 6.76i/s]23462/200000 [57:50/08:13:02, 6.76i/s]23474/200000 [57:51/08:12:55, 6.76i/s]23501/200000 [57:56/08:13:01, 6.76i/s]23517/200000 [57:59/08:13:05, 6.76i/s]23522/200000 [58:00/08:13:09, 6.76i/s]23529/200000 [58:01/08:13:11, 6.76i/s]23543/200000 [58:02/08:13:02, 6.76i/s]23558/200000 [58:03/08:12:52, 6.76i/s]23575/200000 [58:06/08:12:52, 6.76i/s]23580/200000 [58:07/08:12:56, 6.76i/s]23587/200000 [58:08/08:12:57, 6.76i/s]┫ 23598/200000 [58:09/08:12:52, 6.76i/s]23604/200000 [58:10/08:12:55, 6.76i/s]23611/200000 [58:12/08:12:56, 6.76i/s]23619/200000 [58:13/08:12:57, 6.76i/s]23637/200000 [58:15/08:12:54, 6.76i/s]23647/200000 [58:16/08:12:52, 6.76i/s]23739/200000 [58:26/08:12:16, 6.77i/s]23750/200000 [58:27/08:12:13, 6.77i/s]23761/200000 [58:28/08:12:11, 6.77i/s]23809/200000 [58:34/08:12:01, 6.77i/s]┫ 23815/200000 [58:35/08:12:03, 6.77i/s]23825/200000 [58:37/08:12:01, 6.77i/s]23834/200000 [58:38/08:12:01, 6.77i/s]23894/200000 [58:45/08:11:46, 6.78i/s]23910/200000 [58:48/08:11:47, 6.78i/s]23922/200000 [58:49/08:11:41, 6.78i/s]23947/200000 [58:52/08:11:41, 6.78i/s]23952/200000 [58:54/08:11:45, 6.78i/s]23967/200000 [58:56/08:11:45, 6.78i/s]23977/200000 [58:57/08:11:43, 6.78i/s]23986/200000 [58:58/08:11:43, 6.78i/s]23995/200000 [58:59/08:11:41, 6.78i/s]24001/200000 [59:01/08:11:43, 6.78i/s]┫ 24002/200000 [01:00:24/08:23:15, 6.62i/s]24009/200000 [01:00:25/08:23:17, 6.62i/s]24023/200000 [01:00:28/08:23:20, 6.62i/s]24028/200000 [01:00:29/08:23:25, 6.62i/s]24044/200000 [01:00:31/08:23:26, 6.62i/s]24053/200000 [01:00:34/08:23:36, 6.62i/s]24083/200000 [01:00:42/08:24:02, 6.61i/s]           ┫ 24095/200000 [01:00:43/08:23:57, 6.61i/s]24146/200000 [01:00:49/08:23:46, 6.62i/s]24165/200000 [01:00:53/08:23:54, 6.62i/s]24175/200000 [01:00:54/08:23:53, 6.62i/s]24193/200000 [01:00:58/08:24:01, 6.61i/s]24200/200000 [01:00:59/08:24:03, 6.61i/s]24215/200000 [01:01:02/08:24:06, 6.61i/s]24234/200000 [01:01:06/08:24:14, 6.61i/s]24239/200000 [01:01:07/08:24:16, 6.61i/s]24244/200000 [01:01:08/08:24:19, 6.61i/s]24291/200000 [01:01:19/08:24:53, 6.60i/s]█▌           ┫ 24298/200000 [01:01:20/08:24:54, 6.60i/s]24317/200000 [01:01:26/08:25:14, 6.60i/s]24320/200000 [01:01:27/08:25:20, 6.60i/s]24333/200000 [01:01:30/08:25:26, 6.60i/s]┫ 24339/200000 [01:01:31/08:25:28, 6.59i/s]24351/200000 [01:01:32/08:25:24, 6.60i/s]24364/200000 [01:01:33/08:25:18, 6.60i/s]24377/200000 [01:01:35/08:25:19, 6.60i/s]24391/200000 [01:01:38/08:25:20, 6.60i/s]┫ 24422/200000 [01:01:44/08:25:34, 6.59i/s]24428/200000 [01:01:45/08:25:37, 6.59i/s]24441/200000 [01:01:48/08:25:40, 6.59i/s]24455/200000 [01:01:51/08:25:53, 6.59i/s]24463/200000 [01:01:53/08:25:53, 6.59i/s]24473/200000 [01:01:54/08:25:51, 6.59i/s]┫ 24487/200000 [01:01:57/08:25:55, 6.59i/s]24496/200000 [01:01:58/08:25:54, 6.59i/s]┫ 24507/200000 [01:02:00/08:25:59, 6.59i/s]24559/200000 [01:02:06/08:25:46, 6.59i/s]24569/200000 [01:02:09/08:25:52, 6.59i/s]24579/200000 [01:02:11/08:26:00, 6.59i/s]24584/200000 [01:02:12/08:26:03, 6.59i/s]24590/200000 [01:02:14/08:26:06, 6.59i/s]24603/200000 [01:02:16/08:26:11, 6.59i/s]24616/200000 [01:02:19/08:26:16, 6.58i/s]24623/200000 [01:02:20/08:26:18, 6.58i/s]24630/200000 [01:02:21/08:26:20, 6.58i/s]           ┫ 24649/200000 [01:02:25/08:26:28, 6.58i/s]24694/200000 [01:02:35/08:26:48, 6.58i/s]24725/200000 [01:02:39/08:26:49, 6.58i/s]24732/200000 [01:02:41/08:26:51, 6.58i/s]24738/200000 [01:02:42/08:26:54, 6.58i/s]24788/200000 [01:02:51/08:27:02, 6.57i/s]24818/200000 [01:02:55/08:27:04, 6.57i/s]┫ 24845/200000 [01:03:01/08:27:13, 6.57i/s]24861/200000 [01:03:03/08:27:14, 6.57i/s]24867/200000 [01:03:04/08:27:18, 6.57i/s]24894/200000 [01:03:08/08:27:13, 6.57i/s]▌           ┫ 24936/200000 [01:03:14/08:27:10, 6.57i/s]24997/200000 [01:03:22/08:27:01, 6.57i/s]25014/200000 [01:03:25/08:27:02, 6.57i/s]25021/200000 [01:03:26/08:27:03, 6.57i/s]25036/200000 [01:03:27/08:26:53, 6.58i/s]25075/200000 [01:03:32/08:26:44, 6.58i/s]25090/200000 [01:03:34/08:26:46, 6.58i/s]25127/200000 [01:03:42/08:27:00, 6.57i/s]25132/200000 [01:03:43/08:27:03, 6.57i/s]25137/200000 [01:03:44/08:27:07, 6.57i/s]25160/200000 [01:03:48/08:27:08, 6.57i/s]25172/200000 [01:03:50/08:27:13, 6.57i/s]25208/200000 [01:03:55/08:27:08, 6.57i/s]25244/200000 [01:04:02/08:27:22, 6.57i/s]25265/200000 [01:04:07/08:27:37, 6.57i/s]┫ 25285/200000 [01:04:10/08:27:34, 6.57i/s]25289/200000 [01:04:11/08:27:39, 6.57i/s]25294/200000 [01:04:12/08:27:41, 6.57i/s]25303/200000 [01:04:14/08:27:40, 6.57i/s]25316/200000 [01:04:16/08:27:44, 6.57i/s]25348/200000 [01:04:22/08:27:55, 6.56i/s]┫ 25371/200000 [01:04:27/08:28:07, 6.56i/s]25374/200000 [01:04:29/08:28:13, 6.56i/s]25379/200000 [01:04:30/08:28:17, 6.56i/s]25402/200000 [01:04:35/08:28:31, 6.56i/s]25414/200000 [01:04:37/08:28:33, 6.55i/s]25419/200000 [01:04:39/08:28:37, 6.55i/s]25436/200000 [01:04:42/08:28:47, 6.55i/s]25451/200000 [01:04:45/08:28:50, 6.55i/s]25465/200000 [01:04:48/08:28:53, 6.55i/s]25472/200000 [01:04:49/08:28:54, 6.55i/s]25480/200000 [01:04:50/08:28:55, 6.55i/s]25487/200000 [01:04:51/08:28:57, 6.55i/s]25494/200000 [01:04:53/08:28:58, 6.55i/s]25499/200000 [01:04:54/08:29:02, 6.55i/s]25504/200000 [01:04:55/08:29:06, 6.55i/s]25541/200000 [01:05:03/08:29:19, 6.54i/s]25565/200000 [01:05:08/08:29:30, 6.54i/s]25644/200000 [01:05:17/08:29:09, 6.55i/s]25665/200000 [01:05:18/08:28:53, 6.55i/s]┫ 25673/200000 [01:05:19/08:28:53, 6.55i/s]25691/200000 [01:05:22/08:28:52, 6.55i/s]25716/200000 [01:05:24/08:28:41, 6.55i/s]25747/200000 [01:05:26/08:28:21, 6.56i/s]25773/200000 [01:05:29/08:28:08, 6.56i/s]25784/200000 [01:05:30/08:28:02, 6.56i/s]┫ 25813/200000 [01:05:32/08:27:46, 6.56i/s]25826/200000 [01:05:33/08:27:39, 6.57i/s]25840/200000 [01:05:34/08:27:30, 6.57i/s]25857/200000 [01:05:35/08:27:18, 6.57i/s]25871/200000 [01:05:36/08:27:11, 6.57i/s]25897/200000 [01:05:39/08:26:59, 6.57i/s]25917/200000 [01:05:40/08:26:43, 6.58i/s]25930/200000 [01:05:41/08:26:37, 6.58i/s]25936/200000 [01:05:42/08:26:39, 6.58i/s]25946/200000 [01:05:43/08:26:37, 6.58i/s]25981/200000 [01:05:47/08:26:26, 6.58i/s]25990/200000 [01:05:49/08:26:26, 6.58i/s]26007/200000 [01:05:50/08:26:13, 6.58i/s]26016/200000 [01:05:51/08:26:13, 6.58i/s]26030/200000 [01:05:52/08:26:04, 6.59i/s]8.16e-01  15.55%┣██           ┫ 31109/200000 [01:16:00/08:08:35, 6.82i/s]┫ 26086/200000 [01:05:55/08:25:26, 6.60i/s]26114/200000 [01:05:59/08:25:18, 6.60i/s]26119/200000 [01:06:00/08:25:22, 6.60i/s]26134/200000 [01:06:02/08:25:24, 6.60i/s]26177/200000 [01:06:07/08:25:09, 6.60i/s]26225/200000 [01:06:10/08:24:39, 6.61i/s]26239/200000 [01:06:11/08:24:31, 6.61i/s]26249/200000 [01:06:13/08:24:29, 6.61i/s]26288/200000 [01:06:16/08:24:09, 6.61i/s]26301/200000 [01:06:17/08:24:04, 6.61i/s]26311/200000 [01:06:18/08:24:01, 6.61i/s]26318/200000 [01:06:19/08:24:01, 6.61i/s]26327/200000 [01:06:21/08:24:00, 6.61i/s]26358/200000 [01:06:24/08:23:52, 6.62i/s]26372/200000 [01:06:25/08:23:44, 6.62i/s]26384/200000 [01:06:27/08:23:39, 6.62i/s]26422/200000 [01:06:29/08:23:12, 6.62i/s]26459/200000 [01:06:31/08:22:49, 6.63i/s]26475/200000 [01:06:32/08:22:38, 6.63i/s]26492/200000 [01:06:34/08:22:35, 6.63i/s]26522/200000 [01:06:36/08:22:17, 6.64i/s]26551/200000 [01:06:40/08:22:13, 6.64i/s]26565/200000 [01:06:41/08:22:05, 6.64i/s]26633/200000 [01:06:47/08:21:28, 6.65i/s]26655/200000 [01:06:48/08:21:11, 6.65i/s]26677/200000 [01:06:50/08:21:04, 6.65i/s]26700/200000 [01:06:53/08:20:58, 6.65i/s]26742/200000 [01:06:57/08:20:45, 6.66i/s]26768/200000 [01:07:00/08:20:33, 6.66i/s]26780/200000 [01:07:01/08:20:29, 6.66i/s]26804/200000 [01:07:03/08:20:19, 6.66i/s]26818/200000 [01:07:04/08:20:11, 6.66i/s]26844/200000 [01:07:06/08:19:59, 6.67i/s]26903/200000 [01:07:10/08:19:18, 6.68i/s]26915/200000 [01:07:11/08:19:12, 6.68i/s]26959/200000 [01:07:14/08:18:50, 6.68i/s]┫ 26972/200000 [01:07:15/08:18:43, 6.68i/s]┫ 27026/200000 [01:07:21/08:18:28, 6.69i/s]27064/200000 [01:07:25/08:18:13, 6.69i/s]27102/200000 [01:07:30/08:18:05, 6.69i/s]27114/200000 [01:07:31/08:18:02, 6.69i/s]27139/200000 [01:07:35/08:18:00, 6.69i/s]27207/200000 [01:07:42/08:17:38, 6.70i/s]27246/200000 [01:07:45/08:17:22, 6.70i/s]27255/200000 [01:07:47/08:17:21, 6.70i/s]27269/200000 [01:07:48/08:17:14, 6.70i/s]27282/200000 [01:07:49/08:17:09, 6.70i/s]27335/200000 [01:07:54/08:16:45, 6.71i/s]27350/200000 [01:07:55/08:16:39, 6.71i/s]27388/200000 [01:08:00/08:16:33, 6.71i/s]27411/200000 [01:08:03/08:16:33, 6.71i/s]27419/200000 [01:08:05/08:16:33, 6.71i/s]27440/200000 [01:08:07/08:16:27, 6.71i/s]27457/200000 [01:08:08/08:16:16, 6.72i/s]27471/200000 [01:08:09/08:16:08, 6.72i/s]27500/200000 [01:08:11/08:15:54, 6.72i/s]27519/200000 [01:08:14/08:15:54, 6.72i/s]27541/200000 [01:08:17/08:15:48, 6.72i/s]27589/200000 [01:08:20/08:15:22, 6.73i/s]┫ 27634/200000 [01:08:26/08:15:15, 6.73i/s]27646/200000 [01:08:27/08:15:12, 6.73i/s]27673/200000 [01:08:28/08:14:51, 6.74i/s]27722/200000 [01:08:37/08:15:04, 6.73i/s]27734/200000 [01:08:39/08:15:01, 6.73i/s]27749/200000 [01:08:41/08:15:03, 6.73i/s]27770/200000 [01:08:45/08:15:07, 6.73i/s]27789/200000 [01:08:47/08:15:04, 6.73i/s]27807/200000 [01:08:51/08:15:10, 6.73i/s]27812/200000 [01:08:52/08:15:14, 6.73i/s]27829/200000 [01:08:55/08:15:15, 6.73i/s]┫ 27838/200000 [01:08:56/08:15:14, 6.73i/s]27888/200000 [01:08:59/08:14:43, 6.74i/s]27943/200000 [01:09:05/08:14:29, 6.74i/s]27963/200000 [01:09:08/08:14:26, 6.74i/s]27972/200000 [01:09:09/08:14:24, 6.74i/s]27978/200000 [01:09:10/08:14:26, 6.74i/s]28004/200000 [01:09:13/08:14:23, 6.74i/s]28047/200000 [01:09:17/08:14:00, 6.75i/s]28061/200000 [01:09:18/08:13:52, 6.75i/s]28131/200000 [01:09:23/08:13:18, 6.76i/s]28162/200000 [01:09:25/08:13:00, 6.76i/s]28238/200000 [01:09:34/08:12:41, 6.77i/s]28263/200000 [01:09:35/08:12:23, 6.77i/s]28362/200000 [01:09:41/08:11:25, 6.78i/s]28399/200000 [01:09:44/08:11:08, 6.79i/s]28488/200000 [01:09:51/08:10:22, 6.80i/s]28530/200000 [01:09:55/08:10:04, 6.80i/s]28543/200000 [01:09:56/08:09:58, 6.80i/s]28667/200000 [01:10:11/08:09:38, 6.81i/s]28683/200000 [01:10:12/08:09:29, 6.81i/s]28706/200000 [01:10:14/08:09:22, 6.81i/s]┫ 28729/200000 [01:10:17/08:09:16, 6.81i/s]28776/200000 [01:10:20/08:08:52, 6.82i/s]28800/200000 [01:10:22/08:08:42, 6.82i/s]28819/200000 [01:10:25/08:08:38, 6.82i/s]28835/200000 [01:10:27/08:08:39, 6.82i/s]28839/200000 [01:10:28/08:08:42, 6.82i/s]28856/200000 [01:10:30/08:08:39, 6.82i/s]28886/200000 [01:10:32/08:08:25, 6.82i/s]29052/200000 [01:10:47/08:07:19, 6.84i/s]29060/200000 [01:10:48/08:07:19, 6.84i/s]29132/200000 [01:10:59/08:07:22, 6.84i/s]29153/200000 [01:11:03/08:07:27, 6.84i/s]29198/200000 [01:11:09/08:07:23, 6.84i/s]┫ 29214/200000 [01:11:10/08:07:14, 6.84i/s]29238/200000 [01:11:13/08:07:07, 6.84i/s]29301/200000 [01:11:17/08:06:36, 6.85i/s]29326/200000 [01:11:21/08:06:35, 6.85i/s]29354/200000 [01:11:23/08:06:23, 6.85i/s]29396/200000 [01:11:30/08:06:28, 6.85i/s]29405/200000 [01:11:31/08:06:29, 6.85i/s]29420/200000 [01:11:34/08:06:31, 6.85i/s]29427/200000 [01:11:35/08:06:31, 6.85i/s]29451/200000 [01:11:37/08:06:22, 6.85i/s]29463/200000 [01:11:38/08:06:18, 6.85i/s]29507/200000 [01:11:42/08:05:57, 6.86i/s]29611/200000 [01:11:55/08:05:42, 6.86i/s]29640/200000 [01:11:58/08:05:37, 6.86i/s]29650/200000 [01:11:59/08:05:34, 6.86i/s]29662/200000 [01:12:02/08:05:39, 6.86i/s]29675/200000 [01:12:03/08:05:33, 6.87i/s]29700/200000 [01:12:06/08:05:34, 6.86i/s]29711/200000 [01:12:08/08:05:32, 6.87i/s]29750/200000 [01:12:15/08:05:43, 6.86i/s]29761/200000 [01:12:16/08:05:40, 6.86i/s]29776/200000 [01:12:19/08:05:41, 6.86i/s]29823/200000 [01:12:24/08:05:32, 6.87i/s]29844/200000 [01:12:26/08:05:27, 6.87i/s]29856/200000 [01:12:29/08:05:31, 6.87i/s]29881/200000 [01:12:32/08:05:30, 6.87i/s]┫ 29911/200000 [01:12:36/08:05:23, 6.87i/s]29916/200000 [01:12:37/08:05:26, 6.87i/s]29936/200000 [01:12:41/08:05:33, 6.87i/s]29952/200000 [01:12:44/08:05:40, 6.86i/s]29961/200000 [01:12:45/08:05:40, 6.86i/s]┫ 29969/200000 [01:12:46/08:05:39, 6.86i/s]30010/200000 [01:12:51/08:05:33, 6.87i/s]┫ 30027/200000 [01:12:54/08:05:34, 6.86i/s]30055/200000 [01:12:59/08:05:42, 6.86i/s]30071/200000 [01:13:02/08:05:44, 6.86i/s]30091/200000 [01:13:06/08:05:51, 6.86i/s]30121/200000 [01:13:11/08:05:54, 6.86i/s]30155/200000 [01:13:17/08:06:02, 6.86i/s]30171/200000 [01:13:20/08:06:09, 6.86i/s]30177/200000 [01:13:22/08:06:12, 6.86i/s]30201/200000 [01:13:26/08:06:15, 6.86i/s]30209/200000 [01:13:27/08:06:16, 6.86i/s]┫ 30290/200000 [01:13:36/08:05:59, 6.86i/s]30304/200000 [01:13:38/08:06:00, 6.86i/s]30324/200000 [01:13:41/08:05:56, 6.86i/s]30332/200000 [01:13:42/08:05:57, 6.86i/s]30342/200000 [01:13:44/08:06:04, 6.86i/s]30350/200000 [01:13:45/08:06:03, 6.86i/s]30367/200000 [01:13:48/08:06:03, 6.86i/s]30432/200000 [01:14:00/08:06:20, 6.85i/s]30453/200000 [01:14:03/08:06:22, 6.85i/s]30483/200000 [01:14:07/08:06:20, 6.85i/s]30497/200000 [01:14:10/08:06:24, 6.85i/s]30505/200000 [01:14:11/08:06:24, 6.85i/s]30512/200000 [01:14:13/08:06:26, 6.85i/s]┫ 30530/200000 [01:14:15/08:06:26, 6.85i/s]30537/200000 [01:14:17/08:06:28, 6.85i/s]30545/200000 [01:14:18/08:06:28, 6.85i/s]┫ 30560/200000 [01:14:20/08:06:30, 6.85i/s]┫ 30568/200000 [01:14:21/08:06:29, 6.85i/s]┫ 30584/200000 [01:14:24/08:06:30, 6.85i/s]30621/200000 [01:14:29/08:06:32, 6.85i/s]30635/200000 [01:14:32/08:06:35, 6.85i/s]30676/200000 [01:14:35/08:06:16, 6.85i/s]30681/200000 [01:14:36/08:06:19, 6.85i/s]30727/200000 [01:14:43/08:06:17, 6.85i/s]30733/200000 [01:14:44/08:06:20, 6.85i/s]30780/200000 [01:14:56/08:06:53, 6.85i/s]30784/200000 [01:14:57/08:06:58, 6.85i/s]30788/200000 [01:14:59/08:07:03, 6.84i/s]30791/200000 [01:15:00/08:07:08, 6.84i/s]30801/200000 [01:15:03/08:07:16, 6.84i/s]30805/200000 [01:15:04/08:07:20, 6.84i/s]30835/200000 [01:15:10/08:07:32, 6.84i/s]30842/200000 [01:15:11/08:07:33, 6.84i/s]30848/200000 [01:15:12/08:07:36, 6.84i/s]30879/200000 [01:15:18/08:07:45, 6.83i/s]30886/200000 [01:15:20/08:07:47, 6.83i/s]30892/200000 [01:15:21/08:07:49, 6.83i/s]30912/200000 [01:15:25/08:07:56, 6.83i/s]30919/200000 [01:15:26/08:07:58, 6.83i/s]30945/200000 [01:15:31/08:08:06, 6.83i/s]30952/200000 [01:15:33/08:08:07, 6.83i/s]30967/200000 [01:15:35/08:08:08, 6.83i/s]30972/200000 [01:15:36/08:08:11, 6.83i/s]30978/200000 [01:15:37/08:08:14, 6.83i/s]30994/200000 [01:15:40/08:08:15, 6.83i/s]┫ 31041/200000 [01:15:49/08:08:28, 6.82i/s]31055/200000 [01:15:51/08:08:31, 6.82i/s]31063/200000 [01:15:52/08:08:30, 6.82i/s]31069/200000 [01:15:53/08:08:31, 6.82i/s]┫ 31085/200000 [01:15:56/08:08:33, 6.82i/s]31101/200000 [01:15:59/08:08:34, 6.82i/s]3.37e-02  17.74%┣██▎          ┫ 35479/200000 [01:27:04/08:10:47, 6.79i/s]31125/200000 [01:16:02/08:08:34, 6.82i/s]31134/200000 [01:16:03/08:08:32, 6.82i/s]┫ 31176/200000 [01:16:10/08:08:39, 6.82i/s]31184/200000 [01:16:12/08:08:40, 6.82i/s]31238/200000 [01:16:19/08:08:36, 6.82i/s]31273/200000 [01:16:24/08:08:35, 6.82i/s]31311/200000 [01:16:29/08:08:30, 6.82i/s]31317/200000 [01:16:30/08:08:32, 6.82i/s]31330/200000 [01:16:31/08:08:26, 6.82i/s]┫ 31396/200000 [01:16:38/08:08:10, 6.83i/s]31404/200000 [01:16:39/08:08:10, 6.83i/s]31416/200000 [01:16:40/08:08:06, 6.83i/s]31436/200000 [01:16:41/08:07:54, 6.83i/s]31462/200000 [01:16:45/08:07:54, 6.83i/s]31495/200000 [01:16:50/08:07:52, 6.83i/s]31501/200000 [01:16:51/08:07:54, 6.83i/s]31543/200000 [01:16:57/08:07:55, 6.83i/s]31557/200000 [01:16:58/08:07:48, 6.83i/s]31567/200000 [01:16:59/08:07:47, 6.83i/s]31575/200000 [01:17:01/08:07:48, 6.83i/s]31604/200000 [01:17:05/08:07:49, 6.83i/s]31636/200000 [01:17:10/08:07:49, 6.83i/s]31646/200000 [01:17:11/08:07:48, 6.83i/s]┫ 31697/200000 [01:17:15/08:07:28, 6.84i/s]31717/200000 [01:17:18/08:07:23, 6.84i/s]31724/200000 [01:17:19/08:07:24, 6.84i/s]31746/200000 [01:17:21/08:07:18, 6.84i/s]31755/200000 [01:17:22/08:07:18, 6.84i/s]31785/200000 [01:17:26/08:07:13, 6.84i/s]31792/200000 [01:17:27/08:07:14, 6.84i/s]31811/200000 [01:17:29/08:07:11, 6.84i/s]31818/200000 [01:17:31/08:07:12, 6.84i/s]31835/200000 [01:17:33/08:07:12, 6.84i/s]31869/200000 [01:17:37/08:07:03, 6.84i/s]31928/200000 [01:17:44/08:06:58, 6.85i/s]31938/200000 [01:17:46/08:06:57, 6.85i/s]31947/200000 [01:17:47/08:06:58, 6.85i/s]15.98%┣██           ┫ 31952/200000 [01:17:48/08:07:01, 6.84i/s]31960/200000 [01:17:49/08:07:00, 6.84i/s]31986/200000 [01:17:53/08:07:00, 6.84i/s]32023/200000 [01:19:22/08:15:42, 6.72i/s]32028/200000 [01:19:23/08:15:45, 6.72i/s]32036/200000 [01:19:25/08:15:46, 6.72i/s]32057/200000 [01:19:30/08:15:58, 6.72i/s]32062/200000 [01:19:31/08:16:02, 6.72i/s]32072/200000 [01:19:34/08:16:09, 6.72i/s]32083/200000 [01:19:36/08:16:15, 6.72i/s]32121/200000 [01:19:39/08:15:54, 6.72i/s]32134/200000 [01:19:41/08:15:58, 6.72i/s]32140/200000 [01:19:43/08:16:01, 6.72i/s]32152/200000 [01:19:45/08:16:05, 6.72i/s]32159/200000 [01:19:46/08:16:07, 6.72i/s]┫ 32193/200000 [01:19:53/08:16:16, 6.72i/s]32199/200000 [01:19:54/08:16:16, 6.72i/s]32218/200000 [01:19:58/08:16:22, 6.72i/s]32225/200000 [01:19:59/08:16:24, 6.72i/s]32232/200000 [01:20:00/08:16:25, 6.71i/s]32244/200000 [01:20:03/08:16:30, 6.71i/s]┫ 32251/200000 [01:20:04/08:16:31, 6.71i/s]32271/200000 [01:20:09/08:16:42, 6.71i/s]32286/200000 [01:20:13/08:16:52, 6.71i/s]32298/200000 [01:20:15/08:16:57, 6.71i/s]32303/200000 [01:20:16/08:17:01, 6.71i/s]32308/200000 [01:20:18/08:17:04, 6.71i/s]┫ 32320/200000 [01:20:22/08:17:17, 6.70i/s]32339/200000 [01:20:25/08:17:23, 6.70i/s]32369/200000 [01:20:29/08:17:17, 6.70i/s]32377/200000 [01:20:30/08:17:17, 6.70i/s]32411/200000 [01:20:36/08:17:23, 6.70i/s]┫ 32435/200000 [01:20:41/08:17:33, 6.70i/s]┫ 32445/200000 [01:20:44/08:17:37, 6.70i/s]32449/200000 [01:20:45/08:17:41, 6.70i/s]32455/200000 [01:20:46/08:17:43, 6.70i/s]32463/200000 [01:20:47/08:17:44, 6.70i/s]32473/200000 [01:20:49/08:17:43, 6.70i/s]32479/200000 [01:20:50/08:17:43, 6.70i/s]32485/200000 [01:20:51/08:17:45, 6.70i/s]32494/200000 [01:20:52/08:17:45, 6.70i/s]32565/200000 [01:21:02/08:17:42, 6.70i/s]32579/200000 [01:21:06/08:17:50, 6.70i/s]32584/200000 [01:21:07/08:17:53, 6.69i/s]32590/200000 [01:21:08/08:17:56, 6.69i/s]32610/200000 [01:21:12/08:18:01, 6.69i/s]32616/200000 [01:21:13/08:18:03, 6.69i/s]32623/200000 [01:21:15/08:18:05, 6.69i/s]32637/200000 [01:21:17/08:18:08, 6.69i/s]32662/200000 [01:21:22/08:18:16, 6.69i/s]32672/200000 [01:21:24/08:18:20, 6.69i/s]32701/200000 [01:21:31/08:18:31, 6.69i/s]32715/200000 [01:21:33/08:18:33, 6.69i/s]32725/200000 [01:21:34/08:18:30, 6.69i/s]32845/200000 [01:21:55/08:18:49, 6.68i/s]32880/200000 [01:22:00/08:18:50, 6.68i/s]32888/200000 [01:22:02/08:18:50, 6.68i/s]32898/200000 [01:22:03/08:18:47, 6.68i/s]32925/200000 [01:22:08/08:18:53, 6.68i/s]32938/200000 [01:22:09/08:18:47, 6.68i/s]┫ 32950/200000 [01:22:10/08:18:44, 6.68i/s]32985/200000 [01:22:15/08:18:42, 6.68i/s]32993/200000 [01:22:16/08:18:42, 6.68i/s]33001/200000 [01:22:17/08:18:43, 6.68i/s]33009/200000 [01:22:19/08:18:43, 6.68i/s]33017/200000 [01:22:20/08:18:44, 6.68i/s]33088/200000 [01:22:28/08:18:29, 6.69i/s]33092/200000 [01:22:29/08:18:33, 6.69i/s]33124/200000 [01:22:36/08:18:42, 6.68i/s]33135/200000 [01:22:38/08:18:48, 6.68i/s]33153/200000 [01:22:42/08:18:55, 6.68i/s]33192/200000 [01:22:48/08:18:57, 6.68i/s]33230/200000 [01:22:55/08:19:01, 6.68i/s]33237/200000 [01:22:56/08:19:03, 6.68i/s]33245/200000 [01:22:57/08:19:03, 6.68i/s]33294/200000 [01:23:07/08:19:19, 6.68i/s]33309/200000 [01:23:09/08:19:19, 6.68i/s]33314/200000 [01:23:11/08:19:22, 6.68i/s]33321/200000 [01:23:12/08:19:23, 6.67i/s]33328/200000 [01:23:13/08:19:24, 6.67i/s]33333/200000 [01:23:15/08:19:27, 6.67i/s]┫ 33355/200000 [01:23:19/08:19:35, 6.67i/s]33365/200000 [01:23:22/08:19:41, 6.67i/s]33381/200000 [01:23:25/08:19:50, 6.67i/s]33385/200000 [01:23:27/08:19:54, 6.67i/s]33392/200000 [01:23:28/08:19:55, 6.67i/s]33416/200000 [01:23:33/08:20:05, 6.67i/s]33426/200000 [01:23:36/08:20:11, 6.66i/s]33472/200000 [01:23:44/08:20:21, 6.66i/s]33486/200000 [01:23:47/08:20:24, 6.66i/s]33499/200000 [01:23:50/08:20:28, 6.66i/s]33514/200000 [01:23:53/08:20:38, 6.66i/s]33535/200000 [01:23:57/08:20:42, 6.66i/s]33542/200000 [01:23:58/08:20:43, 6.66i/s]┫ 33559/200000 [01:24:02/08:20:48, 6.66i/s]33565/200000 [01:24:03/08:20:51, 6.66i/s]33573/200000 [01:24:05/08:20:51, 6.66i/s]33585/200000 [01:24:07/08:20:56, 6.65i/s]33592/200000 [01:24:08/08:20:57, 6.65i/s]33640/200000 [01:24:12/08:20:34, 6.66i/s]33653/200000 [01:24:13/08:20:29, 6.66i/s]33691/200000 [01:24:18/08:20:23, 6.66i/s]33738/200000 [01:24:21/08:20:01, 6.67i/s]33756/200000 [01:24:22/08:19:54, 6.67i/s]33769/200000 [01:24:23/08:19:48, 6.67i/s]33809/200000 [01:24:27/08:19:32, 6.67i/s]33830/200000 [01:24:29/08:19:27, 6.67i/s]33893/200000 [01:24:33/08:18:56, 6.68i/s]33918/200000 [01:24:35/08:18:47, 6.68i/s]33938/200000 [01:24:38/08:18:43, 6.68i/s]34007/200000 [01:24:44/08:18:22, 6.69i/s]34015/200000 [01:24:46/08:18:23, 6.69i/s]34080/200000 [01:24:50/08:17:50, 6.70i/s]34104/200000 [01:24:52/08:17:42, 6.70i/s]34113/200000 [01:24:53/08:17:42, 6.70i/s]34135/200000 [01:24:57/08:17:46, 6.70i/s]34148/200000 [01:24:58/08:17:40, 6.70i/s]34208/200000 [01:25:04/08:17:22, 6.70i/s]┫ 34237/200000 [01:25:06/08:17:10, 6.70i/s]34248/200000 [01:25:07/08:17:07, 6.71i/s]34304/200000 [01:25:12/08:16:44, 6.71i/s]34328/200000 [01:25:15/08:16:43, 6.71i/s]▏          ┫ 34425/200000 [01:25:23/08:16:05, 6.72i/s]34482/200000 [01:25:28/08:15:43, 6.72i/s]34534/200000 [01:25:33/08:15:25, 6.73i/s]34561/200000 [01:25:36/08:15:21, 6.73i/s]34581/200000 [01:25:38/08:15:18, 6.73i/s]34662/200000 [01:25:43/08:14:36, 6.74i/s]34692/200000 [01:25:47/08:14:32, 6.74i/s]34734/200000 [01:25:51/08:14:21, 6.74i/s]34742/200000 [01:25:53/08:14:22, 6.74i/s]34753/200000 [01:25:54/08:14:20, 6.74i/s]34768/200000 [01:25:55/08:14:13, 6.74i/s]34791/200000 [01:25:57/08:14:06, 6.75i/s]34804/200000 [01:25:58/08:14:01, 6.75i/s]34829/200000 [01:26:00/08:13:53, 6.75i/s]34849/200000 [01:26:01/08:13:42, 6.75i/s]34902/200000 [01:26:05/08:13:15, 6.76i/s]34914/200000 [01:26:06/08:13:12, 6.76i/s]34928/200000 [01:26:07/08:13:07, 6.76i/s]34944/200000 [01:26:08/08:13:00, 6.76i/s]34990/200000 [01:26:11/08:12:40, 6.77i/s]34997/200000 [01:26:13/08:12:40, 6.77i/s]35005/200000 [01:26:14/08:12:41, 6.77i/s]35018/200000 [01:26:15/08:12:38, 6.77i/s]35048/200000 [01:26:18/08:12:26, 6.77i/s]35079/200000 [01:26:21/08:12:21, 6.77i/s]35090/200000 [01:26:24/08:12:25, 6.77i/s]35113/200000 [01:26:26/08:12:19, 6.77i/s]35127/200000 [01:26:28/08:12:20, 6.77i/s]35157/200000 [01:26:32/08:12:14, 6.77i/s]35194/200000 [01:26:36/08:12:06, 6.77i/s]┫ 35231/200000 [01:26:39/08:11:53, 6.78i/s]35278/200000 [01:26:43/08:11:40, 6.78i/s]35291/200000 [01:26:44/08:11:35, 6.78i/s]35305/200000 [01:26:45/08:11:29, 6.78i/s]35344/200000 [01:26:49/08:11:15, 6.79i/s]35397/200000 [01:26:56/08:11:11, 6.79i/s]35417/200000 [01:26:58/08:11:09, 6.79i/s]35463/200000 [01:27:03/08:10:55, 6.79i/s]1.04e+00  19.79%┣██▌          ┫ 39575/200000 [01:35:54/08:04:37, 6.88i/s]35515/200000 [01:27:07/08:10:36, 6.79i/s]┫ 35565/200000 [01:27:13/08:10:27, 6.80i/s]35606/200000 [01:27:17/08:10:14, 6.80i/s]35616/200000 [01:27:18/08:10:12, 6.80i/s]35673/200000 [01:27:22/08:09:51, 6.80i/s]35680/200000 [01:27:23/08:09:51, 6.80i/s]35684/200000 [01:27:24/08:09:54, 6.80i/s]35703/200000 [01:27:27/08:09:53, 6.80i/s]35709/200000 [01:27:28/08:09:56, 6.80i/s]35714/200000 [01:27:30/08:09:59, 6.80i/s]35737/200000 [01:27:34/08:10:01, 6.80i/s]┫ 35753/200000 [01:27:36/08:10:03, 6.80i/s]35763/200000 [01:27:38/08:10:02, 6.80i/s]35777/200000 [01:27:40/08:10:03, 6.80i/s]35818/200000 [01:27:46/08:10:05, 6.80i/s]35824/200000 [01:27:48/08:10:08, 6.80i/s]35832/200000 [01:27:49/08:10:09, 6.80i/s]35847/200000 [01:27:50/08:10:02, 6.80i/s]35867/200000 [01:27:51/08:09:51, 6.80i/s]35892/200000 [01:27:53/08:09:44, 6.81i/s]┫ 35902/200000 [01:27:54/08:09:41, 6.81i/s]35917/200000 [01:27:57/08:09:43, 6.81i/s]35956/200000 [01:28:00/08:09:31, 6.81i/s]35966/200000 [01:28:02/08:09:30, 6.81i/s]┫ 35976/200000 [01:28:03/08:09:29, 6.81i/s]35982/200000 [01:28:04/08:09:31, 6.81i/s]35990/200000 [01:28:05/08:09:30, 6.81i/s]36010/200000 [01:28:08/08:09:28, 6.81i/s]36088/200000 [01:28:14/08:08:58, 6.82i/s]36103/200000 [01:28:15/08:08:51, 6.82i/s]36138/200000 [01:28:17/08:08:35, 6.82i/s]36155/200000 [01:28:18/08:08:26, 6.82i/s]36171/200000 [01:28:20/08:08:26, 6.82i/s]36186/200000 [01:28:21/08:08:19, 6.83i/s]36229/200000 [01:28:26/08:08:11, 6.83i/s]36245/200000 [01:28:27/08:08:04, 6.83i/s]36263/200000 [01:28:28/08:07:55, 6.83i/s]36282/200000 [01:28:29/08:07:45, 6.83i/s]36298/200000 [01:28:30/08:07:38, 6.84i/s]36314/200000 [01:28:31/08:07:31, 6.84i/s]36362/200000 [01:28:34/08:07:10, 6.84i/s]36415/200000 [01:28:39/08:06:52, 6.85i/s]36470/200000 [01:28:42/08:06:26, 6.85i/s]36583/200000 [01:28:53/08:05:55, 6.86i/s]36589/200000 [01:28:54/08:05:57, 6.86i/s]36607/200000 [01:28:57/08:05:56, 6.86i/s]36640/200000 [01:29:00/08:05:48, 6.86i/s]36654/200000 [01:29:02/08:05:50, 6.86i/s]36717/200000 [01:29:08/08:05:33, 6.87i/s]36724/200000 [01:29:10/08:05:34, 6.86i/s]36739/200000 [01:29:11/08:05:28, 6.87i/s]36782/200000 [01:29:14/08:05:12, 6.87i/s]36788/200000 [01:29:15/08:05:14, 6.87i/s]36806/200000 [01:29:16/08:05:05, 6.87i/s]36822/200000 [01:29:18/08:05:04, 6.87i/s]36837/200000 [01:29:21/08:05:05, 6.87i/s]36841/200000 [01:29:22/08:05:08, 6.87i/s]36866/200000 [01:29:24/08:05:00, 6.87i/s]36931/200000 [01:29:28/08:04:33, 6.88i/s]36946/200000 [01:29:29/08:04:27, 6.88i/s]37005/200000 [01:29:36/08:04:17, 6.88i/s]37039/200000 [01:29:40/08:04:09, 6.88i/s]37063/200000 [01:29:42/08:04:03, 6.89i/s]37134/200000 [01:29:53/08:04:06, 6.89i/s]37141/200000 [01:29:54/08:04:07, 6.89i/s]37155/200000 [01:29:57/08:04:10, 6.88i/s]37171/200000 [01:29:59/08:04:12, 6.88i/s]37193/200000 [01:30:02/08:04:07, 6.89i/s]37203/200000 [01:30:03/08:04:06, 6.89i/s]37219/200000 [01:30:04/08:03:59, 6.89i/s]37230/200000 [01:30:05/08:03:56, 6.89i/s]37259/200000 [01:30:07/08:03:46, 6.89i/s]37291/200000 [01:30:09/08:03:32, 6.89i/s]37319/200000 [01:30:13/08:03:27, 6.89i/s]37361/200000 [01:30:17/08:03:19, 6.90i/s]37369/200000 [01:30:18/08:03:20, 6.90i/s]37393/200000 [01:30:22/08:03:22, 6.90i/s]37402/200000 [01:30:24/08:03:22, 6.90i/s]37407/200000 [01:30:25/08:03:23, 6.90i/s]37457/200000 [01:30:30/08:03:15, 6.90i/s]37467/200000 [01:30:32/08:03:14, 6.90i/s]┫ 37483/200000 [01:30:33/08:03:08, 6.90i/s]37496/200000 [01:30:34/08:03:03, 6.90i/s]37517/200000 [01:30:35/08:02:53, 6.90i/s]37537/200000 [01:30:37/08:02:50, 6.90i/s]37557/200000 [01:30:41/08:02:55, 6.90i/s]37576/200000 [01:30:43/08:02:52, 6.90i/s]37613/200000 [01:30:48/08:02:48, 6.90i/s]┫ 37671/200000 [01:30:55/08:02:42, 6.91i/s]37699/200000 [01:30:59/08:02:41, 6.91i/s]          ┫ 37740/200000 [01:31:06/08:02:47, 6.90i/s]37769/200000 [01:31:10/08:02:44, 6.91i/s]37824/200000 [01:31:16/08:02:35, 6.91i/s]37833/200000 [01:31:17/08:02:34, 6.91i/s]37908/200000 [01:31:27/08:02:28, 6.91i/s]37916/200000 [01:31:28/08:02:29, 6.91i/s]37931/200000 [01:31:31/08:02:32, 6.91i/s]37936/200000 [01:31:32/08:02:34, 6.91i/s]37954/200000 [01:31:36/08:02:41, 6.91i/s]37963/200000 [01:31:37/08:02:40, 6.91i/s]37981/200000 [01:31:39/08:02:37, 6.91i/s]37991/200000 [01:31:40/08:02:36, 6.91i/s]38001/200000 [01:31:42/08:02:35, 6.91i/s]38010/200000 [01:31:43/08:02:35, 6.91i/s]38033/200000 [01:31:47/08:02:38, 6.91i/s]38055/200000 [01:31:51/08:02:42, 6.91i/s]38061/200000 [01:31:52/08:02:44, 6.91i/s]38078/200000 [01:31:55/08:02:45, 6.90i/s]38091/200000 [01:31:57/08:02:49, 6.90i/s]38096/200000 [01:31:59/08:02:52, 6.90i/s]38112/200000 [01:32:01/08:02:52, 6.90i/s]38121/200000 [01:32:02/08:02:52, 6.90i/s]38149/200000 [01:32:07/08:02:56, 6.90i/s]38161/200000 [01:32:09/08:03:00, 6.90i/s]38171/200000 [01:32:12/08:03:04, 6.90i/s]38186/200000 [01:32:14/08:03:06, 6.90i/s]38209/200000 [01:32:18/08:03:09, 6.90i/s]┫ 38216/200000 [01:32:19/08:03:09, 6.90i/s]┫ 38233/200000 [01:32:20/08:03:02, 6.90i/s]┫ 38249/200000 [01:32:22/08:03:00, 6.90i/s]38265/200000 [01:32:24/08:02:59, 6.90i/s]38273/200000 [01:32:26/08:03:00, 6.90i/s]38281/200000 [01:32:27/08:02:59, 6.90i/s]38293/200000 [01:32:29/08:03:02, 6.90i/s]38299/200000 [01:32:30/08:03:04, 6.90i/s]38321/200000 [01:32:33/08:03:00, 6.90i/s]38333/200000 [01:32:35/08:03:03, 6.90i/s]38345/200000 [01:32:38/08:03:08, 6.90i/s]38371/200000 [01:32:42/08:03:08, 6.90i/s]38401/200000 [01:32:47/08:03:13, 6.90i/s]38422/200000 [01:32:51/08:03:17, 6.90i/s]38433/200000 [01:32:53/08:03:22, 6.90i/s]38440/200000 [01:32:54/08:03:23, 6.90i/s]38459/200000 [01:32:57/08:03:21, 6.90i/s]38465/200000 [01:32:58/08:03:23, 6.90i/s]38471/200000 [01:32:59/08:03:25, 6.90i/s]38497/200000 [01:33:03/08:03:26, 6.90i/s]38524/200000 [01:33:07/08:03:26, 6.90i/s]38556/200000 [01:33:12/08:03:29, 6.89i/s]38563/200000 [01:33:14/08:03:30, 6.89i/s]38577/200000 [01:33:16/08:03:31, 6.89i/s]38593/200000 [01:33:18/08:03:30, 6.89i/s]┫ 38658/200000 [01:33:26/08:03:23, 6.90i/s]38677/200000 [01:33:28/08:03:20, 6.90i/s]38686/200000 [01:33:30/08:03:20, 6.90i/s]38692/200000 [01:33:31/08:03:23, 6.90i/s]┫ 38702/200000 [01:33:32/08:03:20, 6.90i/s]38725/200000 [01:33:34/08:03:15, 6.90i/s]38727/200000 [01:33:36/08:03:21, 6.90i/s]38737/200000 [01:33:38/08:03:26, 6.90i/s]38752/200000 [01:33:41/08:03:28, 6.89i/s]38767/200000 [01:33:44/08:03:37, 6.89i/s]38774/200000 [01:33:46/08:03:38, 6.89i/s]38778/200000 [01:33:47/08:03:42, 6.89i/s]38780/200000 [01:33:49/08:03:49, 6.89i/s]38791/200000 [01:33:53/08:04:01, 6.89i/s]38805/200000 [01:33:57/08:04:11, 6.88i/s]┫ 38810/200000 [01:33:58/08:04:13, 6.88i/s]38832/200000 [01:34:02/08:04:16, 6.88i/s]38836/200000 [01:34:03/08:04:19, 6.88i/s]38849/200000 [01:34:05/08:04:23, 6.88i/s]38860/200000 [01:34:08/08:04:28, 6.88i/s]38866/200000 [01:34:09/08:04:30, 6.88i/s]38895/200000 [01:34:14/08:04:35, 6.88i/s]38921/200000 [01:34:19/08:04:42, 6.88i/s]38933/200000 [01:34:22/08:04:44, 6.88i/s]38938/200000 [01:34:23/08:04:46, 6.88i/s]38972/200000 [01:34:29/08:04:52, 6.87i/s]38993/200000 [01:34:32/08:04:54, 6.87i/s]┫ 39004/200000 [01:34:35/08:04:59, 6.87i/s]39025/200000 [01:34:39/08:05:03, 6.87i/s]39030/200000 [01:34:40/08:05:05, 6.87i/s]39050/200000 [01:34:43/08:05:08, 6.87i/s]39097/200000 [01:34:51/08:05:12, 6.87i/s]39106/200000 [01:34:52/08:05:12, 6.87i/s]39123/200000 [01:34:55/08:05:11, 6.87i/s]39161/200000 [01:35:01/08:05:14, 6.87i/s]39169/200000 [01:35:02/08:05:14, 6.87i/s]39176/200000 [01:35:03/08:05:16, 6.87i/s]┫ 39238/200000 [01:35:12/08:05:14, 6.87i/s]39248/200000 [01:35:13/08:05:13, 6.87i/s]39255/200000 [01:35:15/08:05:15, 6.87i/s]39261/200000 [01:35:16/08:05:17, 6.87i/s]39303/200000 [01:35:20/08:05:09, 6.87i/s]39311/200000 [01:35:22/08:05:09, 6.87i/s]39337/200000 [01:35:25/08:05:07, 6.87i/s]39350/200000 [01:35:26/08:05:03, 6.87i/s]39386/200000 [01:35:30/08:04:54, 6.87i/s]39444/200000 [01:35:35/08:04:41, 6.88i/s]39478/200000 [01:35:40/08:04:41, 6.88i/s]39482/200000 [01:35:42/08:04:44, 6.88i/s]39495/200000 [01:35:43/08:04:40, 6.88i/s]39501/200000 [01:35:44/08:04:41, 6.88i/s]39507/200000 [01:35:45/08:04:44, 6.88i/s]39522/200000 [01:35:48/08:04:46, 6.88i/s]39529/200000 [01:35:49/08:04:47, 6.88i/s]39557/200000 [01:35:51/08:04:37, 6.88i/s]39567/200000 [01:35:52/08:04:36, 6.88i/s]7.65e-02  21.98%┣██▊          ┫ 43968/200000 [01:46:57/08:06:30, 6.85i/s]39631/200000 [01:36:02/08:04:37, 6.88i/s]39637/200000 [01:36:03/08:04:39, 6.88i/s]39699/200000 [01:36:09/08:04:23, 6.88i/s]39730/200000 [01:36:12/08:04:18, 6.88i/s]39763/200000 [01:36:16/08:04:12, 6.88i/s]39780/200000 [01:36:18/08:04:12, 6.88i/s]39798/200000 [01:36:21/08:04:12, 6.88i/s]39820/200000 [01:36:24/08:04:09, 6.88i/s]39829/200000 [01:36:25/08:04:09, 6.88i/s]39852/200000 [01:36:27/08:04:04, 6.89i/s]39899/200000 [01:36:33/08:04:00, 6.89i/s]39919/200000 [01:36:36/08:03:58, 6.89i/s]39956/200000 [01:36:42/08:04:00, 6.89i/s]39965/200000 [01:36:43/08:03:59, 6.89i/s]39974/200000 [01:36:44/08:03:59, 6.89i/s]39992/200000 [01:36:47/08:03:59, 6.89i/s]40002/200000 [01:38:11/08:10:55, 6.79i/s]40009/200000 [01:38:13/08:10:57, 6.79i/s]40023/200000 [01:38:15/08:10:59, 6.79i/s]40035/200000 [01:38:18/08:11:03, 6.79i/s]40053/200000 [01:38:22/08:11:09, 6.79i/s]40057/200000 [01:38:23/08:11:12, 6.79i/s]40077/200000 [01:38:28/08:11:23, 6.78i/s]40121/200000 [01:38:32/08:11:09, 6.79i/s]40143/200000 [01:38:36/08:11:16, 6.79i/s]40185/200000 [01:38:44/08:11:24, 6.78i/s]40195/200000 [01:38:46/08:11:28, 6.78i/s]40209/200000 [01:38:49/08:11:30, 6.78i/s]40234/200000 [01:38:54/08:11:37, 6.78i/s]40251/200000 [01:38:57/08:11:42, 6.78i/s]40282/200000 [01:39:05/08:11:56, 6.78i/s]40291/200000 [01:39:07/08:12:02, 6.77i/s]┫ 40303/200000 [01:39:10/08:12:06, 6.77i/s]40317/200000 [01:39:14/08:12:15, 6.77i/s]40320/200000 [01:39:15/08:12:19, 6.77i/s]40333/200000 [01:39:18/08:12:23, 6.77i/s]40350/200000 [01:39:20/08:12:21, 6.77i/s]40360/200000 [01:39:21/08:12:20, 6.77i/s]40369/200000 [01:39:22/08:12:20, 6.77i/s]40385/200000 [01:39:25/08:12:19, 6.77i/s]40417/200000 [01:39:31/08:12:27, 6.77i/s]40435/200000 [01:39:35/08:12:33, 6.77i/s]40441/200000 [01:39:36/08:12:33, 6.77i/s]40445/200000 [01:39:37/08:12:37, 6.77i/s]40502/200000 [01:39:47/08:12:44, 6.76i/s]40550/200000 [01:39:53/08:12:40, 6.77i/s]40559/200000 [01:39:55/08:12:40, 6.77i/s]40579/200000 [01:39:59/08:12:48, 6.76i/s]40584/200000 [01:40:01/08:12:51, 6.76i/s]40642/200000 [01:40:12/08:13:06, 6.76i/s]40668/200000 [01:40:17/08:13:12, 6.76i/s]40674/200000 [01:40:19/08:13:14, 6.76i/s]40680/200000 [01:40:20/08:13:16, 6.76i/s]40691/200000 [01:40:22/08:13:20, 6.76i/s]40701/200000 [01:40:24/08:13:24, 6.76i/s]40709/200000 [01:40:26/08:13:24, 6.76i/s]40719/200000 [01:40:27/08:13:22, 6.76i/s]┫ 40724/200000 [01:40:28/08:13:24, 6.76i/s]40743/200000 [01:40:31/08:13:27, 6.76i/s]40804/200000 [01:40:41/08:13:32, 6.75i/s]40812/200000 [01:40:43/08:13:32, 6.75i/s]┫ 40832/200000 [01:40:47/08:13:37, 6.75i/s]40845/200000 [01:40:49/08:13:40, 6.75i/s]40862/200000 [01:40:52/08:13:41, 6.75i/s]40881/200000 [01:40:54/08:13:39, 6.75i/s]40904/200000 [01:40:58/08:13:39, 6.75i/s]40938/200000 [01:41:03/08:13:38, 6.75i/s]40950/200000 [01:41:04/08:13:35, 6.75i/s]40957/200000 [01:41:05/08:13:36, 6.75i/s]40964/200000 [01:41:06/08:13:36, 6.75i/s]40976/200000 [01:41:07/08:13:33, 6.75i/s]40982/200000 [01:41:08/08:13:34, 6.75i/s]40990/200000 [01:41:10/08:13:35, 6.75i/s]40998/200000 [01:41:11/08:13:35, 6.75i/s]41007/200000 [01:41:12/08:13:35, 6.75i/s]41022/200000 [01:41:15/08:13:37, 6.75i/s]41069/200000 [01:41:19/08:13:24, 6.76i/s]41075/200000 [01:41:20/08:13:25, 6.76i/s]┫ 41083/200000 [01:41:21/08:13:26, 6.76i/s]41095/200000 [01:41:24/08:13:28, 6.75i/s]41100/200000 [01:41:25/08:13:30, 6.75i/s]41114/200000 [01:41:27/08:13:33, 6.75i/s]41131/200000 [01:41:31/08:13:38, 6.75i/s]┫ 41136/200000 [01:41:32/08:13:40, 6.75i/s]41139/200000 [01:41:33/08:13:44, 6.75i/s]41145/200000 [01:41:35/08:13:45, 6.75i/s]41179/200000 [01:41:40/08:13:45, 6.75i/s]41193/200000 [01:41:42/08:13:47, 6.75i/s]┫ 41203/200000 [01:41:43/08:13:45, 6.75i/s]41211/200000 [01:41:44/08:13:46, 6.75i/s]41217/200000 [01:41:46/08:13:47, 6.75i/s]41229/200000 [01:41:48/08:13:51, 6.75i/s]41235/200000 [01:41:49/08:13:51, 6.75i/s]41252/200000 [01:41:53/08:13:57, 6.75i/s]41259/200000 [01:41:54/08:13:59, 6.75i/s]┫ 41263/200000 [01:41:56/08:14:02, 6.75i/s]41273/200000 [01:41:57/08:14:00, 6.75i/s]┫ 41286/200000 [01:41:59/08:14:02, 6.75i/s]41291/200000 [01:42:00/08:14:05, 6.75i/s]41299/200000 [01:42:02/08:14:05, 6.75i/s]41333/200000 [01:42:08/08:14:12, 6.74i/s]41340/200000 [01:42:09/08:14:13, 6.74i/s]41358/200000 [01:42:13/08:14:18, 6.74i/s]41362/200000 [01:42:14/08:14:21, 6.74i/s]41372/200000 [01:42:17/08:14:26, 6.74i/s]41383/200000 [01:42:19/08:14:31, 6.74i/s]41395/200000 [01:42:22/08:14:34, 6.74i/s]41400/200000 [01:42:23/08:14:37, 6.74i/s]41428/200000 [01:42:30/08:14:48, 6.74i/s]41440/200000 [01:42:32/08:14:52, 6.74i/s]41469/200000 [01:42:37/08:14:54, 6.74i/s]41474/200000 [01:42:38/08:14:56, 6.73i/s]41488/200000 [01:42:40/08:14:57, 6.73i/s]┫ 41499/200000 [01:42:43/08:15:01, 6.73i/s]41504/200000 [01:42:44/08:15:03, 6.73i/s]41514/200000 [01:42:47/08:15:09, 6.73i/s]41526/200000 [01:42:49/08:15:11, 6.73i/s]41533/200000 [01:42:50/08:15:12, 6.73i/s]41566/200000 [01:42:56/08:15:19, 6.73i/s]41580/200000 [01:42:59/08:15:20, 6.73i/s]41590/200000 [01:43:01/08:15:24, 6.73i/s]41622/200000 [01:43:03/08:15:11, 6.73i/s]41643/200000 [01:43:05/08:15:07, 6.73i/s]41663/200000 [01:43:06/08:14:58, 6.73i/s]41727/200000 [01:43:14/08:14:47, 6.74i/s]41768/200000 [01:43:17/08:14:32, 6.74i/s]41807/200000 [01:43:20/08:14:20, 6.74i/s]41815/200000 [01:43:21/08:14:19, 6.74i/s]41827/200000 [01:43:22/08:14:16, 6.74i/s]41880/200000 [01:43:27/08:14:00, 6.75i/s]41897/200000 [01:43:28/08:13:53, 6.75i/s]41948/200000 [01:43:32/08:13:38, 6.75i/s]41970/200000 [01:43:35/08:13:35, 6.75i/s]41990/200000 [01:43:37/08:13:33, 6.75i/s]42016/200000 [01:43:40/08:13:26, 6.76i/s]42071/200000 [01:43:43/08:13:04, 6.76i/s]42100/200000 [01:43:46/08:12:55, 6.76i/s]┫ 42112/200000 [01:43:47/08:12:53, 6.76i/s]42123/200000 [01:43:49/08:12:57, 6.76i/s]42132/200000 [01:43:51/08:12:57, 6.76i/s]42156/200000 [01:43:53/08:12:50, 6.76i/s]┫ 42174/200000 [01:43:55/08:12:49, 6.76i/s]42185/200000 [01:43:56/08:12:46, 6.76i/s]42265/200000 [01:44:03/08:12:22, 6.77i/s]42305/200000 [01:44:06/08:12:09, 6.77i/s]▊          ┫ 42322/200000 [01:44:09/08:12:09, 6.77i/s]42339/200000 [01:44:11/08:12:09, 6.77i/s]42361/200000 [01:44:13/08:12:04, 6.77i/s]42404/200000 [01:44:17/08:11:50, 6.78i/s]42439/200000 [01:44:18/08:11:30, 6.78i/s]42459/200000 [01:44:20/08:11:27, 6.78i/s]42508/200000 [01:44:24/08:11:13, 6.79i/s]42522/200000 [01:44:25/08:11:08, 6.79i/s]┫ 42551/200000 [01:44:29/08:11:07, 6.79i/s]42576/200000 [01:44:32/08:11:00, 6.79i/s]42588/200000 [01:44:33/08:10:58, 6.79i/s]42611/200000 [01:44:34/08:10:48, 6.79i/s]42626/200000 [01:44:35/08:10:42, 6.79i/s]42644/200000 [01:44:36/08:10:35, 6.79i/s]42818/200000 [01:44:53/08:09:54, 6.80i/s]42829/200000 [01:44:54/08:09:52, 6.80i/s]42872/200000 [01:44:56/08:09:33, 6.81i/s]42903/200000 [01:44:59/08:09:22, 6.81i/s]42928/200000 [01:45:01/08:09:15, 6.81i/s]42972/200000 [01:45:04/08:09:01, 6.82i/s]43006/200000 [01:45:08/08:08:56, 6.82i/s]43018/200000 [01:45:09/08:08:52, 6.82i/s]43026/200000 [01:45:10/08:08:52, 6.82i/s]43057/200000 [01:45:12/08:08:41, 6.82i/s]43074/200000 [01:45:15/08:08:40, 6.82i/s]43101/200000 [01:45:18/08:08:39, 6.82i/s]43144/200000 [01:45:24/08:08:37, 6.82i/s]43157/200000 [01:45:25/08:08:33, 6.82i/s]┫ 43184/200000 [01:45:28/08:08:25, 6.82i/s]43193/200000 [01:45:29/08:08:25, 6.82i/s]43282/200000 [01:45:38/08:08:06, 6.83i/s]┫ 43295/200000 [01:45:39/08:08:02, 6.83i/s]43372/200000 [01:45:46/08:07:44, 6.83i/s]43377/200000 [01:45:47/08:07:46, 6.83i/s]43388/200000 [01:45:49/08:07:44, 6.83i/s]43457/200000 [01:45:57/08:07:35, 6.84i/s]43468/200000 [01:45:58/08:07:32, 6.84i/s]┫ 43602/200000 [01:46:11/08:07:05, 6.84i/s]43646/200000 [01:46:16/08:06:58, 6.85i/s]43673/200000 [01:46:17/08:06:45, 6.85i/s]43680/200000 [01:46:18/08:06:45, 6.85i/s]43684/200000 [01:46:20/08:06:48, 6.85i/s]43731/200000 [01:46:27/08:06:52, 6.85i/s]43748/200000 [01:46:30/08:06:53, 6.85i/s]43797/200000 [01:46:37/08:06:53, 6.85i/s]43818/200000 [01:46:41/08:06:57, 6.85i/s]43824/200000 [01:46:42/08:06:59, 6.84i/s]43832/200000 [01:46:44/08:07:00, 6.84i/s]43891/200000 [01:46:48/08:06:39, 6.85i/s]43909/200000 [01:46:50/08:06:37, 6.85i/s]43929/200000 [01:46:52/08:06:34, 6.85i/s]43960/200000 [01:46:56/08:06:29, 6.85i/s]5.85e-01  24.12%┣███▏         ┫ 48232/200000 [01:57:48/08:08:28, 6.82i/s]44001/200000 [01:47:02/08:06:28, 6.85i/s]44010/200000 [01:47:03/08:06:27, 6.85i/s]44028/200000 [01:47:04/08:06:21, 6.85i/s]44065/200000 [01:47:06/08:06:06, 6.86i/s]44165/200000 [01:47:14/08:05:35, 6.86i/s]┫ 44171/200000 [01:47:15/08:05:37, 6.86i/s]44187/200000 [01:47:16/08:05:31, 6.87i/s]44199/200000 [01:47:17/08:05:29, 6.87i/s]44208/200000 [01:47:19/08:05:29, 6.87i/s]44323/200000 [01:47:27/08:04:51, 6.87i/s]44346/200000 [01:47:28/08:04:41, 6.88i/s]44391/200000 [01:47:31/08:04:26, 6.88i/s]44407/200000 [01:47:34/08:04:25, 6.88i/s]44439/200000 [01:47:36/08:04:14, 6.88i/s]44458/200000 [01:47:37/08:04:06, 6.89i/s]██▉          ┫ 44476/200000 [01:47:38/08:03:59, 6.89i/s]44488/200000 [01:47:39/08:03:57, 6.89i/s]44514/200000 [01:47:42/08:03:52, 6.89i/s]44543/200000 [01:47:44/08:03:42, 6.89i/s]44586/200000 [01:47:48/08:03:35, 6.89i/s]44597/200000 [01:47:51/08:03:38, 6.89i/s]┫ 44683/200000 [01:48:00/08:03:24, 6.90i/s]44699/200000 [01:48:01/08:03:19, 6.90i/s]44706/200000 [01:48:02/08:03:20, 6.90i/s]44729/200000 [01:48:05/08:03:17, 6.90i/s]44760/200000 [01:48:07/08:03:07, 6.90i/s]44776/200000 [01:48:08/08:03:02, 6.90i/s]44800/200000 [01:48:11/08:02:56, 6.90i/s]┫ 44819/200000 [01:48:13/08:02:53, 6.90i/s]44872/200000 [01:48:20/08:02:50, 6.90i/s]44945/200000 [01:48:24/08:02:23, 6.91i/s]44952/200000 [01:48:25/08:02:23, 6.91i/s]┫ 44983/200000 [01:48:29/08:02:21, 6.91i/s]45010/200000 [01:48:32/08:02:18, 6.91i/s]45039/200000 [01:48:35/08:02:09, 6.91i/s]45053/200000 [01:48:36/08:02:05, 6.91i/s]┫ 45063/200000 [01:48:37/08:02:04, 6.91i/s]45085/200000 [01:48:40/08:02:05, 6.91i/s]45102/200000 [01:48:41/08:01:59, 6.92i/s]45106/200000 [01:48:43/08:02:02, 6.92i/s]45112/200000 [01:48:44/08:02:04, 6.91i/s]45126/200000 [01:48:47/08:02:06, 6.91i/s]45134/200000 [01:48:48/08:02:07, 6.91i/s]45148/200000 [01:48:50/08:02:09, 6.91i/s]45155/200000 [01:48:52/08:02:11, 6.91i/s]45171/200000 [01:48:54/08:02:12, 6.91i/s]45182/200000 [01:48:56/08:02:11, 6.91i/s]45192/200000 [01:48:57/08:02:09, 6.91i/s]45200/200000 [01:48:58/08:02:09, 6.91i/s]45237/200000 [01:49:01/08:02:00, 6.92i/s]45308/200000 [01:49:07/08:01:38, 6.92i/s]┫ 45339/200000 [01:49:10/08:01:33, 6.92i/s]45351/200000 [01:49:11/08:01:30, 6.92i/s]45359/200000 [01:49:12/08:01:30, 6.92i/s]┫ 45368/200000 [01:49:13/08:01:30, 6.92i/s]45376/200000 [01:49:14/08:01:29, 6.92i/s]45395/200000 [01:49:18/08:01:34, 6.92i/s]45412/200000 [01:49:21/08:01:35, 6.92i/s]45522/200000 [01:49:31/08:01:09, 6.93i/s]45538/200000 [01:49:33/08:01:08, 6.93i/s]45544/200000 [01:49:34/08:01:10, 6.93i/s]45589/200000 [01:49:41/08:01:09, 6.93i/s]45601/200000 [01:49:42/08:01:06, 6.93i/s]45610/200000 [01:49:43/08:01:06, 6.93i/s]45621/200000 [01:49:44/08:01:04, 6.93i/s]45640/200000 [01:49:46/08:01:02, 6.93i/s]45655/200000 [01:49:48/08:01:02, 6.93i/s]45681/200000 [01:49:52/08:01:01, 6.93i/s]┫ 45717/200000 [01:49:56/08:00:58, 6.93i/s]45731/200000 [01:49:59/08:00:59, 6.93i/s]45735/200000 [01:50:00/08:01:02, 6.93i/s]45739/200000 [01:50:01/08:01:05, 6.93i/s]45758/200000 [01:50:03/08:01:03, 6.93i/s]┫ 45776/200000 [01:50:06/08:01:02, 6.93i/s]45785/200000 [01:50:07/08:01:02, 6.93i/s]45792/200000 [01:50:08/08:01:02, 6.93i/s]45799/200000 [01:50:10/08:01:03, 6.93i/s]45835/200000 [01:50:13/08:00:55, 6.93i/s]45848/200000 [01:50:14/08:00:51, 6.93i/s]45868/200000 [01:50:17/08:00:54, 6.93i/s]45886/200000 [01:50:20/08:00:54, 6.93i/s]45908/200000 [01:50:22/08:00:50, 6.93i/s]45916/200000 [01:50:23/08:00:50, 6.93i/s]45931/200000 [01:50:26/08:00:52, 6.93i/s]┫ 45936/200000 [01:50:27/08:00:55, 6.93i/s]45947/200000 [01:50:30/08:00:57, 6.93i/s]┫ 45952/200000 [01:50:31/08:01:00, 6.93i/s]45961/200000 [01:50:32/08:01:00, 6.93i/s]┫ 46010/200000 [01:50:38/08:00:56, 6.93i/s]46027/200000 [01:50:41/08:00:56, 6.93i/s]46033/200000 [01:50:42/08:00:58, 6.93i/s]46100/200000 [01:50:55/08:01:13, 6.93i/s]46112/200000 [01:50:56/08:01:10, 6.93i/s]46149/200000 [01:51:02/08:01:13, 6.93i/s]46155/200000 [01:51:04/08:01:15, 6.93i/s]46162/200000 [01:51:05/08:01:16, 6.93i/s]46196/200000 [01:51:11/08:01:22, 6.92i/s]┫ 46210/200000 [01:51:14/08:01:24, 6.92i/s]46221/200000 [01:51:15/08:01:21, 6.92i/s]46238/200000 [01:51:16/08:01:16, 6.93i/s]46276/200000 [01:51:20/08:01:12, 6.93i/s]46298/200000 [01:51:24/08:01:15, 6.93i/s]46326/200000 [01:51:28/08:01:12, 6.93i/s]┫ 46333/200000 [01:51:29/08:01:14, 6.93i/s]46338/200000 [01:51:30/08:01:16, 6.93i/s]┫ 46354/200000 [01:51:33/08:01:17, 6.93i/s]46379/200000 [01:51:37/08:01:19, 6.93i/s]46389/200000 [01:51:38/08:01:18, 6.93i/s]46422/200000 [01:51:45/08:01:25, 6.92i/s]46433/200000 [01:51:47/08:01:29, 6.92i/s]46506/200000 [01:51:58/08:01:32, 6.92i/s]46524/200000 [01:52:01/08:01:32, 6.92i/s]46568/200000 [01:52:08/08:01:35, 6.92i/s]46584/200000 [01:52:10/08:01:36, 6.92i/s]46592/200000 [01:52:11/08:01:35, 6.92i/s]46598/200000 [01:52:13/08:01:37, 6.92i/s]46616/200000 [01:52:15/08:01:35, 6.92i/s]46621/200000 [01:52:16/08:01:37, 6.92i/s]┫ 46635/200000 [01:52:19/08:01:40, 6.92i/s]46654/200000 [01:52:20/08:01:32, 6.92i/s]46703/200000 [01:52:27/08:01:32, 6.92i/s]┫ 46733/200000 [01:52:32/08:01:35, 6.92i/s]46743/200000 [01:52:34/08:01:38, 6.92i/s]┫ 46750/200000 [01:52:35/08:01:40, 6.92i/s]46757/200000 [01:52:37/08:01:41, 6.92i/s]46767/200000 [01:52:39/08:01:46, 6.92i/s]46784/200000 [01:52:45/08:02:00, 6.92i/s]46795/200000 [01:52:49/08:02:10, 6.91i/s]46814/200000 [01:52:54/08:02:20, 6.91i/s]46819/200000 [01:52:56/08:02:24, 6.91i/s]46836/200000 [01:52:58/08:02:23, 6.91i/s]46849/200000 [01:53:01/08:02:27, 6.91i/s]46879/200000 [01:53:07/08:02:33, 6.91i/s]46886/200000 [01:53:08/08:02:34, 6.91i/s]46908/200000 [01:53:13/08:02:41, 6.91i/s]46916/200000 [01:53:14/08:02:40, 6.91i/s]46945/200000 [01:53:19/08:02:48, 6.90i/s]46960/200000 [01:53:22/08:02:49, 6.90i/s]46978/200000 [01:53:25/08:02:53, 6.90i/s]┫ 46985/200000 [01:53:27/08:02:54, 6.90i/s]47001/200000 [01:53:29/08:02:55, 6.90i/s]47030/200000 [01:53:35/08:03:02, 6.90i/s]47038/200000 [01:53:36/08:03:03, 6.90i/s]47052/200000 [01:53:39/08:03:04, 6.90i/s]47066/200000 [01:53:41/08:03:06, 6.90i/s]47142/200000 [01:53:53/08:03:08, 6.90i/s]47175/200000 [01:53:59/08:03:13, 6.90i/s]47184/200000 [01:54:00/08:03:13, 6.90i/s]47212/200000 [01:54:04/08:03:13, 6.90i/s]47218/200000 [01:54:05/08:03:14, 6.90i/s]┫ 47259/200000 [01:54:11/08:03:15, 6.90i/s]47283/200000 [01:54:14/08:03:10, 6.90i/s]47316/200000 [01:54:18/08:03:10, 6.90i/s]47386/200000 [01:54:25/08:02:56, 6.90i/s]47396/200000 [01:54:27/08:02:56, 6.90i/s]47416/200000 [01:54:29/08:02:53, 6.90i/s]47444/200000 [01:54:31/08:02:46, 6.90i/s]47478/200000 [01:54:36/08:02:45, 6.90i/s]47501/200000 [01:54:40/08:02:46, 6.90i/s]47517/200000 [01:54:42/08:02:48, 6.90i/s]47522/200000 [01:54:44/08:02:50, 6.90i/s]47577/200000 [01:54:49/08:02:40, 6.91i/s]47590/200000 [01:54:52/08:02:43, 6.91i/s]┫ 47633/200000 [01:54:58/08:02:41, 6.91i/s]47666/200000 [01:55:01/08:02:36, 6.91i/s]47701/200000 [01:55:05/08:02:29, 6.91i/s]47730/200000 [01:55:08/08:02:26, 6.91i/s]47742/200000 [01:55:09/08:02:23, 6.91i/s]47749/200000 [01:55:10/08:02:24, 6.91i/s]47760/200000 [01:55:11/08:02:21, 6.91i/s]47768/200000 [01:55:12/08:02:22, 6.91i/s]┫ 47779/200000 [01:55:13/08:02:19, 6.91i/s]47795/200000 [01:55:16/08:02:20, 6.91i/s]47815/200000 [01:55:18/08:02:18, 6.91i/s]47895/200000 [01:55:28/08:02:11, 6.91i/s]47913/200000 [01:55:30/08:02:09, 6.91i/s]┫ 47923/200000 [01:55:32/08:02:08, 6.91i/s]47941/200000 [01:55:34/08:02:07, 6.91i/s]47977/200000 [01:55:40/08:02:10, 6.91i/s]47986/200000 [01:55:41/08:02:10, 6.91i/s]48001/200000 [01:55:43/08:02:10, 6.91i/s]48002/200000 [01:57:06/08:07:54, 6.83i/s]48009/200000 [01:57:07/08:07:55, 6.83i/s]48015/200000 [01:57:09/08:07:57, 6.83i/s]48044/200000 [01:57:14/08:08:00, 6.83i/s]48049/200000 [01:57:15/08:08:03, 6.83i/s]48057/200000 [01:57:18/08:08:08, 6.83i/s]┫ 48065/200000 [01:57:20/08:08:14, 6.83i/s]48077/200000 [01:57:23/08:08:17, 6.83i/s]48083/200000 [01:57:24/08:08:19, 6.83i/s]48095/200000 [01:57:25/08:08:17, 6.83i/s]48121/200000 [01:57:26/08:08:06, 6.83i/s]┫ 48128/200000 [01:57:27/08:08:06, 6.83i/s]48137/200000 [01:57:30/08:08:11, 6.83i/s]48195/200000 [01:57:41/08:08:22, 6.83i/s]┫ 48215/200000 [01:57:45/08:08:24, 6.82i/s]48227/200000 [01:57:47/08:08:27, 6.82i/s]4.60e-01  26.25%┣███▍         ┫ 52476/200000 [02:06:31/08:02:12, 6.91i/s]48257/200000 [01:57:53/08:08:33, 6.82i/s]48267/200000 [01:57:55/08:08:38, 6.82i/s]48277/200000 [01:57:58/08:08:43, 6.82i/s]48282/200000 [01:57:59/08:08:45, 6.82i/s]48286/200000 [01:58:01/08:08:48, 6.82i/s]48291/200000 [01:58:02/08:08:50, 6.82i/s]48303/200000 [01:58:05/08:08:54, 6.82i/s]48317/200000 [01:58:08/08:09:02, 6.82i/s]48320/200000 [01:58:10/08:09:05, 6.82i/s]48324/200000 [01:58:11/08:09:09, 6.81i/s]48333/200000 [01:58:12/08:09:08, 6.81i/s]48364/200000 [01:58:16/08:09:05, 6.82i/s]48391/200000 [01:58:21/08:09:06, 6.82i/s]48435/200000 [01:58:29/08:09:17, 6.81i/s]48449/200000 [01:58:33/08:09:23, 6.81i/s]48455/200000 [01:58:34/08:09:24, 6.81i/s]┫ 48479/200000 [01:58:38/08:09:25, 6.81i/s]48517/200000 [01:58:44/08:09:28, 6.81i/s]48559/200000 [01:58:49/08:09:23, 6.81i/s]48581/200000 [01:58:54/08:09:31, 6.81i/s]48604/200000 [01:58:59/08:09:38, 6.81i/s]48613/200000 [01:59:01/08:09:38, 6.81i/s]48625/200000 [01:59:03/08:09:41, 6.81i/s]48633/200000 [01:59:05/08:09:42, 6.81i/s]┫ 48640/200000 [01:59:06/08:09:43, 6.81i/s]48651/200000 [01:59:08/08:09:46, 6.81i/s]48657/200000 [01:59:09/08:09:47, 6.81i/s]48668/200000 [01:59:12/08:09:49, 6.81i/s]48672/200000 [01:59:13/08:09:51, 6.80i/s]48683/200000 [01:59:15/08:09:54, 6.80i/s]48694/200000 [01:59:17/08:09:58, 6.80i/s]48724/200000 [01:59:22/08:09:59, 6.80i/s]48755/200000 [01:59:27/08:10:00, 6.80i/s]┫ 48769/200000 [01:59:30/08:10:03, 6.80i/s]48804/200000 [01:59:36/08:10:06, 6.80i/s]48810/200000 [01:59:37/08:10:07, 6.80i/s]48859/200000 [01:59:46/08:10:15, 6.80i/s]48866/200000 [01:59:47/08:10:16, 6.80i/s]48877/200000 [01:59:48/08:10:14, 6.80i/s]48904/200000 [01:59:52/08:10:12, 6.80i/s]48916/200000 [01:59:54/08:10:15, 6.80i/s]48936/200000 [01:59:57/08:10:12, 6.80i/s]48944/200000 [01:59:58/08:10:13, 6.80i/s]48998/200000 [02:00:05/08:10:11, 6.80i/s]49022/200000 [02:00:09/08:10:12, 6.80i/s]49057/200000 [02:00:12/08:10:05, 6.80i/s]49075/200000 [02:00:15/08:10:03, 6.80i/s]49083/200000 [02:00:16/08:10:03, 6.80i/s]49095/200000 [02:00:18/08:10:05, 6.80i/s]┫ 49100/200000 [02:00:19/08:10:07, 6.80i/s]49114/200000 [02:00:22/08:10:09, 6.80i/s]49141/200000 [02:00:28/08:10:19, 6.80i/s]49160/200000 [02:00:31/08:10:17, 6.80i/s]49182/200000 [02:00:34/08:10:19, 6.80i/s]49190/200000 [02:00:36/08:10:20, 6.80i/s]49208/200000 [02:00:38/08:10:18, 6.80i/s]49213/200000 [02:00:39/08:10:20, 6.80i/s]49219/200000 [02:00:41/08:10:22, 6.80i/s]49224/200000 [02:00:42/08:10:24, 6.80i/s]49239/200000 [02:00:44/08:10:25, 6.80i/s]49269/200000 [02:00:51/08:10:34, 6.79i/s]49285/200000 [02:00:53/08:10:33, 6.80i/s]49311/200000 [02:00:58/08:10:37, 6.79i/s]49317/200000 [02:00:59/08:10:39, 6.79i/s]49325/200000 [02:01:00/08:10:39, 6.79i/s]49340/200000 [02:01:04/08:10:44, 6.79i/s]49346/200000 [02:01:05/08:10:45, 6.79i/s]┫ 49352/200000 [02:01:06/08:10:47, 6.79i/s]49413/200000 [02:01:20/08:11:06, 6.79i/s]49418/200000 [02:01:21/08:11:08, 6.79i/s]49423/200000 [02:01:23/08:11:10, 6.79i/s]┫ 49448/200000 [02:01:28/08:11:16, 6.79i/s]49455/200000 [02:01:29/08:11:18, 6.78i/s]49461/200000 [02:01:30/08:11:19, 6.78i/s]49488/200000 [02:01:35/08:11:21, 6.78i/s]49504/200000 [02:01:38/08:11:26, 6.78i/s]49509/200000 [02:01:40/08:11:28, 6.78i/s]49533/200000 [02:01:44/08:11:33, 6.78i/s]49553/200000 [02:01:48/08:11:37, 6.78i/s]49566/200000 [02:01:51/08:11:40, 6.78i/s]49574/200000 [02:01:52/08:11:40, 6.78i/s]49609/200000 [02:01:57/08:11:37, 6.78i/s]49622/200000 [02:01:58/08:11:34, 6.78i/s]49643/200000 [02:02:00/08:11:30, 6.78i/s]49662/200000 [02:02:01/08:11:23, 6.78i/s]49678/200000 [02:02:03/08:11:22, 6.78i/s]49693/200000 [02:02:06/08:11:24, 6.78i/s]49705/200000 [02:02:07/08:11:20, 6.78i/s]49735/200000 [02:02:09/08:11:11, 6.79i/s]49784/200000 [02:02:13/08:10:59, 6.79i/s]49813/200000 [02:02:15/08:10:52, 6.79i/s]49879/200000 [02:02:21/08:10:35, 6.79i/s]┫ 49937/200000 [02:02:25/08:10:19, 6.80i/s]49959/200000 [02:02:28/08:10:15, 6.80i/s]49978/200000 [02:02:30/08:10:14, 6.80i/s]50004/200000 [02:02:33/08:10:08, 6.80i/s]50021/200000 [02:02:35/08:10:06, 6.80i/s]50068/200000 [02:02:37/08:09:47, 6.81i/s]50113/200000 [02:02:41/08:09:39, 6.81i/s]         ┫ 50125/200000 [02:02:44/08:09:42, 6.81i/s]50135/200000 [02:02:45/08:09:40, 6.81i/s]50148/200000 [02:02:46/08:09:38, 6.81i/s]50161/200000 [02:02:47/08:09:34, 6.81i/s]50170/200000 [02:02:48/08:09:34, 6.81i/s]50191/200000 [02:02:51/08:09:31, 6.81i/s]50267/200000 [02:02:57/08:09:12, 6.81i/s]┫ 50283/200000 [02:02:58/08:09:07, 6.82i/s]50317/200000 [02:03:02/08:09:01, 6.82i/s]50345/200000 [02:03:06/08:09:00, 6.82i/s]50383/200000 [02:03:09/08:08:51, 6.82i/s]50482/200000 [02:03:16/08:08:21, 6.83i/s]50496/200000 [02:03:17/08:08:18, 6.83i/s]50511/200000 [02:03:18/08:08:13, 6.83i/s]50534/200000 [02:03:20/08:08:09, 6.83i/s]50545/200000 [02:03:22/08:08:07, 6.83i/s]50628/200000 [02:03:30/08:07:52, 6.83i/s]50672/200000 [02:03:33/08:07:37, 6.84i/s]50681/200000 [02:03:34/08:07:37, 6.84i/s]50701/200000 [02:03:36/08:07:34, 6.84i/s]50724/200000 [02:03:38/08:07:30, 6.84i/s]50742/200000 [02:03:41/08:07:29, 6.84i/s]50753/200000 [02:03:42/08:07:27, 6.84i/s]50768/200000 [02:03:43/08:07:23, 6.84i/s]50780/200000 [02:03:44/08:07:20, 6.84i/s]50804/200000 [02:03:46/08:07:15, 6.84i/s]50819/200000 [02:03:47/08:07:10, 6.84i/s]50872/200000 [02:03:51/08:06:53, 6.85i/s]50903/200000 [02:03:53/08:06:44, 6.85i/s]50959/200000 [02:03:57/08:06:30, 6.85i/s]50972/200000 [02:03:58/08:06:26, 6.85i/s]┫ 50990/200000 [02:04:00/08:06:20, 6.85i/s]50997/200000 [02:04:01/08:06:21, 6.85i/s]51005/200000 [02:04:02/08:06:21, 6.85i/s]51018/200000 [02:04:03/08:06:19, 6.85i/s]51031/200000 [02:04:05/08:06:17, 6.85i/s]51048/200000 [02:04:06/08:06:11, 6.86i/s]51149/200000 [02:04:19/08:06:05, 6.86i/s]51188/200000 [02:04:22/08:05:56, 6.86i/s]51194/200000 [02:04:23/08:05:57, 6.86i/s]51220/200000 [02:04:26/08:05:51, 6.86i/s]51243/200000 [02:04:28/08:05:46, 6.86i/s]51262/200000 [02:04:30/08:05:44, 6.86i/s]51328/200000 [02:04:36/08:05:30, 6.87i/s]┫ 51373/200000 [02:04:40/08:05:22, 6.87i/s]51383/200000 [02:04:41/08:05:20, 6.87i/s]51408/200000 [02:04:45/08:05:21, 6.87i/s]51462/200000 [02:04:51/08:05:13, 6.87i/s]51490/200000 [02:04:53/08:05:05, 6.87i/s]51507/200000 [02:04:54/08:05:00, 6.87i/s]51520/200000 [02:04:57/08:05:02, 6.87i/s]51558/200000 [02:05:00/08:04:55, 6.87i/s]51595/200000 [02:05:04/08:04:47, 6.88i/s]51605/200000 [02:05:05/08:04:46, 6.88i/s]51673/200000 [02:05:11/08:04:30, 6.88i/s]51680/200000 [02:05:12/08:04:30, 6.88i/s]51684/200000 [02:05:13/08:04:32, 6.88i/s]51691/200000 [02:05:14/08:04:34, 6.88i/s]51703/200000 [02:05:16/08:04:32, 6.88i/s]51709/200000 [02:05:17/08:04:33, 6.88i/s]51714/200000 [02:05:18/08:04:36, 6.88i/s]51737/200000 [02:05:22/08:04:38, 6.88i/s]51748/200000 [02:05:23/08:04:37, 6.88i/s]51763/200000 [02:05:26/08:04:38, 6.88i/s]51797/200000 [02:05:31/08:04:38, 6.88i/s]51805/200000 [02:05:32/08:04:38, 6.88i/s]51818/200000 [02:05:35/08:04:41, 6.88i/s]51832/200000 [02:05:37/08:04:43, 6.88i/s]51880/200000 [02:05:40/08:04:28, 6.88i/s]51891/200000 [02:05:41/08:04:26, 6.88i/s]51895/200000 [02:05:42/08:04:28, 6.88i/s]51908/200000 [02:05:43/08:04:25, 6.88i/s]┫ 51915/200000 [02:05:45/08:04:26, 6.88i/s]51929/200000 [02:05:46/08:04:22, 6.88i/s]51960/200000 [02:05:49/08:04:18, 6.88i/s]51977/200000 [02:05:52/08:04:18, 6.88i/s]51986/200000 [02:05:53/08:04:17, 6.88i/s]51991/200000 [02:05:54/08:04:19, 6.88i/s]52001/200000 [02:05:55/08:04:17, 6.88i/s]52028/200000 [02:05:57/08:04:11, 6.88i/s]52065/200000 [02:06:00/08:03:59, 6.89i/s]52077/200000 [02:06:01/08:03:57, 6.89i/s]52088/200000 [02:06:02/08:03:56, 6.89i/s]52134/200000 [02:06:05/08:03:42, 6.89i/s]52150/200000 [02:06:06/08:03:37, 6.89i/s]52187/200000 [02:06:10/08:03:29, 6.89i/s]52199/200000 [02:06:11/08:03:28, 6.89i/s]52218/200000 [02:06:13/08:03:26, 6.90i/s]52263/200000 [02:06:16/08:03:13, 6.90i/s]52282/200000 [02:06:17/08:03:07, 6.90i/s]52298/200000 [02:06:18/08:03:02, 6.90i/s]52314/200000 [02:06:19/08:02:57, 6.90i/s]52362/200000 [02:06:23/08:02:43, 6.91i/s]52391/200000 [02:06:25/08:02:35, 6.91i/s]52399/200000 [02:06:26/08:02:34, 6.91i/s]52407/200000 [02:06:27/08:02:34, 6.91i/s]52439/200000 [02:06:29/08:02:24, 6.91i/s]52488/200000 [02:06:32/08:02:10, 6.91i/s]52503/200000 [02:06:33/08:02:06, 6.91i/s]8.70e-01  28.20%┣███▋         ┫ 56391/200000 [02:17:15/08:06:46, 6.85i/s]52539/200000 [02:06:37/08:01:58, 6.92i/s]52583/200000 [02:06:41/08:01:51, 6.92i/s]52589/200000 [02:06:42/08:01:53, 6.92i/s]52617/200000 [02:06:46/08:01:51, 6.92i/s]52640/200000 [02:06:48/08:01:47, 6.92i/s]52647/200000 [02:06:50/08:01:48, 6.92i/s]52655/200000 [02:06:51/08:01:48, 6.92i/s]52664/200000 [02:06:52/08:01:48, 6.92i/s]52717/200000 [02:06:57/08:01:37, 6.92i/s]┫ 52765/200000 [02:07:01/08:01:28, 6.92i/s]52792/200000 [02:07:04/08:01:22, 6.92i/s]52835/200000 [02:07:09/08:01:17, 6.93i/s]52839/200000 [02:07:10/08:01:19, 6.93i/s]52845/200000 [02:07:11/08:01:20, 6.93i/s]52889/200000 [02:07:14/08:01:09, 6.93i/s]52929/200000 [02:07:17/08:00:56, 6.93i/s]52945/200000 [02:07:18/08:00:51, 6.93i/s]52973/200000 [02:07:22/08:00:51, 6.93i/s]53010/200000 [02:07:26/08:00:47, 6.93i/s]53037/200000 [02:07:28/08:00:40, 6.93i/s]53063/200000 [02:07:31/08:00:36, 6.94i/s]53072/200000 [02:07:32/08:00:35, 6.94i/s]53079/200000 [02:07:33/08:00:36, 6.94i/s]53106/200000 [02:07:36/08:00:34, 6.94i/s]53120/200000 [02:07:39/08:00:36, 6.94i/s]53163/200000 [02:07:46/08:00:41, 6.93i/s]53221/200000 [02:07:53/08:00:33, 6.94i/s]53231/200000 [02:07:54/08:00:32, 6.94i/s]53259/200000 [02:07:56/08:00:25, 6.94i/s]53308/200000 [02:08:00/08:00:15, 6.94i/s]53319/200000 [02:08:01/08:00:12, 6.94i/s]53351/200000 [02:08:05/08:00:07, 6.94i/s]┫ 53361/200000 [02:08:06/08:00:07, 6.94i/s]53406/200000 [02:08:13/08:00:10, 6.94i/s]┫ 53414/200000 [02:08:15/08:00:11, 6.94i/s]53421/200000 [02:08:16/08:00:12, 6.94i/s]53433/200000 [02:08:17/08:00:09, 6.94i/s]53457/200000 [02:08:19/08:00:05, 6.94i/s]┫ 53467/200000 [02:08:20/08:00:04, 6.94i/s]53482/200000 [02:08:21/08:00:00, 6.94i/s]53494/200000 [02:08:23/07:59:58, 6.95i/s]53516/200000 [02:08:24/07:59:50, 6.95i/s]53529/200000 [02:08:25/07:59:48, 6.95i/s]53653/200000 [02:08:42/07:59:43, 6.95i/s]53659/200000 [02:08:43/07:59:45, 6.95i/s]53694/200000 [02:08:47/07:59:42, 6.95i/s]53725/200000 [02:08:51/07:59:40, 6.95i/s]53739/200000 [02:08:55/07:59:48, 6.95i/s]53782/200000 [02:09:01/07:59:47, 6.95i/s]┫ 53790/200000 [02:09:02/07:59:47, 6.95i/s]53831/200000 [02:09:07/07:59:41, 6.95i/s]┫ 53856/200000 [02:09:10/07:59:41, 6.95i/s]53882/200000 [02:09:14/07:59:41, 6.95i/s]53944/200000 [02:09:24/07:59:44, 6.95i/s]53950/200000 [02:09:25/07:59:46, 6.95i/s]53956/200000 [02:09:26/07:59:47, 6.95i/s]53974/200000 [02:09:28/07:59:46, 6.95i/s]53982/200000 [02:09:30/07:59:46, 6.95i/s]54043/200000 [02:09:38/07:59:46, 6.95i/s]54062/200000 [02:09:42/07:59:50, 6.95i/s]┫ 54078/200000 [02:09:45/07:59:50, 6.95i/s]54086/200000 [02:09:46/07:59:50, 6.95i/s]54091/200000 [02:09:47/07:59:54, 6.95i/s]54096/200000 [02:09:49/07:59:57, 6.95i/s]54101/200000 [02:09:50/07:59:59, 6.94i/s]54131/200000 [02:09:54/07:59:56, 6.95i/s]54163/200000 [02:10:00/08:00:02, 6.94i/s]54169/200000 [02:10:01/08:00:04, 6.94i/s]54182/200000 [02:10:04/08:00:07, 6.94i/s]54230/200000 [02:10:10/08:00:04, 6.94i/s]54279/200000 [02:10:16/08:00:00, 6.94i/s]┫ 54288/200000 [02:10:17/07:59:59, 6.94i/s]54293/200000 [02:10:18/08:00:00, 6.94i/s]54300/200000 [02:10:20/08:00:01, 6.94i/s]54311/200000 [02:10:21/07:59:59, 6.94i/s]54331/200000 [02:10:23/07:59:58, 6.94i/s]54337/200000 [02:10:24/08:00:00, 6.94i/s]54341/200000 [02:10:26/08:00:02, 6.94i/s]54393/200000 [02:10:33/08:00:03, 6.94i/s]54404/200000 [02:10:36/08:00:05, 6.94i/s]54417/200000 [02:10:38/08:00:08, 6.94i/s]54423/200000 [02:10:39/08:00:09, 6.94i/s]54429/200000 [02:10:41/08:00:11, 6.94i/s]┫ 54471/200000 [02:10:47/08:00:13, 6.94i/s]54484/200000 [02:10:48/08:00:10, 6.94i/s]54492/200000 [02:10:50/08:00:11, 6.94i/s]54522/200000 [02:10:55/08:00:13, 6.94i/s]54538/200000 [02:10:58/08:00:16, 6.94i/s]54556/200000 [02:11:01/08:00:16, 6.94i/s]┫ 54563/200000 [02:11:02/08:00:17, 6.94i/s]54602/200000 [02:11:08/08:00:19, 6.94i/s]54612/200000 [02:11:09/08:00:18, 6.94i/s]54620/200000 [02:11:10/08:00:18, 6.94i/s]54647/200000 [02:11:14/08:00:16, 6.94i/s]54661/200000 [02:11:15/08:00:12, 6.94i/s]54678/200000 [02:11:17/08:00:13, 6.94i/s]54727/200000 [02:11:24/08:00:13, 6.94i/s]54733/200000 [02:11:26/08:00:15, 6.94i/s]54752/200000 [02:11:29/08:00:18, 6.94i/s]54767/200000 [02:11:33/08:00:24, 6.94i/s]54774/200000 [02:11:34/08:00:24, 6.94i/s]54778/200000 [02:11:35/08:00:27, 6.94i/s]54795/200000 [02:11:42/08:00:43, 6.93i/s]54800/200000 [02:11:44/08:00:46, 6.93i/s]54816/200000 [02:11:48/08:00:51, 6.93i/s]54819/200000 [02:11:49/08:00:56, 6.93i/s]54836/200000 [02:11:51/08:00:54, 6.93i/s]54848/200000 [02:11:54/08:00:57, 6.93i/s]54880/200000 [02:12:00/08:01:03, 6.93i/s]54887/200000 [02:12:01/08:01:05, 6.93i/s]54893/200000 [02:12:03/08:01:06, 6.93i/s]54905/200000 [02:12:05/08:01:09, 6.93i/s]54933/200000 [02:12:10/08:01:13, 6.93i/s]┫ 54973/200000 [02:12:18/08:01:18, 6.93i/s]┫ 54978/200000 [02:12:19/08:01:20, 6.93i/s]55007/200000 [02:12:24/08:01:24, 6.92i/s]55028/200000 [02:12:28/08:01:28, 6.92i/s]55054/200000 [02:12:33/08:01:33, 6.92i/s]55075/200000 [02:12:37/08:01:34, 6.92i/s]55089/200000 [02:12:39/08:01:35, 6.92i/s]55095/200000 [02:12:40/08:01:36, 6.92i/s]55102/200000 [02:12:41/08:01:37, 6.92i/s]55112/200000 [02:12:42/08:01:35, 6.92i/s]55155/200000 [02:12:49/08:01:37, 6.92i/s]┫ 55169/200000 [02:12:52/08:01:39, 6.92i/s]55184/200000 [02:12:54/08:01:40, 6.92i/s]55218/200000 [02:12:59/08:01:42, 6.92i/s]55228/200000 [02:13:00/08:01:40, 6.92i/s]55238/200000 [02:13:02/08:01:39, 6.92i/s]55248/200000 [02:13:03/08:01:39, 6.92i/s]55259/200000 [02:13:05/08:01:42, 6.92i/s]55273/200000 [02:13:07/08:01:40, 6.92i/s]┫ 55289/200000 [02:13:09/08:01:39, 6.92i/s]55300/200000 [02:13:10/08:01:37, 6.92i/s]55309/200000 [02:13:11/08:01:37, 6.92i/s]55338/200000 [02:13:15/08:01:35, 6.92i/s]55351/200000 [02:13:16/08:01:33, 6.92i/s]55416/200000 [02:13:23/08:01:24, 6.92i/s]┫ 55436/200000 [02:13:24/08:01:17, 6.93i/s]55444/200000 [02:13:25/08:01:17, 6.93i/s]55453/200000 [02:13:27/08:01:17, 6.93i/s]55474/200000 [02:13:29/08:01:15, 6.93i/s]55482/200000 [02:13:32/08:01:20, 6.93i/s]┫ 55495/200000 [02:13:33/08:01:17, 6.93i/s]55501/200000 [02:13:34/08:01:18, 6.93i/s]55507/200000 [02:13:35/08:01:20, 6.93i/s]55517/200000 [02:13:37/08:01:20, 6.93i/s]55522/200000 [02:13:38/08:01:21, 6.92i/s]55529/200000 [02:13:39/08:01:22, 6.92i/s]55569/200000 [02:13:42/08:01:14, 6.93i/s]55590/200000 [02:13:46/08:01:15, 6.93i/s]55605/200000 [02:13:48/08:01:15, 6.93i/s]55613/200000 [02:13:49/08:01:16, 6.93i/s]55624/200000 [02:13:51/08:01:15, 6.93i/s]55633/200000 [02:13:52/08:01:14, 6.93i/s]55653/200000 [02:13:54/08:01:13, 6.93i/s]55692/200000 [02:13:58/08:01:04, 6.93i/s]55713/200000 [02:14:00/08:01:01, 6.93i/s]55721/200000 [02:14:01/08:01:01, 6.93i/s]55730/200000 [02:14:02/08:01:01, 6.93i/s]55742/200000 [02:14:03/08:00:59, 6.93i/s]55760/200000 [02:14:05/08:00:57, 6.93i/s]55795/200000 [02:14:10/08:00:56, 6.93i/s]┫ 55812/200000 [02:14:12/08:00:55, 6.93i/s]55863/200000 [02:14:18/08:00:49, 6.93i/s]55869/200000 [02:14:19/08:00:50, 6.93i/s]┫ 55877/200000 [02:14:20/08:00:50, 6.93i/s]55904/200000 [02:14:24/08:00:49, 6.93i/s]55913/200000 [02:14:25/08:00:48, 6.93i/s]55952/200000 [02:14:31/08:00:48, 6.93i/s]55960/200000 [02:14:32/08:00:48, 6.93i/s]55967/200000 [02:14:33/08:00:48, 6.93i/s]55977/200000 [02:14:34/08:00:48, 6.93i/s]56015/200000 [02:16:03/08:05:47, 6.86i/s]56028/200000 [02:16:06/08:05:50, 6.86i/s]56044/200000 [02:16:09/08:05:50, 6.86i/s]56057/200000 [02:16:12/08:05:57, 6.86i/s]56151/200000 [02:16:27/08:06:02, 6.86i/s]56156/200000 [02:16:28/08:06:03, 6.86i/s]56185/200000 [02:16:34/08:06:07, 6.86i/s]┫ 56210/200000 [02:16:38/08:06:11, 6.86i/s]56230/200000 [02:16:42/08:06:14, 6.86i/s]56242/200000 [02:16:45/08:06:17, 6.85i/s]56249/200000 [02:16:46/08:06:18, 6.85i/s]56256/200000 [02:16:47/08:06:19, 6.85i/s]56271/200000 [02:16:51/08:06:25, 6.85i/s]56277/200000 [02:16:53/08:06:26, 6.85i/s]56286/200000 [02:16:55/08:06:31, 6.85i/s]┫ 56291/200000 [02:16:56/08:06:33, 6.85i/s]56298/200000 [02:16:58/08:06:34, 6.85i/s]56303/200000 [02:16:59/08:06:36, 6.85i/s]         ┫ 56316/200000 [02:17:03/08:06:43, 6.85i/s]56320/200000 [02:17:04/08:06:45, 6.85i/s]56324/200000 [02:17:06/08:06:48, 6.85i/s]56347/200000 [02:17:09/08:06:48, 6.85i/s]56376/200000 [02:17:12/08:06:44, 6.85i/s]┫ 56384/200000 [02:17:13/08:06:45, 6.85i/s]3.42e-01  28.20%┣███▋         ┫ 56397/200000 [02:17:16/08:06:47, 6.85i/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Excessive output truncated after 524289 bytes."
     ]
    }
   ],
   "source": [
    "r11 = trainresults(\"myfile1.jld2\",mymodel2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "converge(itr; alpha=0.1)\n",
       "\\end{verbatim}\n",
       "Return an iterator which acts exactly like \\texttt{itr}, but quits when values from \\texttt{itr} stop decreasing. \\texttt{itr} should produce numeric values.\n",
       "\n",
       "It can be used to train a model with the data cycled:\n",
       "\n",
       "\\begin{verbatim}\n",
       "progress!(converge(minimize(model,cycle(data))))\n",
       "\\end{verbatim}\n",
       "\\texttt{alpha} controls the exponential average of values to detect convergence. Here is how convergence is decided:\n",
       "\n",
       "\\begin{verbatim}\n",
       "p = x - avgx\n",
       "avgx = c.alpha * x + (1-c.alpha) * avgx\n",
       "avgp = c.alpha * p + (1-c.alpha) * avgp\n",
       "avgp > 0.0 && return nothing\n",
       "\\end{verbatim}\n",
       "\\texttt{converge!(...)} is equivalent to \\texttt{(for x in converge(...) end)}, i.e.  iterates over the object created by \\texttt{converge(...)} and returns \\texttt{nothing}.\n",
       "\n"
      ],
      "text/markdown": [
       "```\n",
       "converge(itr; alpha=0.1)\n",
       "```\n",
       "\n",
       "Return an iterator which acts exactly like `itr`, but quits when values from `itr` stop decreasing. `itr` should produce numeric values.\n",
       "\n",
       "It can be used to train a model with the data cycled:\n",
       "\n",
       "```\n",
       "progress!(converge(minimize(model,cycle(data))))\n",
       "```\n",
       "\n",
       "`alpha` controls the exponential average of values to detect convergence. Here is how convergence is decided:\n",
       "\n",
       "```\n",
       "p = x - avgx\n",
       "avgx = c.alpha * x + (1-c.alpha) * avgx\n",
       "avgp = c.alpha * p + (1-c.alpha) * avgp\n",
       "avgp > 0.0 && return nothing\n",
       "```\n",
       "\n",
       "`converge!(...)` is equivalent to `(for x in converge(...) end)`, i.e.  iterates over the object created by `converge(...)` and returns `nothing`.\n"
      ],
      "text/plain": [
       "\u001b[36m  converge(itr; alpha=0.1)\u001b[39m\n",
       "\n",
       "  Return an iterator which acts exactly like \u001b[36mitr\u001b[39m, but quits when\n",
       "  values from \u001b[36mitr\u001b[39m stop decreasing. \u001b[36mitr\u001b[39m should produce numeric values.\n",
       "\n",
       "  It can be used to train a model with the data cycled:\n",
       "\n",
       "\u001b[36m  progress!(converge(minimize(model,cycle(data))))\u001b[39m\n",
       "\n",
       "  \u001b[36malpha\u001b[39m controls the exponential average of values to detect\n",
       "  convergence. Here is how convergence is decided:\n",
       "\n",
       "\u001b[36m  p = x - avgx\u001b[39m\n",
       "\u001b[36m  avgx = c.alpha * x + (1-c.alpha) * avgx\u001b[39m\n",
       "\u001b[36m  avgp = c.alpha * p + (1-c.alpha) * avgp\u001b[39m\n",
       "\u001b[36m  avgp > 0.0 && return nothing\u001b[39m\n",
       "\n",
       "  \u001b[36mconverge!(...)\u001b[39m is equivalent to \u001b[36m(for x in converge(...) end)\u001b[39m, i.e.\n",
       "  iterates over the object created by \u001b[36mconverge(...)\u001b[39m and returns\n",
       "  \u001b[36mnothing\u001b[39m."
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@doc converge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "every(n,itr) = (x for (i,x) in enumerate(itr) if i%n == 0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29-element Array{Int64,1}:\n",
       "  1\n",
       "  2\n",
       "  2\n",
       "  2\n",
       "  7\n",
       "  8\n",
       "  2\n",
       "  8\n",
       "  9\n",
       "  9\n",
       "  9\n",
       "  9\n",
       "  9\n",
       "  ⋮\n",
       "  8\n",
       " 22\n",
       " 22\n",
       " 19\n",
       " 24\n",
       " 22\n",
       " 22\n",
       " 29\n",
       " 29\n",
       " 29\n",
       " 22\n",
       "  2"
      ]
     },
     "execution_count": 606,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterate(data5)[1][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Base.Generator{Base.Iterators.Filter{getfield(Main, Symbol(\"##52#54\")){Int64},Base.Iterators.Enumerate{UnitRange{Int64}}},getfield(Main, Symbol(\"##51#53\"))}(getfield(Main, Symbol(\"##51#53\"))(), Base.Iterators.Filter{getfield(Main, Symbol(\"##52#54\")){Int64},Base.Iterators.Enumerate{UnitRange{Int64}}}(getfield(Main, Symbol(\"##52#54\")){Int64}(10), Base.Iterators.Enumerate{UnitRange{Int64}}(1:100)))"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abc = every(10, 1:100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "rnn = RNN(inputSize, hiddenSize; opts...)\n",
       "rnn(x; batchSizes) => y\n",
       "rnn.h, rnn.c  # hidden and cell states\n",
       "\\end{verbatim}\n",
       "\\texttt{RNN} returns a callable RNN object \\texttt{rnn}. Given a minibatch of sequences \\texttt{x}, \\texttt{rnn(x)} returns \\texttt{y}, the hidden states of the final layer for each time step. \\texttt{rnn.h} and \\texttt{rnn.c} fields can be used to set the initial hidden states and read the final hidden states of all layers.  Note that the final time step of \\texttt{y} always contains the final hidden state of the last layer, equivalent to \\texttt{rnn.h} for a single layer network.\n",
       "\n",
       "\\textbf{Dimensions:} The input \\texttt{x} can be 1, 2, or 3 dimensional and \\texttt{y} will have the same number of dimensions as \\texttt{x}. size(x)=(X,[B,T]) and size(y)=(H/2H,[B,T]) where X is inputSize, B is batchSize, T is seqLength, H is hiddenSize, 2H is for bidirectional RNNs. By default a 1-D \\texttt{x} represents a single instance for a single time step, a 2-D \\texttt{x} represents a single minibatch for a single time step, and a 3-D \\texttt{x} represents a sequence of identically sized minibatches for multiple time steps. The output \\texttt{y} gives the hidden state (of the final layer for multi-layer RNNs) for each time step. The fields \\texttt{rnn.h} and \\texttt{rnn.c} represent the hidden states of all layers in a single time step and have size (H,B,L/2L) where L is numLayers and 2L is for bidirectional RNNs.\n",
       "\n",
       "\\textbf{batchSizes:} If \\texttt{batchSizes=nothing} (default), all sequences in a minibatch are assumed to be the same length. If \\texttt{batchSizes} is an array of (non-increasing) integers, it gives us the batch size for each time step (allowing different sequences in the minibatch to have different lengths). In this case \\texttt{x} will typically be 2-D with the second dimension representing variable size batches for time steps. If \\texttt{batchSizes} is used, \\texttt{sum(batchSizes)} should equal \\texttt{length(x) ÷ size(x,1)}. When the batch size is different in every time step, hidden states will have size (H,B,L/2L) where B is always the size of the first (largest) minibatch.\n",
       "\n",
       "\\textbf{Hidden states:} The hidden and cell states are kept in \\texttt{rnn.h} and \\texttt{rnn.c} fields (the cell state is only used by LSTM). They can be initialized during construction using the \\texttt{h} and \\texttt{c} keyword arguments, or modified later by direct assignment. Valid values are \\texttt{nothing} (default), \\texttt{0}, or an array of the right type and size possibly wrapped in a \\texttt{Param}. If the value is \\texttt{nothing} the initial state is assumed to be zero and the final state is discarded keeping the value \\texttt{nothing}. If the value is \\texttt{0} the initial state is assumed to be zero and \\texttt{0} is replaced by the final state on return. If the value is a valid state, it is used as the initial state and is replaced by the final state on return.\n",
       "\n",
       "In a differentiation context the returned final hidden states will be wrapped in \\texttt{Result} types. This is necessary if the same RNN object is to be called multiple times in a single iteration. Between iterations (i.e. after diff/update) the hidden states need to be unboxed with e.g. \\texttt{rnn.h = value(rnn.h)} to prevent spurious dependencies. This happens automatically during the backward pass for GPU RNNs but needs to be done manually for CPU RNNs. See the \\href{https://github.com/denizyuret/Knet.jl/blob/master/tutorial/80.charlm.ipynb}{CharLM Tutorial} for an example.\n",
       "\n",
       "\\textbf{Keyword arguments for RNN:}\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{h=nothing}: Initial hidden state.\n",
       "\n",
       "\n",
       "\\item \\texttt{c=nothing}: Initial cell state.\n",
       "\n",
       "\n",
       "\\item \\texttt{rnnType=:lstm} Type of RNN: One of :relu, :tanh, :lstm, :gru.\n",
       "\n",
       "\n",
       "\\item \\texttt{numLayers=1}: Number of RNN layers.\n",
       "\n",
       "\n",
       "\\item \\texttt{bidirectional=false}: Create a bidirectional RNN if \\texttt{true}.\n",
       "\n",
       "\n",
       "\\item \\texttt{dropout=0}: Dropout probability. Applied to input and between layers.\n",
       "\n",
       "\n",
       "\\item \\texttt{skipInput=false}: Do not multiply the input with a matrix if \\texttt{true}.\n",
       "\n",
       "\n",
       "\\item \\texttt{dataType=Float32}: Data type to use for weights.\n",
       "\n",
       "\n",
       "\\item \\texttt{algo=0}: Algorithm to use, see CUDNN docs for details.\n",
       "\n",
       "\n",
       "\\item \\texttt{seed=0}: Random number seed for dropout. Uses \\texttt{time()} if 0.\n",
       "\n",
       "\n",
       "\\item \\texttt{winit=xavier}: Weight initialization method for matrices.\n",
       "\n",
       "\n",
       "\\item \\texttt{binit=zeros}: Weight initialization method for bias vectors.\n",
       "\n",
       "\n",
       "\\item \\texttt{usegpu=(gpu()>=0)}: GPU used by default if one exists.\n",
       "\n",
       "\\end{itemize}\n",
       "\\textbf{Formulas:} RNNs compute the output h[t] for a given iteration from the recurrent input h[t-1] and the previous layer input x[t] given matrices W, R and biases bW, bR from the following equations:\n",
       "\n",
       "\\texttt{:relu} and \\texttt{:tanh}: Single gate RNN with activation function f:\n",
       "\n",
       "\\begin{verbatim}\n",
       "h[t] = f(W * x[t] .+ R * h[t-1] .+ bW .+ bR)\n",
       "\\end{verbatim}\n",
       "\\texttt{:gru}: Gated recurrent unit:\n",
       "\n",
       "\\begin{verbatim}\n",
       "i[t] = sigm(Wi * x[t] .+ Ri * h[t-1] .+ bWi .+ bRi) # input gate\n",
       "r[t] = sigm(Wr * x[t] .+ Rr * h[t-1] .+ bWr .+ bRr) # reset gate\n",
       "n[t] = tanh(Wn * x[t] .+ r[t] .* (Rn * h[t-1] .+ bRn) .+ bWn) # new gate\n",
       "h[t] = (1 - i[t]) .* n[t] .+ i[t] .* h[t-1]\n",
       "\\end{verbatim}\n",
       "\\texttt{:lstm}: Long short term memory unit with no peephole connections:\n",
       "\n",
       "\\begin{verbatim}\n",
       "i[t] = sigm(Wi * x[t] .+ Ri * h[t-1] .+ bWi .+ bRi) # input gate\n",
       "f[t] = sigm(Wf * x[t] .+ Rf * h[t-1] .+ bWf .+ bRf) # forget gate\n",
       "o[t] = sigm(Wo * x[t] .+ Ro * h[t-1] .+ bWo .+ bRo) # output gate\n",
       "n[t] = tanh(Wn * x[t] .+ Rn * h[t-1] .+ bWn .+ bRn) # new gate\n",
       "c[t] = f[t] .* c[t-1] .+ i[t] .* n[t]               # cell output\n",
       "h[t] = o[t] .* tanh(c[t])\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "```\n",
       "rnn = RNN(inputSize, hiddenSize; opts...)\n",
       "rnn(x; batchSizes) => y\n",
       "rnn.h, rnn.c  # hidden and cell states\n",
       "```\n",
       "\n",
       "`RNN` returns a callable RNN object `rnn`. Given a minibatch of sequences `x`, `rnn(x)` returns `y`, the hidden states of the final layer for each time step. `rnn.h` and `rnn.c` fields can be used to set the initial hidden states and read the final hidden states of all layers.  Note that the final time step of `y` always contains the final hidden state of the last layer, equivalent to `rnn.h` for a single layer network.\n",
       "\n",
       "**Dimensions:** The input `x` can be 1, 2, or 3 dimensional and `y` will have the same number of dimensions as `x`. size(x)=(X,[B,T]) and size(y)=(H/2H,[B,T]) where X is inputSize, B is batchSize, T is seqLength, H is hiddenSize, 2H is for bidirectional RNNs. By default a 1-D `x` represents a single instance for a single time step, a 2-D `x` represents a single minibatch for a single time step, and a 3-D `x` represents a sequence of identically sized minibatches for multiple time steps. The output `y` gives the hidden state (of the final layer for multi-layer RNNs) for each time step. The fields `rnn.h` and `rnn.c` represent the hidden states of all layers in a single time step and have size (H,B,L/2L) where L is numLayers and 2L is for bidirectional RNNs.\n",
       "\n",
       "**batchSizes:** If `batchSizes=nothing` (default), all sequences in a minibatch are assumed to be the same length. If `batchSizes` is an array of (non-increasing) integers, it gives us the batch size for each time step (allowing different sequences in the minibatch to have different lengths). In this case `x` will typically be 2-D with the second dimension representing variable size batches for time steps. If `batchSizes` is used, `sum(batchSizes)` should equal `length(x) ÷ size(x,1)`. When the batch size is different in every time step, hidden states will have size (H,B,L/2L) where B is always the size of the first (largest) minibatch.\n",
       "\n",
       "**Hidden states:** The hidden and cell states are kept in `rnn.h` and `rnn.c` fields (the cell state is only used by LSTM). They can be initialized during construction using the `h` and `c` keyword arguments, or modified later by direct assignment. Valid values are `nothing` (default), `0`, or an array of the right type and size possibly wrapped in a `Param`. If the value is `nothing` the initial state is assumed to be zero and the final state is discarded keeping the value `nothing`. If the value is `0` the initial state is assumed to be zero and `0` is replaced by the final state on return. If the value is a valid state, it is used as the initial state and is replaced by the final state on return.\n",
       "\n",
       "In a differentiation context the returned final hidden states will be wrapped in `Result` types. This is necessary if the same RNN object is to be called multiple times in a single iteration. Between iterations (i.e. after diff/update) the hidden states need to be unboxed with e.g. `rnn.h = value(rnn.h)` to prevent spurious dependencies. This happens automatically during the backward pass for GPU RNNs but needs to be done manually for CPU RNNs. See the [CharLM Tutorial](https://github.com/denizyuret/Knet.jl/blob/master/tutorial/80.charlm.ipynb) for an example.\n",
       "\n",
       "**Keyword arguments for RNN:**\n",
       "\n",
       "  * `h=nothing`: Initial hidden state.\n",
       "  * `c=nothing`: Initial cell state.\n",
       "  * `rnnType=:lstm` Type of RNN: One of :relu, :tanh, :lstm, :gru.\n",
       "  * `numLayers=1`: Number of RNN layers.\n",
       "  * `bidirectional=false`: Create a bidirectional RNN if `true`.\n",
       "  * `dropout=0`: Dropout probability. Applied to input and between layers.\n",
       "  * `skipInput=false`: Do not multiply the input with a matrix if `true`.\n",
       "  * `dataType=Float32`: Data type to use for weights.\n",
       "  * `algo=0`: Algorithm to use, see CUDNN docs for details.\n",
       "  * `seed=0`: Random number seed for dropout. Uses `time()` if 0.\n",
       "  * `winit=xavier`: Weight initialization method for matrices.\n",
       "  * `binit=zeros`: Weight initialization method for bias vectors.\n",
       "  * `usegpu=(gpu()>=0)`: GPU used by default if one exists.\n",
       "\n",
       "**Formulas:** RNNs compute the output h[t] for a given iteration from the recurrent input h[t-1] and the previous layer input x[t] given matrices W, R and biases bW, bR from the following equations:\n",
       "\n",
       "`:relu` and `:tanh`: Single gate RNN with activation function f:\n",
       "\n",
       "```\n",
       "h[t] = f(W * x[t] .+ R * h[t-1] .+ bW .+ bR)\n",
       "```\n",
       "\n",
       "`:gru`: Gated recurrent unit:\n",
       "\n",
       "```\n",
       "i[t] = sigm(Wi * x[t] .+ Ri * h[t-1] .+ bWi .+ bRi) # input gate\n",
       "r[t] = sigm(Wr * x[t] .+ Rr * h[t-1] .+ bWr .+ bRr) # reset gate\n",
       "n[t] = tanh(Wn * x[t] .+ r[t] .* (Rn * h[t-1] .+ bRn) .+ bWn) # new gate\n",
       "h[t] = (1 - i[t]) .* n[t] .+ i[t] .* h[t-1]\n",
       "```\n",
       "\n",
       "`:lstm`: Long short term memory unit with no peephole connections:\n",
       "\n",
       "```\n",
       "i[t] = sigm(Wi * x[t] .+ Ri * h[t-1] .+ bWi .+ bRi) # input gate\n",
       "f[t] = sigm(Wf * x[t] .+ Rf * h[t-1] .+ bWf .+ bRf) # forget gate\n",
       "o[t] = sigm(Wo * x[t] .+ Ro * h[t-1] .+ bWo .+ bRo) # output gate\n",
       "n[t] = tanh(Wn * x[t] .+ Rn * h[t-1] .+ bWn .+ bRn) # new gate\n",
       "c[t] = f[t] .* c[t-1] .+ i[t] .* n[t]               # cell output\n",
       "h[t] = o[t] .* tanh(c[t])\n",
       "```\n"
      ],
      "text/plain": [
       "\u001b[36m  rnn = RNN(inputSize, hiddenSize; opts...)\u001b[39m\n",
       "\u001b[36m  rnn(x; batchSizes) => y\u001b[39m\n",
       "\u001b[36m  rnn.h, rnn.c  # hidden and cell states\u001b[39m\n",
       "\n",
       "  \u001b[36mRNN\u001b[39m returns a callable RNN object \u001b[36mrnn\u001b[39m. Given a minibatch of\n",
       "  sequences \u001b[36mx\u001b[39m, \u001b[36mrnn(x)\u001b[39m returns \u001b[36my\u001b[39m, the hidden states of the final layer\n",
       "  for each time step. \u001b[36mrnn.h\u001b[39m and \u001b[36mrnn.c\u001b[39m fields can be used to set the\n",
       "  initial hidden states and read the final hidden states of all\n",
       "  layers. Note that the final time step of \u001b[36my\u001b[39m always contains the final\n",
       "  hidden state of the last layer, equivalent to \u001b[36mrnn.h\u001b[39m for a single\n",
       "  layer network.\n",
       "\n",
       "  \u001b[1mDimensions:\u001b[22m The input \u001b[36mx\u001b[39m can be 1, 2, or 3 dimensional and \u001b[36my\u001b[39m will\n",
       "  have the same number of dimensions as \u001b[36mx\u001b[39m. size(x)=(X,[B,T]) and\n",
       "  size(y)=(H/2H,[B,T]) where X is inputSize, B is batchSize, T is\n",
       "  seqLength, H is hiddenSize, 2H is for bidirectional RNNs. By default\n",
       "  a 1-D \u001b[36mx\u001b[39m represents a single instance for a single time step, a 2-D \u001b[36mx\u001b[39m\n",
       "  represents a single minibatch for a single time step, and a 3-D \u001b[36mx\u001b[39m\n",
       "  represents a sequence of identically sized minibatches for multiple\n",
       "  time steps. The output \u001b[36my\u001b[39m gives the hidden state (of the final layer\n",
       "  for multi-layer RNNs) for each time step. The fields \u001b[36mrnn.h\u001b[39m and \u001b[36mrnn.c\u001b[39m\n",
       "  represent the hidden states of all layers in a single time step and\n",
       "  have size (H,B,L/2L) where L is numLayers and 2L is for\n",
       "  bidirectional RNNs.\n",
       "\n",
       "  \u001b[1mbatchSizes:\u001b[22m If \u001b[36mbatchSizes=nothing\u001b[39m (default), all sequences in a\n",
       "  minibatch are assumed to be the same length. If \u001b[36mbatchSizes\u001b[39m is an\n",
       "  array of (non-increasing) integers, it gives us the batch size for\n",
       "  each time step (allowing different sequences in the minibatch to\n",
       "  have different lengths). In this case \u001b[36mx\u001b[39m will typically be 2-D with\n",
       "  the second dimension representing variable size batches for time\n",
       "  steps. If \u001b[36mbatchSizes\u001b[39m is used, \u001b[36msum(batchSizes)\u001b[39m should equal \u001b[36mlength(x)\n",
       "  ÷ size(x,1)\u001b[39m. When the batch size is different in every time step,\n",
       "  hidden states will have size (H,B,L/2L) where B is always the size\n",
       "  of the first (largest) minibatch.\n",
       "\n",
       "  \u001b[1mHidden states:\u001b[22m The hidden and cell states are kept in \u001b[36mrnn.h\u001b[39m and\n",
       "  \u001b[36mrnn.c\u001b[39m fields (the cell state is only used by LSTM). They can be\n",
       "  initialized during construction using the \u001b[36mh\u001b[39m and \u001b[36mc\u001b[39m keyword arguments,\n",
       "  or modified later by direct assignment. Valid values are \u001b[36mnothing\u001b[39m\n",
       "  (default), \u001b[36m0\u001b[39m, or an array of the right type and size possibly\n",
       "  wrapped in a \u001b[36mParam\u001b[39m. If the value is \u001b[36mnothing\u001b[39m the initial state is\n",
       "  assumed to be zero and the final state is discarded keeping the\n",
       "  value \u001b[36mnothing\u001b[39m. If the value is \u001b[36m0\u001b[39m the initial state is assumed to be\n",
       "  zero and \u001b[36m0\u001b[39m is replaced by the final state on return. If the value is\n",
       "  a valid state, it is used as the initial state and is replaced by\n",
       "  the final state on return.\n",
       "\n",
       "  In a differentiation context the returned final hidden states will\n",
       "  be wrapped in \u001b[36mResult\u001b[39m types. This is necessary if the same RNN object\n",
       "  is to be called multiple times in a single iteration. Between\n",
       "  iterations (i.e. after diff/update) the hidden states need to be\n",
       "  unboxed with e.g. \u001b[36mrnn.h = value(rnn.h)\u001b[39m to prevent spurious\n",
       "  dependencies. This happens automatically during the backward pass\n",
       "  for GPU RNNs but needs to be done manually for CPU RNNs. See the\n",
       "  CharLM Tutorial\n",
       "  (https://github.com/denizyuret/Knet.jl/blob/master/tutorial/80.charlm.ipynb)\n",
       "  for an example.\n",
       "\n",
       "  \u001b[1mKeyword arguments for RNN:\u001b[22m\n",
       "\n",
       "    •    \u001b[36mh=nothing\u001b[39m: Initial hidden state.\n",
       "\n",
       "    •    \u001b[36mc=nothing\u001b[39m: Initial cell state.\n",
       "\n",
       "    •    \u001b[36mrnnType=:lstm\u001b[39m Type of RNN: One of :relu, :tanh, :lstm,\n",
       "        :gru.\n",
       "\n",
       "    •    \u001b[36mnumLayers=1\u001b[39m: Number of RNN layers.\n",
       "\n",
       "    •    \u001b[36mbidirectional=false\u001b[39m: Create a bidirectional RNN if \u001b[36mtrue\u001b[39m.\n",
       "\n",
       "    •    \u001b[36mdropout=0\u001b[39m: Dropout probability. Applied to input and\n",
       "        between layers.\n",
       "\n",
       "    •    \u001b[36mskipInput=false\u001b[39m: Do not multiply the input with a matrix\n",
       "        if \u001b[36mtrue\u001b[39m.\n",
       "\n",
       "    •    \u001b[36mdataType=Float32\u001b[39m: Data type to use for weights.\n",
       "\n",
       "    •    \u001b[36malgo=0\u001b[39m: Algorithm to use, see CUDNN docs for details.\n",
       "\n",
       "    •    \u001b[36mseed=0\u001b[39m: Random number seed for dropout. Uses \u001b[36mtime()\u001b[39m if 0.\n",
       "\n",
       "    •    \u001b[36mwinit=xavier\u001b[39m: Weight initialization method for matrices.\n",
       "\n",
       "    •    \u001b[36mbinit=zeros\u001b[39m: Weight initialization method for bias\n",
       "        vectors.\n",
       "\n",
       "    •    \u001b[36musegpu=(gpu()>=0)\u001b[39m: GPU used by default if one exists.\n",
       "\n",
       "  \u001b[1mFormulas:\u001b[22m RNNs compute the output h[t] for a given iteration from\n",
       "  the recurrent input h[t-1] and the previous layer input x[t] given\n",
       "  matrices W, R and biases bW, bR from the following equations:\n",
       "\n",
       "  \u001b[36m:relu\u001b[39m and \u001b[36m:tanh\u001b[39m: Single gate RNN with activation function f:\n",
       "\n",
       "\u001b[36m  h[t] = f(W * x[t] .+ R * h[t-1] .+ bW .+ bR)\u001b[39m\n",
       "\n",
       "  \u001b[36m:gru\u001b[39m: Gated recurrent unit:\n",
       "\n",
       "\u001b[36m  i[t] = sigm(Wi * x[t] .+ Ri * h[t-1] .+ bWi .+ bRi) # input gate\u001b[39m\n",
       "\u001b[36m  r[t] = sigm(Wr * x[t] .+ Rr * h[t-1] .+ bWr .+ bRr) # reset gate\u001b[39m\n",
       "\u001b[36m  n[t] = tanh(Wn * x[t] .+ r[t] .* (Rn * h[t-1] .+ bRn) .+ bWn) # new gate\u001b[39m\n",
       "\u001b[36m  h[t] = (1 - i[t]) .* n[t] .+ i[t] .* h[t-1]\u001b[39m\n",
       "\n",
       "  \u001b[36m:lstm\u001b[39m: Long short term memory unit with no peephole connections:\n",
       "\n",
       "\u001b[36m  i[t] = sigm(Wi * x[t] .+ Ri * h[t-1] .+ bWi .+ bRi) # input gate\u001b[39m\n",
       "\u001b[36m  f[t] = sigm(Wf * x[t] .+ Rf * h[t-1] .+ bWf .+ bRf) # forget gate\u001b[39m\n",
       "\u001b[36m  o[t] = sigm(Wo * x[t] .+ Ro * h[t-1] .+ bWo .+ bRo) # output gate\u001b[39m\n",
       "\u001b[36m  n[t] = tanh(Wn * x[t] .+ Rn * h[t-1] .+ bWn .+ bRn) # new gate\u001b[39m\n",
       "\u001b[36m  c[t] = f[t] .* c[t-1] .+ i[t] .* n[t]               # cell output\u001b[39m\n",
       "\u001b[36m  h[t] = o[t] .* tanh(c[t])\u001b[39m"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@doc Knet.RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "view(A, inds...)\n",
       "\\end{verbatim}\n",
       "Like \\href{@ref}{\\texttt{getindex}}, but returns a view into the parent array \\texttt{A} with the given indices instead of making a copy.  Calling \\href{@ref}{\\texttt{getindex}} or \\href{@ref}{\\texttt{setindex!}} on the returned \\texttt{SubArray} computes the indices to the parent array on the fly without checking bounds.\n",
       "\n",
       "\\section{Examples}\n",
       "\\begin{verbatim}\n",
       "julia> A = [1 2; 3 4]\n",
       "2×2 Array{Int64,2}:\n",
       " 1  2\n",
       " 3  4\n",
       "\n",
       "julia> b = view(A, :, 1)\n",
       "2-element view(::Array{Int64,2}, :, 1) with eltype Int64:\n",
       " 1\n",
       " 3\n",
       "\n",
       "julia> fill!(b, 0)\n",
       "2-element view(::Array{Int64,2}, :, 1) with eltype Int64:\n",
       " 0\n",
       " 0\n",
       "\n",
       "julia> A # Note A has changed even though we modified b\n",
       "2×2 Array{Int64,2}:\n",
       " 0  2\n",
       " 0  4\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "```\n",
       "view(A, inds...)\n",
       "```\n",
       "\n",
       "Like [`getindex`](@ref), but returns a view into the parent array `A` with the given indices instead of making a copy.  Calling [`getindex`](@ref) or [`setindex!`](@ref) on the returned `SubArray` computes the indices to the parent array on the fly without checking bounds.\n",
       "\n",
       "# Examples\n",
       "\n",
       "```jldoctest\n",
       "julia> A = [1 2; 3 4]\n",
       "2×2 Array{Int64,2}:\n",
       " 1  2\n",
       " 3  4\n",
       "\n",
       "julia> b = view(A, :, 1)\n",
       "2-element view(::Array{Int64,2}, :, 1) with eltype Int64:\n",
       " 1\n",
       " 3\n",
       "\n",
       "julia> fill!(b, 0)\n",
       "2-element view(::Array{Int64,2}, :, 1) with eltype Int64:\n",
       " 0\n",
       " 0\n",
       "\n",
       "julia> A # Note A has changed even though we modified b\n",
       "2×2 Array{Int64,2}:\n",
       " 0  2\n",
       " 0  4\n",
       "```\n"
      ],
      "text/plain": [
       "\u001b[36m  view(A, inds...)\u001b[39m\n",
       "\n",
       "  Like \u001b[36mgetindex\u001b[39m, but returns a view into the parent array \u001b[36mA\u001b[39m with the\n",
       "  given indices instead of making a copy. Calling \u001b[36mgetindex\u001b[39m or\n",
       "  \u001b[36msetindex!\u001b[39m on the returned \u001b[36mSubArray\u001b[39m computes the indices to the\n",
       "  parent array on the fly without checking bounds.\n",
       "\n",
       "\u001b[1m  Examples\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "\u001b[36m  julia> A = [1 2; 3 4]\u001b[39m\n",
       "\u001b[36m  2×2 Array{Int64,2}:\u001b[39m\n",
       "\u001b[36m   1  2\u001b[39m\n",
       "\u001b[36m   3  4\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> b = view(A, :, 1)\u001b[39m\n",
       "\u001b[36m  2-element view(::Array{Int64,2}, :, 1) with eltype Int64:\u001b[39m\n",
       "\u001b[36m   1\u001b[39m\n",
       "\u001b[36m   3\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> fill!(b, 0)\u001b[39m\n",
       "\u001b[36m  2-element view(::Array{Int64,2}, :, 1) with eltype Int64:\u001b[39m\n",
       "\u001b[36m   0\u001b[39m\n",
       "\u001b[36m   0\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> A # Note A has changed even though we modified b\u001b[39m\n",
       "\u001b[36m  2×2 Array{Int64,2}:\u001b[39m\n",
       "\u001b[36m   0  2\u001b[39m\n",
       "\u001b[36m   0  4\u001b[39m"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@doc view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "getindex(type[, elements...])\n",
       "\\end{verbatim}\n",
       "Construct a 1-d array of the specified type. This is usually called with the syntax \\texttt{Type[]}. Element values can be specified using \\texttt{Type[a,b,c,...]}.\n",
       "\n",
       "\\section{Examples}\n",
       "\\begin{verbatim}\n",
       "julia> Int8[1, 2, 3]\n",
       "3-element Array{Int8,1}:\n",
       " 1\n",
       " 2\n",
       " 3\n",
       "\n",
       "julia> getindex(Int8, 1, 2, 3)\n",
       "3-element Array{Int8,1}:\n",
       " 1\n",
       " 2\n",
       " 3\n",
       "\\end{verbatim}\n",
       "\\begin{verbatim}\n",
       "getindex(collection, key...)\n",
       "\\end{verbatim}\n",
       "Retrieve the value(s) stored at the given key or index within a collection. The syntax \\texttt{a[i,j,...]} is converted by the compiler to \\texttt{getindex(a, i, j, ...)}.\n",
       "\n",
       "\\section{Examples}\n",
       "\\begin{verbatim}\n",
       "julia> A = Dict(\"a\" => 1, \"b\" => 2)\n",
       "Dict{String,Int64} with 2 entries:\n",
       "  \"b\" => 2\n",
       "  \"a\" => 1\n",
       "\n",
       "julia> getindex(A, \"a\")\n",
       "1\n",
       "\\end{verbatim}\n",
       "\\begin{verbatim}\n",
       "getindex(A, inds...)\n",
       "\\end{verbatim}\n",
       "Return a subset of array \\texttt{A} as specified by \\texttt{inds}, where each \\texttt{ind} may be an \\texttt{Int}, an \\href{@ref}{\\texttt{AbstractRange}}, or a \\href{@ref}{\\texttt{Vector}}. See the manual section on \\href{@ref man-array-indexing}{array indexing} for details.\n",
       "\n",
       "\\section{Examples}\n",
       "\\begin{verbatim}\n",
       "julia> A = [1 2; 3 4]\n",
       "2×2 Array{Int64,2}:\n",
       " 1  2\n",
       " 3  4\n",
       "\n",
       "julia> getindex(A, 1)\n",
       "1\n",
       "\n",
       "julia> getindex(A, [2, 1])\n",
       "2-element Array{Int64,1}:\n",
       " 3\n",
       " 1\n",
       "\n",
       "julia> getindex(A, 2:4)\n",
       "3-element Array{Int64,1}:\n",
       " 3\n",
       " 2\n",
       " 4\n",
       "\\end{verbatim}\n",
       "\\begin{verbatim}\n",
       "getindex(tree::GitTree, target::AbstractString) -> GitObject\n",
       "\\end{verbatim}\n",
       "Look up \\texttt{target} path in the \\texttt{tree}, returning a \\href{@ref}{\\texttt{GitObject}} (a \\href{@ref}{\\texttt{GitBlob}} in the case of a file, or another \\href{@ref}{\\texttt{GitTree}} if looking up a directory).\n",
       "\n",
       "\\section{Examples}\n",
       "\\begin{verbatim}\n",
       "tree = LibGit2.GitTree(repo, \"HEAD^{tree}\")\n",
       "readme = tree[\"README.md\"]\n",
       "subtree = tree[\"test\"]\n",
       "runtests = subtree[\"runtests.jl\"]\n",
       "\\end{verbatim}\n",
       "\\begin{verbatim}\n",
       "v = sd[k]\n",
       "\\end{verbatim}\n",
       "Argument \\texttt{sd} is a SortedDict and \\texttt{k} is a key. In an expression, this retrieves the value (\\texttt{v}) associated with the key (or \\texttt{KeyError} if none). On the left-hand side of an assignment, this assigns or reassigns the value associated with the key. (For assigning and reassigning, see also \\texttt{insert!} below.) Time: O(\\emph{c} log \\emph{n})\n",
       "\n",
       "\\begin{verbatim}\n",
       "cb[i]\n",
       "\\end{verbatim}\n",
       "Get the i-th element of CircularBuffer.\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{cb[1]} to get the element at the front\n",
       "\n",
       "\n",
       "\\item \\texttt{cb[end]} to get the element at the back\n",
       "\n",
       "\\end{itemize}\n"
      ],
      "text/markdown": [
       "```\n",
       "getindex(type[, elements...])\n",
       "```\n",
       "\n",
       "Construct a 1-d array of the specified type. This is usually called with the syntax `Type[]`. Element values can be specified using `Type[a,b,c,...]`.\n",
       "\n",
       "# Examples\n",
       "\n",
       "```jldoctest\n",
       "julia> Int8[1, 2, 3]\n",
       "3-element Array{Int8,1}:\n",
       " 1\n",
       " 2\n",
       " 3\n",
       "\n",
       "julia> getindex(Int8, 1, 2, 3)\n",
       "3-element Array{Int8,1}:\n",
       " 1\n",
       " 2\n",
       " 3\n",
       "```\n",
       "\n",
       "```\n",
       "getindex(collection, key...)\n",
       "```\n",
       "\n",
       "Retrieve the value(s) stored at the given key or index within a collection. The syntax `a[i,j,...]` is converted by the compiler to `getindex(a, i, j, ...)`.\n",
       "\n",
       "# Examples\n",
       "\n",
       "```jldoctest\n",
       "julia> A = Dict(\"a\" => 1, \"b\" => 2)\n",
       "Dict{String,Int64} with 2 entries:\n",
       "  \"b\" => 2\n",
       "  \"a\" => 1\n",
       "\n",
       "julia> getindex(A, \"a\")\n",
       "1\n",
       "```\n",
       "\n",
       "```\n",
       "getindex(A, inds...)\n",
       "```\n",
       "\n",
       "Return a subset of array `A` as specified by `inds`, where each `ind` may be an `Int`, an [`AbstractRange`](@ref), or a [`Vector`](@ref). See the manual section on [array indexing](@ref man-array-indexing) for details.\n",
       "\n",
       "# Examples\n",
       "\n",
       "```jldoctest\n",
       "julia> A = [1 2; 3 4]\n",
       "2×2 Array{Int64,2}:\n",
       " 1  2\n",
       " 3  4\n",
       "\n",
       "julia> getindex(A, 1)\n",
       "1\n",
       "\n",
       "julia> getindex(A, [2, 1])\n",
       "2-element Array{Int64,1}:\n",
       " 3\n",
       " 1\n",
       "\n",
       "julia> getindex(A, 2:4)\n",
       "3-element Array{Int64,1}:\n",
       " 3\n",
       " 2\n",
       " 4\n",
       "```\n",
       "\n",
       "```\n",
       "getindex(tree::GitTree, target::AbstractString) -> GitObject\n",
       "```\n",
       "\n",
       "Look up `target` path in the `tree`, returning a [`GitObject`](@ref) (a [`GitBlob`](@ref) in the case of a file, or another [`GitTree`](@ref) if looking up a directory).\n",
       "\n",
       "# Examples\n",
       "\n",
       "```julia\n",
       "tree = LibGit2.GitTree(repo, \"HEAD^{tree}\")\n",
       "readme = tree[\"README.md\"]\n",
       "subtree = tree[\"test\"]\n",
       "runtests = subtree[\"runtests.jl\"]\n",
       "```\n",
       "\n",
       "```\n",
       "v = sd[k]\n",
       "```\n",
       "\n",
       "Argument `sd` is a SortedDict and `k` is a key. In an expression, this retrieves the value (`v`) associated with the key (or `KeyError` if none). On the left-hand side of an assignment, this assigns or reassigns the value associated with the key. (For assigning and reassigning, see also `insert!` below.) Time: O(*c* log *n*)\n",
       "\n",
       "```\n",
       "cb[i]\n",
       "```\n",
       "\n",
       "Get the i-th element of CircularBuffer.\n",
       "\n",
       "  * `cb[1]` to get the element at the front\n",
       "  * `cb[end]` to get the element at the back\n"
      ],
      "text/plain": [
       "\u001b[36m  getindex(type[, elements...])\u001b[39m\n",
       "\n",
       "  Construct a 1-d array of the specified type. This is usually called\n",
       "  with the syntax \u001b[36mType[]\u001b[39m. Element values can be specified using\n",
       "  \u001b[36mType[a,b,c,...]\u001b[39m.\n",
       "\n",
       "\u001b[1m  Examples\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "\u001b[36m  julia> Int8[1, 2, 3]\u001b[39m\n",
       "\u001b[36m  3-element Array{Int8,1}:\u001b[39m\n",
       "\u001b[36m   1\u001b[39m\n",
       "\u001b[36m   2\u001b[39m\n",
       "\u001b[36m   3\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> getindex(Int8, 1, 2, 3)\u001b[39m\n",
       "\u001b[36m  3-element Array{Int8,1}:\u001b[39m\n",
       "\u001b[36m   1\u001b[39m\n",
       "\u001b[36m   2\u001b[39m\n",
       "\u001b[36m   3\u001b[39m\n",
       "\n",
       "\u001b[36m  getindex(collection, key...)\u001b[39m\n",
       "\n",
       "  Retrieve the value(s) stored at the given key or index within a\n",
       "  collection. The syntax \u001b[36ma[i,j,...]\u001b[39m is converted by the compiler to\n",
       "  \u001b[36mgetindex(a, i, j, ...)\u001b[39m.\n",
       "\n",
       "\u001b[1m  Examples\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "\u001b[36m  julia> A = Dict(\"a\" => 1, \"b\" => 2)\u001b[39m\n",
       "\u001b[36m  Dict{String,Int64} with 2 entries:\u001b[39m\n",
       "\u001b[36m    \"b\" => 2\u001b[39m\n",
       "\u001b[36m    \"a\" => 1\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> getindex(A, \"a\")\u001b[39m\n",
       "\u001b[36m  1\u001b[39m\n",
       "\n",
       "\u001b[36m  getindex(A, inds...)\u001b[39m\n",
       "\n",
       "  Return a subset of array \u001b[36mA\u001b[39m as specified by \u001b[36minds\u001b[39m, where each \u001b[36mind\u001b[39m may\n",
       "  be an \u001b[36mInt\u001b[39m, an \u001b[36mAbstractRange\u001b[39m, or a \u001b[36mVector\u001b[39m. See the manual section on\n",
       "  array indexing for details.\n",
       "\n",
       "\u001b[1m  Examples\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "\u001b[36m  julia> A = [1 2; 3 4]\u001b[39m\n",
       "\u001b[36m  2×2 Array{Int64,2}:\u001b[39m\n",
       "\u001b[36m   1  2\u001b[39m\n",
       "\u001b[36m   3  4\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> getindex(A, 1)\u001b[39m\n",
       "\u001b[36m  1\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> getindex(A, [2, 1])\u001b[39m\n",
       "\u001b[36m  2-element Array{Int64,1}:\u001b[39m\n",
       "\u001b[36m   3\u001b[39m\n",
       "\u001b[36m   1\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> getindex(A, 2:4)\u001b[39m\n",
       "\u001b[36m  3-element Array{Int64,1}:\u001b[39m\n",
       "\u001b[36m   3\u001b[39m\n",
       "\u001b[36m   2\u001b[39m\n",
       "\u001b[36m   4\u001b[39m\n",
       "\n",
       "\u001b[36m  getindex(tree::GitTree, target::AbstractString) -> GitObject\u001b[39m\n",
       "\n",
       "  Look up \u001b[36mtarget\u001b[39m path in the \u001b[36mtree\u001b[39m, returning a \u001b[36mGitObject\u001b[39m (a \u001b[36mGitBlob\u001b[39m in\n",
       "  the case of a file, or another \u001b[36mGitTree\u001b[39m if looking up a directory).\n",
       "\n",
       "\u001b[1m  Examples\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "\u001b[36m  tree = LibGit2.GitTree(repo, \"HEAD^{tree}\")\u001b[39m\n",
       "\u001b[36m  readme = tree[\"README.md\"]\u001b[39m\n",
       "\u001b[36m  subtree = tree[\"test\"]\u001b[39m\n",
       "\u001b[36m  runtests = subtree[\"runtests.jl\"]\u001b[39m\n",
       "\n",
       "\u001b[36m  v = sd[k]\u001b[39m\n",
       "\n",
       "  Argument \u001b[36msd\u001b[39m is a SortedDict and \u001b[36mk\u001b[39m is a key. In an expression, this\n",
       "  retrieves the value (\u001b[36mv\u001b[39m) associated with the key (or \u001b[36mKeyError\u001b[39m if\n",
       "  none). On the left-hand side of an assignment, this assigns or\n",
       "  reassigns the value associated with the key. (For assigning and\n",
       "  reassigning, see also \u001b[36minsert!\u001b[39m below.) Time: O(\u001b[4mc\u001b[24m log \u001b[4mn\u001b[24m)\n",
       "\n",
       "\u001b[36m  cb[i]\u001b[39m\n",
       "\n",
       "  Get the i-th element of CircularBuffer.\n",
       "\n",
       "    •    \u001b[36mcb[1]\u001b[39m to get the element at the front\n",
       "\n",
       "    •    \u001b[36mcb[end]\u001b[39m to get the element at the back"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@doc getindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "sort!(v; alg::Algorithm=defalg(v), lt=isless, by=identity, rev::Bool=false, order::Ordering=Forward)\n",
       "\\end{verbatim}\n",
       "Sort the vector \\texttt{v} in place. \\texttt{QuickSort} is used by default for numeric arrays while \\texttt{MergeSort} is used for other arrays. You can specify an algorithm to use via the \\texttt{alg} keyword (see Sorting Algorithms for available algorithms). The \\texttt{by} keyword lets you provide a function that will be applied to each element before comparison; the \\texttt{lt} keyword allows providing a custom \"less than\" function; use \\texttt{rev=true} to reverse the sorting order. These options are independent and can be used together in all possible combinations: if both \\texttt{by} and \\texttt{lt} are specified, the \\texttt{lt} function is applied to the result of the \\texttt{by} function; \\texttt{rev=true} reverses whatever ordering specified via the \\texttt{by} and \\texttt{lt} keywords.\n",
       "\n",
       "\\section{Examples}\n",
       "\\begin{verbatim}\n",
       "julia> v = [3, 1, 2]; sort!(v); v\n",
       "3-element Array{Int64,1}:\n",
       " 1\n",
       " 2\n",
       " 3\n",
       "\n",
       "julia> v = [3, 1, 2]; sort!(v, rev = true); v\n",
       "3-element Array{Int64,1}:\n",
       " 3\n",
       " 2\n",
       " 1\n",
       "\n",
       "julia> v = [(1, \"c\"), (3, \"a\"), (2, \"b\")]; sort!(v, by = x -> x[1]); v\n",
       "3-element Array{Tuple{Int64,String},1}:\n",
       " (1, \"c\")\n",
       " (2, \"b\")\n",
       " (3, \"a\")\n",
       "\n",
       "julia> v = [(1, \"c\"), (3, \"a\"), (2, \"b\")]; sort!(v, by = x -> x[2]); v\n",
       "3-element Array{Tuple{Int64,String},1}:\n",
       " (3, \"a\")\n",
       " (2, \"b\")\n",
       " (1, \"c\")\n",
       "\\end{verbatim}\n",
       "\\begin{verbatim}\n",
       "sort!(A; dims::Integer, alg::Algorithm=defalg(v), lt=isless, by=identity, rev::Bool=false, order::Ordering=Forward)\n",
       "\\end{verbatim}\n",
       "Sort the multidimensional array \\texttt{A} along dimension \\texttt{dims}. See \\href{@ref}{\\texttt{sort!}} for a description of possible keyword arguments.\n",
       "\n",
       "To sort slices of an array, refer to \\href{@ref}{\\texttt{sortslices}}.\n",
       "\n",
       "\\begin{quote}\n",
       "\\textbf{compat}\n",
       "\n",
       "Julia 1.1\n",
       "\n",
       "This function requires at least Julia 1.1.\n",
       "\n",
       "\\end{quote}\n",
       "\\section{Examples}\n",
       "\\begin{verbatim}\n",
       "julia> A = [4 3; 1 2]\n",
       "2×2 Array{Int64,2}:\n",
       " 4  3\n",
       " 1  2\n",
       "\n",
       "julia> sort!(A, dims = 1); A\n",
       "2×2 Array{Int64,2}:\n",
       " 1  2\n",
       " 4  3\n",
       "\n",
       "julia> sort!(A, dims = 2); A\n",
       "2×2 Array{Int64,2}:\n",
       " 1  2\n",
       " 3  4\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "```\n",
       "sort!(v; alg::Algorithm=defalg(v), lt=isless, by=identity, rev::Bool=false, order::Ordering=Forward)\n",
       "```\n",
       "\n",
       "Sort the vector `v` in place. `QuickSort` is used by default for numeric arrays while `MergeSort` is used for other arrays. You can specify an algorithm to use via the `alg` keyword (see Sorting Algorithms for available algorithms). The `by` keyword lets you provide a function that will be applied to each element before comparison; the `lt` keyword allows providing a custom \"less than\" function; use `rev=true` to reverse the sorting order. These options are independent and can be used together in all possible combinations: if both `by` and `lt` are specified, the `lt` function is applied to the result of the `by` function; `rev=true` reverses whatever ordering specified via the `by` and `lt` keywords.\n",
       "\n",
       "# Examples\n",
       "\n",
       "```jldoctest\n",
       "julia> v = [3, 1, 2]; sort!(v); v\n",
       "3-element Array{Int64,1}:\n",
       " 1\n",
       " 2\n",
       " 3\n",
       "\n",
       "julia> v = [3, 1, 2]; sort!(v, rev = true); v\n",
       "3-element Array{Int64,1}:\n",
       " 3\n",
       " 2\n",
       " 1\n",
       "\n",
       "julia> v = [(1, \"c\"), (3, \"a\"), (2, \"b\")]; sort!(v, by = x -> x[1]); v\n",
       "3-element Array{Tuple{Int64,String},1}:\n",
       " (1, \"c\")\n",
       " (2, \"b\")\n",
       " (3, \"a\")\n",
       "\n",
       "julia> v = [(1, \"c\"), (3, \"a\"), (2, \"b\")]; sort!(v, by = x -> x[2]); v\n",
       "3-element Array{Tuple{Int64,String},1}:\n",
       " (3, \"a\")\n",
       " (2, \"b\")\n",
       " (1, \"c\")\n",
       "```\n",
       "\n",
       "```\n",
       "sort!(A; dims::Integer, alg::Algorithm=defalg(v), lt=isless, by=identity, rev::Bool=false, order::Ordering=Forward)\n",
       "```\n",
       "\n",
       "Sort the multidimensional array `A` along dimension `dims`. See [`sort!`](@ref) for a description of possible keyword arguments.\n",
       "\n",
       "To sort slices of an array, refer to [`sortslices`](@ref).\n",
       "\n",
       "!!! compat \"Julia 1.1\"\n",
       "    This function requires at least Julia 1.1.\n",
       "\n",
       "\n",
       "# Examples\n",
       "\n",
       "```jldoctest\n",
       "julia> A = [4 3; 1 2]\n",
       "2×2 Array{Int64,2}:\n",
       " 4  3\n",
       " 1  2\n",
       "\n",
       "julia> sort!(A, dims = 1); A\n",
       "2×2 Array{Int64,2}:\n",
       " 1  2\n",
       " 4  3\n",
       "\n",
       "julia> sort!(A, dims = 2); A\n",
       "2×2 Array{Int64,2}:\n",
       " 1  2\n",
       " 3  4\n",
       "```\n"
      ],
      "text/plain": [
       "\u001b[36m  sort!(v; alg::Algorithm=defalg(v), lt=isless, by=identity, rev::Bool=false, order::Ordering=Forward)\u001b[39m\n",
       "\n",
       "  Sort the vector \u001b[36mv\u001b[39m in place. \u001b[36mQuickSort\u001b[39m is used by default for numeric\n",
       "  arrays while \u001b[36mMergeSort\u001b[39m is used for other arrays. You can specify an\n",
       "  algorithm to use via the \u001b[36malg\u001b[39m keyword (see Sorting Algorithms for\n",
       "  available algorithms). The \u001b[36mby\u001b[39m keyword lets you provide a function\n",
       "  that will be applied to each element before comparison; the \u001b[36mlt\u001b[39m\n",
       "  keyword allows providing a custom \"less than\" function; use \u001b[36mrev=true\u001b[39m\n",
       "  to reverse the sorting order. These options are independent and can\n",
       "  be used together in all possible combinations: if both \u001b[36mby\u001b[39m and \u001b[36mlt\u001b[39m are\n",
       "  specified, the \u001b[36mlt\u001b[39m function is applied to the result of the \u001b[36mby\u001b[39m\n",
       "  function; \u001b[36mrev=true\u001b[39m reverses whatever ordering specified via the \u001b[36mby\u001b[39m\n",
       "  and \u001b[36mlt\u001b[39m keywords.\n",
       "\n",
       "\u001b[1m  Examples\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "\u001b[36m  julia> v = [3, 1, 2]; sort!(v); v\u001b[39m\n",
       "\u001b[36m  3-element Array{Int64,1}:\u001b[39m\n",
       "\u001b[36m   1\u001b[39m\n",
       "\u001b[36m   2\u001b[39m\n",
       "\u001b[36m   3\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> v = [3, 1, 2]; sort!(v, rev = true); v\u001b[39m\n",
       "\u001b[36m  3-element Array{Int64,1}:\u001b[39m\n",
       "\u001b[36m   3\u001b[39m\n",
       "\u001b[36m   2\u001b[39m\n",
       "\u001b[36m   1\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> v = [(1, \"c\"), (3, \"a\"), (2, \"b\")]; sort!(v, by = x -> x[1]); v\u001b[39m\n",
       "\u001b[36m  3-element Array{Tuple{Int64,String},1}:\u001b[39m\n",
       "\u001b[36m   (1, \"c\")\u001b[39m\n",
       "\u001b[36m   (2, \"b\")\u001b[39m\n",
       "\u001b[36m   (3, \"a\")\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> v = [(1, \"c\"), (3, \"a\"), (2, \"b\")]; sort!(v, by = x -> x[2]); v\u001b[39m\n",
       "\u001b[36m  3-element Array{Tuple{Int64,String},1}:\u001b[39m\n",
       "\u001b[36m   (3, \"a\")\u001b[39m\n",
       "\u001b[36m   (2, \"b\")\u001b[39m\n",
       "\u001b[36m   (1, \"c\")\u001b[39m\n",
       "\n",
       "\u001b[36m  sort!(A; dims::Integer, alg::Algorithm=defalg(v), lt=isless, by=identity, rev::Bool=false, order::Ordering=Forward)\u001b[39m\n",
       "\n",
       "  Sort the multidimensional array \u001b[36mA\u001b[39m along dimension \u001b[36mdims\u001b[39m. See \u001b[36msort!\u001b[39m\n",
       "  for a description of possible keyword arguments.\n",
       "\n",
       "  To sort slices of an array, refer to \u001b[36msortslices\u001b[39m.\n",
       "\n",
       "\u001b[39m\u001b[1m  │ \u001b[22m\u001b[39m\u001b[1mJulia 1.1\u001b[22m\n",
       "\u001b[39m\u001b[1m  │\u001b[22m\n",
       "\u001b[39m\u001b[1m  │\u001b[22m  This function requires at least Julia 1.1.\n",
       "\n",
       "\u001b[1m  Examples\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "\u001b[36m  julia> A = [4 3; 1 2]\u001b[39m\n",
       "\u001b[36m  2×2 Array{Int64,2}:\u001b[39m\n",
       "\u001b[36m   4  3\u001b[39m\n",
       "\u001b[36m   1  2\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> sort!(A, dims = 1); A\u001b[39m\n",
       "\u001b[36m  2×2 Array{Int64,2}:\u001b[39m\n",
       "\u001b[36m   1  2\u001b[39m\n",
       "\u001b[36m   4  3\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> sort!(A, dims = 2); A\u001b[39m\n",
       "\u001b[36m  2×2 Array{Int64,2}:\u001b[39m\n",
       "\u001b[36m   1  2\u001b[39m\n",
       "\u001b[36m   3  4\u001b[39m"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@doc sort!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parse1 (generic function with 1 method)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Parse1(;o...) = \n",
    "Chain(tirtEmbed(), RNN(te_out, hi; h=0, c=0, o...), hiMLP())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "rnn = RNN(inputSize, hiddenSize; opts...)\n",
       "rnn(x; batchSizes) => y\n",
       "rnn.h, rnn.c  # hidden and cell states\n",
       "\\end{verbatim}\n",
       "\\texttt{RNN} returns a callable RNN object \\texttt{rnn}. Given a minibatch of sequences \\texttt{x}, \\texttt{rnn(x)} returns \\texttt{y}, the hidden states of the final layer for each time step. \\texttt{rnn.h} and \\texttt{rnn.c} fields can be used to set the initial hidden states and read the final hidden states of all layers.  Note that the final time step of \\texttt{y} always contains the final hidden state of the last layer, equivalent to \\texttt{rnn.h} for a single layer network.\n",
       "\n",
       "\\textbf{Dimensions:} The input \\texttt{x} can be 1, 2, or 3 dimensional and \\texttt{y} will have the same number of dimensions as \\texttt{x}. size(x)=(X,[B,T]) and size(y)=(H/2H,[B,T]) where X is inputSize, B is batchSize, T is seqLength, H is hiddenSize, 2H is for bidirectional RNNs. By default a 1-D \\texttt{x} represents a single instance for a single time step, a 2-D \\texttt{x} represents a single minibatch for a single time step, and a 3-D \\texttt{x} represents a sequence of identically sized minibatches for multiple time steps. The output \\texttt{y} gives the hidden state (of the final layer for multi-layer RNNs) for each time step. The fields \\texttt{rnn.h} and \\texttt{rnn.c} represent the hidden states of all layers in a single time step and have size (H,B,L/2L) where L is numLayers and 2L is for bidirectional RNNs.\n",
       "\n",
       "\\textbf{batchSizes:} If \\texttt{batchSizes=nothing} (default), all sequences in a minibatch are assumed to be the same length. If \\texttt{batchSizes} is an array of (non-increasing) integers, it gives us the batch size for each time step (allowing different sequences in the minibatch to have different lengths). In this case \\texttt{x} will typically be 2-D with the second dimension representing variable size batches for time steps. If \\texttt{batchSizes} is used, \\texttt{sum(batchSizes)} should equal \\texttt{length(x) ÷ size(x,1)}. When the batch size is different in every time step, hidden states will have size (H,B,L/2L) where B is always the size of the first (largest) minibatch.\n",
       "\n",
       "\\textbf{Hidden states:} The hidden and cell states are kept in \\texttt{rnn.h} and \\texttt{rnn.c} fields (the cell state is only used by LSTM). They can be initialized during construction using the \\texttt{h} and \\texttt{c} keyword arguments, or modified later by direct assignment. Valid values are \\texttt{nothing} (default), \\texttt{0}, or an array of the right type and size possibly wrapped in a \\texttt{Param}. If the value is \\texttt{nothing} the initial state is assumed to be zero and the final state is discarded keeping the value \\texttt{nothing}. If the value is \\texttt{0} the initial state is assumed to be zero and \\texttt{0} is replaced by the final state on return. If the value is a valid state, it is used as the initial state and is replaced by the final state on return.\n",
       "\n",
       "In a differentiation context the returned final hidden states will be wrapped in \\texttt{Result} types. This is necessary if the same RNN object is to be called multiple times in a single iteration. Between iterations (i.e. after diff/update) the hidden states need to be unboxed with e.g. \\texttt{rnn.h = value(rnn.h)} to prevent spurious dependencies. This happens automatically during the backward pass for GPU RNNs but needs to be done manually for CPU RNNs. See the \\href{https://github.com/denizyuret/Knet.jl/blob/master/tutorial/80.charlm.ipynb}{CharLM Tutorial} for an example.\n",
       "\n",
       "\\textbf{Keyword arguments for RNN:}\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{h=nothing}: Initial hidden state.\n",
       "\n",
       "\n",
       "\\item \\texttt{c=nothing}: Initial cell state.\n",
       "\n",
       "\n",
       "\\item \\texttt{rnnType=:lstm} Type of RNN: One of :relu, :tanh, :lstm, :gru.\n",
       "\n",
       "\n",
       "\\item \\texttt{numLayers=1}: Number of RNN layers.\n",
       "\n",
       "\n",
       "\\item \\texttt{bidirectional=false}: Create a bidirectional RNN if \\texttt{true}.\n",
       "\n",
       "\n",
       "\\item \\texttt{dropout=0}: Dropout probability. Applied to input and between layers.\n",
       "\n",
       "\n",
       "\\item \\texttt{skipInput=false}: Do not multiply the input with a matrix if \\texttt{true}.\n",
       "\n",
       "\n",
       "\\item \\texttt{dataType=Float32}: Data type to use for weights.\n",
       "\n",
       "\n",
       "\\item \\texttt{algo=0}: Algorithm to use, see CUDNN docs for details.\n",
       "\n",
       "\n",
       "\\item \\texttt{seed=0}: Random number seed for dropout. Uses \\texttt{time()} if 0.\n",
       "\n",
       "\n",
       "\\item \\texttt{winit=xavier}: Weight initialization method for matrices.\n",
       "\n",
       "\n",
       "\\item \\texttt{binit=zeros}: Weight initialization method for bias vectors.\n",
       "\n",
       "\n",
       "\\item \\texttt{usegpu=(gpu()>=0)}: GPU used by default if one exists.\n",
       "\n",
       "\\end{itemize}\n",
       "\\textbf{Formulas:} RNNs compute the output h[t] for a given iteration from the recurrent input h[t-1] and the previous layer input x[t] given matrices W, R and biases bW, bR from the following equations:\n",
       "\n",
       "\\texttt{:relu} and \\texttt{:tanh}: Single gate RNN with activation function f:\n",
       "\n",
       "\\begin{verbatim}\n",
       "h[t] = f(W * x[t] .+ R * h[t-1] .+ bW .+ bR)\n",
       "\\end{verbatim}\n",
       "\\texttt{:gru}: Gated recurrent unit:\n",
       "\n",
       "\\begin{verbatim}\n",
       "i[t] = sigm(Wi * x[t] .+ Ri * h[t-1] .+ bWi .+ bRi) # input gate\n",
       "r[t] = sigm(Wr * x[t] .+ Rr * h[t-1] .+ bWr .+ bRr) # reset gate\n",
       "n[t] = tanh(Wn * x[t] .+ r[t] .* (Rn * h[t-1] .+ bRn) .+ bWn) # new gate\n",
       "h[t] = (1 - i[t]) .* n[t] .+ i[t] .* h[t-1]\n",
       "\\end{verbatim}\n",
       "\\texttt{:lstm}: Long short term memory unit with no peephole connections:\n",
       "\n",
       "\\begin{verbatim}\n",
       "i[t] = sigm(Wi * x[t] .+ Ri * h[t-1] .+ bWi .+ bRi) # input gate\n",
       "f[t] = sigm(Wf * x[t] .+ Rf * h[t-1] .+ bWf .+ bRf) # forget gate\n",
       "o[t] = sigm(Wo * x[t] .+ Ro * h[t-1] .+ bWo .+ bRo) # output gate\n",
       "n[t] = tanh(Wn * x[t] .+ Rn * h[t-1] .+ bWn .+ bRn) # new gate\n",
       "c[t] = f[t] .* c[t-1] .+ i[t] .* n[t]               # cell output\n",
       "h[t] = o[t] .* tanh(c[t])\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "```\n",
       "rnn = RNN(inputSize, hiddenSize; opts...)\n",
       "rnn(x; batchSizes) => y\n",
       "rnn.h, rnn.c  # hidden and cell states\n",
       "```\n",
       "\n",
       "`RNN` returns a callable RNN object `rnn`. Given a minibatch of sequences `x`, `rnn(x)` returns `y`, the hidden states of the final layer for each time step. `rnn.h` and `rnn.c` fields can be used to set the initial hidden states and read the final hidden states of all layers.  Note that the final time step of `y` always contains the final hidden state of the last layer, equivalent to `rnn.h` for a single layer network.\n",
       "\n",
       "**Dimensions:** The input `x` can be 1, 2, or 3 dimensional and `y` will have the same number of dimensions as `x`. size(x)=(X,[B,T]) and size(y)=(H/2H,[B,T]) where X is inputSize, B is batchSize, T is seqLength, H is hiddenSize, 2H is for bidirectional RNNs. By default a 1-D `x` represents a single instance for a single time step, a 2-D `x` represents a single minibatch for a single time step, and a 3-D `x` represents a sequence of identically sized minibatches for multiple time steps. The output `y` gives the hidden state (of the final layer for multi-layer RNNs) for each time step. The fields `rnn.h` and `rnn.c` represent the hidden states of all layers in a single time step and have size (H,B,L/2L) where L is numLayers and 2L is for bidirectional RNNs.\n",
       "\n",
       "**batchSizes:** If `batchSizes=nothing` (default), all sequences in a minibatch are assumed to be the same length. If `batchSizes` is an array of (non-increasing) integers, it gives us the batch size for each time step (allowing different sequences in the minibatch to have different lengths). In this case `x` will typically be 2-D with the second dimension representing variable size batches for time steps. If `batchSizes` is used, `sum(batchSizes)` should equal `length(x) ÷ size(x,1)`. When the batch size is different in every time step, hidden states will have size (H,B,L/2L) where B is always the size of the first (largest) minibatch.\n",
       "\n",
       "**Hidden states:** The hidden and cell states are kept in `rnn.h` and `rnn.c` fields (the cell state is only used by LSTM). They can be initialized during construction using the `h` and `c` keyword arguments, or modified later by direct assignment. Valid values are `nothing` (default), `0`, or an array of the right type and size possibly wrapped in a `Param`. If the value is `nothing` the initial state is assumed to be zero and the final state is discarded keeping the value `nothing`. If the value is `0` the initial state is assumed to be zero and `0` is replaced by the final state on return. If the value is a valid state, it is used as the initial state and is replaced by the final state on return.\n",
       "\n",
       "In a differentiation context the returned final hidden states will be wrapped in `Result` types. This is necessary if the same RNN object is to be called multiple times in a single iteration. Between iterations (i.e. after diff/update) the hidden states need to be unboxed with e.g. `rnn.h = value(rnn.h)` to prevent spurious dependencies. This happens automatically during the backward pass for GPU RNNs but needs to be done manually for CPU RNNs. See the [CharLM Tutorial](https://github.com/denizyuret/Knet.jl/blob/master/tutorial/80.charlm.ipynb) for an example.\n",
       "\n",
       "**Keyword arguments for RNN:**\n",
       "\n",
       "  * `h=nothing`: Initial hidden state.\n",
       "  * `c=nothing`: Initial cell state.\n",
       "  * `rnnType=:lstm` Type of RNN: One of :relu, :tanh, :lstm, :gru.\n",
       "  * `numLayers=1`: Number of RNN layers.\n",
       "  * `bidirectional=false`: Create a bidirectional RNN if `true`.\n",
       "  * `dropout=0`: Dropout probability. Applied to input and between layers.\n",
       "  * `skipInput=false`: Do not multiply the input with a matrix if `true`.\n",
       "  * `dataType=Float32`: Data type to use for weights.\n",
       "  * `algo=0`: Algorithm to use, see CUDNN docs for details.\n",
       "  * `seed=0`: Random number seed for dropout. Uses `time()` if 0.\n",
       "  * `winit=xavier`: Weight initialization method for matrices.\n",
       "  * `binit=zeros`: Weight initialization method for bias vectors.\n",
       "  * `usegpu=(gpu()>=0)`: GPU used by default if one exists.\n",
       "\n",
       "**Formulas:** RNNs compute the output h[t] for a given iteration from the recurrent input h[t-1] and the previous layer input x[t] given matrices W, R and biases bW, bR from the following equations:\n",
       "\n",
       "`:relu` and `:tanh`: Single gate RNN with activation function f:\n",
       "\n",
       "```\n",
       "h[t] = f(W * x[t] .+ R * h[t-1] .+ bW .+ bR)\n",
       "```\n",
       "\n",
       "`:gru`: Gated recurrent unit:\n",
       "\n",
       "```\n",
       "i[t] = sigm(Wi * x[t] .+ Ri * h[t-1] .+ bWi .+ bRi) # input gate\n",
       "r[t] = sigm(Wr * x[t] .+ Rr * h[t-1] .+ bWr .+ bRr) # reset gate\n",
       "n[t] = tanh(Wn * x[t] .+ r[t] .* (Rn * h[t-1] .+ bRn) .+ bWn) # new gate\n",
       "h[t] = (1 - i[t]) .* n[t] .+ i[t] .* h[t-1]\n",
       "```\n",
       "\n",
       "`:lstm`: Long short term memory unit with no peephole connections:\n",
       "\n",
       "```\n",
       "i[t] = sigm(Wi * x[t] .+ Ri * h[t-1] .+ bWi .+ bRi) # input gate\n",
       "f[t] = sigm(Wf * x[t] .+ Rf * h[t-1] .+ bWf .+ bRf) # forget gate\n",
       "o[t] = sigm(Wo * x[t] .+ Ro * h[t-1] .+ bWo .+ bRo) # output gate\n",
       "n[t] = tanh(Wn * x[t] .+ Rn * h[t-1] .+ bWn .+ bRn) # new gate\n",
       "c[t] = f[t] .* c[t-1] .+ i[t] .* n[t]               # cell output\n",
       "h[t] = o[t] .* tanh(c[t])\n",
       "```\n"
      ],
      "text/plain": [
       "\u001b[36m  rnn = RNN(inputSize, hiddenSize; opts...)\u001b[39m\n",
       "\u001b[36m  rnn(x; batchSizes) => y\u001b[39m\n",
       "\u001b[36m  rnn.h, rnn.c  # hidden and cell states\u001b[39m\n",
       "\n",
       "  \u001b[36mRNN\u001b[39m returns a callable RNN object \u001b[36mrnn\u001b[39m. Given a minibatch of\n",
       "  sequences \u001b[36mx\u001b[39m, \u001b[36mrnn(x)\u001b[39m returns \u001b[36my\u001b[39m, the hidden states of the final layer\n",
       "  for each time step. \u001b[36mrnn.h\u001b[39m and \u001b[36mrnn.c\u001b[39m fields can be used to set the\n",
       "  initial hidden states and read the final hidden states of all\n",
       "  layers. Note that the final time step of \u001b[36my\u001b[39m always contains the final\n",
       "  hidden state of the last layer, equivalent to \u001b[36mrnn.h\u001b[39m for a single\n",
       "  layer network.\n",
       "\n",
       "  \u001b[1mDimensions:\u001b[22m The input \u001b[36mx\u001b[39m can be 1, 2, or 3 dimensional and \u001b[36my\u001b[39m will\n",
       "  have the same number of dimensions as \u001b[36mx\u001b[39m. size(x)=(X,[B,T]) and\n",
       "  size(y)=(H/2H,[B,T]) where X is inputSize, B is batchSize, T is\n",
       "  seqLength, H is hiddenSize, 2H is for bidirectional RNNs. By default\n",
       "  a 1-D \u001b[36mx\u001b[39m represents a single instance for a single time step, a 2-D \u001b[36mx\u001b[39m\n",
       "  represents a single minibatch for a single time step, and a 3-D \u001b[36mx\u001b[39m\n",
       "  represents a sequence of identically sized minibatches for multiple\n",
       "  time steps. The output \u001b[36my\u001b[39m gives the hidden state (of the final layer\n",
       "  for multi-layer RNNs) for each time step. The fields \u001b[36mrnn.h\u001b[39m and \u001b[36mrnn.c\u001b[39m\n",
       "  represent the hidden states of all layers in a single time step and\n",
       "  have size (H,B,L/2L) where L is numLayers and 2L is for\n",
       "  bidirectional RNNs.\n",
       "\n",
       "  \u001b[1mbatchSizes:\u001b[22m If \u001b[36mbatchSizes=nothing\u001b[39m (default), all sequences in a\n",
       "  minibatch are assumed to be the same length. If \u001b[36mbatchSizes\u001b[39m is an\n",
       "  array of (non-increasing) integers, it gives us the batch size for\n",
       "  each time step (allowing different sequences in the minibatch to\n",
       "  have different lengths). In this case \u001b[36mx\u001b[39m will typically be 2-D with\n",
       "  the second dimension representing variable size batches for time\n",
       "  steps. If \u001b[36mbatchSizes\u001b[39m is used, \u001b[36msum(batchSizes)\u001b[39m should equal \u001b[36mlength(x)\n",
       "  ÷ size(x,1)\u001b[39m. When the batch size is different in every time step,\n",
       "  hidden states will have size (H,B,L/2L) where B is always the size\n",
       "  of the first (largest) minibatch.\n",
       "\n",
       "  \u001b[1mHidden states:\u001b[22m The hidden and cell states are kept in \u001b[36mrnn.h\u001b[39m and\n",
       "  \u001b[36mrnn.c\u001b[39m fields (the cell state is only used by LSTM). They can be\n",
       "  initialized during construction using the \u001b[36mh\u001b[39m and \u001b[36mc\u001b[39m keyword arguments,\n",
       "  or modified later by direct assignment. Valid values are \u001b[36mnothing\u001b[39m\n",
       "  (default), \u001b[36m0\u001b[39m, or an array of the right type and size possibly\n",
       "  wrapped in a \u001b[36mParam\u001b[39m. If the value is \u001b[36mnothing\u001b[39m the initial state is\n",
       "  assumed to be zero and the final state is discarded keeping the\n",
       "  value \u001b[36mnothing\u001b[39m. If the value is \u001b[36m0\u001b[39m the initial state is assumed to be\n",
       "  zero and \u001b[36m0\u001b[39m is replaced by the final state on return. If the value is\n",
       "  a valid state, it is used as the initial state and is replaced by\n",
       "  the final state on return.\n",
       "\n",
       "  In a differentiation context the returned final hidden states will\n",
       "  be wrapped in \u001b[36mResult\u001b[39m types. This is necessary if the same RNN object\n",
       "  is to be called multiple times in a single iteration. Between\n",
       "  iterations (i.e. after diff/update) the hidden states need to be\n",
       "  unboxed with e.g. \u001b[36mrnn.h = value(rnn.h)\u001b[39m to prevent spurious\n",
       "  dependencies. This happens automatically during the backward pass\n",
       "  for GPU RNNs but needs to be done manually for CPU RNNs. See the\n",
       "  CharLM Tutorial\n",
       "  (https://github.com/denizyuret/Knet.jl/blob/master/tutorial/80.charlm.ipynb)\n",
       "  for an example.\n",
       "\n",
       "  \u001b[1mKeyword arguments for RNN:\u001b[22m\n",
       "\n",
       "    •    \u001b[36mh=nothing\u001b[39m: Initial hidden state.\n",
       "\n",
       "    •    \u001b[36mc=nothing\u001b[39m: Initial cell state.\n",
       "\n",
       "    •    \u001b[36mrnnType=:lstm\u001b[39m Type of RNN: One of :relu, :tanh, :lstm,\n",
       "        :gru.\n",
       "\n",
       "    •    \u001b[36mnumLayers=1\u001b[39m: Number of RNN layers.\n",
       "\n",
       "    •    \u001b[36mbidirectional=false\u001b[39m: Create a bidirectional RNN if \u001b[36mtrue\u001b[39m.\n",
       "\n",
       "    •    \u001b[36mdropout=0\u001b[39m: Dropout probability. Applied to input and\n",
       "        between layers.\n",
       "\n",
       "    •    \u001b[36mskipInput=false\u001b[39m: Do not multiply the input with a matrix\n",
       "        if \u001b[36mtrue\u001b[39m.\n",
       "\n",
       "    •    \u001b[36mdataType=Float32\u001b[39m: Data type to use for weights.\n",
       "\n",
       "    •    \u001b[36malgo=0\u001b[39m: Algorithm to use, see CUDNN docs for details.\n",
       "\n",
       "    •    \u001b[36mseed=0\u001b[39m: Random number seed for dropout. Uses \u001b[36mtime()\u001b[39m if 0.\n",
       "\n",
       "    •    \u001b[36mwinit=xavier\u001b[39m: Weight initialization method for matrices.\n",
       "\n",
       "    •    \u001b[36mbinit=zeros\u001b[39m: Weight initialization method for bias\n",
       "        vectors.\n",
       "\n",
       "    •    \u001b[36musegpu=(gpu()>=0)\u001b[39m: GPU used by default if one exists.\n",
       "\n",
       "  \u001b[1mFormulas:\u001b[22m RNNs compute the output h[t] for a given iteration from\n",
       "  the recurrent input h[t-1] and the previous layer input x[t] given\n",
       "  matrices W, R and biases bW, bR from the following equations:\n",
       "\n",
       "  \u001b[36m:relu\u001b[39m and \u001b[36m:tanh\u001b[39m: Single gate RNN with activation function f:\n",
       "\n",
       "\u001b[36m  h[t] = f(W * x[t] .+ R * h[t-1] .+ bW .+ bR)\u001b[39m\n",
       "\n",
       "  \u001b[36m:gru\u001b[39m: Gated recurrent unit:\n",
       "\n",
       "\u001b[36m  i[t] = sigm(Wi * x[t] .+ Ri * h[t-1] .+ bWi .+ bRi) # input gate\u001b[39m\n",
       "\u001b[36m  r[t] = sigm(Wr * x[t] .+ Rr * h[t-1] .+ bWr .+ bRr) # reset gate\u001b[39m\n",
       "\u001b[36m  n[t] = tanh(Wn * x[t] .+ r[t] .* (Rn * h[t-1] .+ bRn) .+ bWn) # new gate\u001b[39m\n",
       "\u001b[36m  h[t] = (1 - i[t]) .* n[t] .+ i[t] .* h[t-1]\u001b[39m\n",
       "\n",
       "  \u001b[36m:lstm\u001b[39m: Long short term memory unit with no peephole connections:\n",
       "\n",
       "\u001b[36m  i[t] = sigm(Wi * x[t] .+ Ri * h[t-1] .+ bWi .+ bRi) # input gate\u001b[39m\n",
       "\u001b[36m  f[t] = sigm(Wf * x[t] .+ Rf * h[t-1] .+ bWf .+ bRf) # forget gate\u001b[39m\n",
       "\u001b[36m  o[t] = sigm(Wo * x[t] .+ Ro * h[t-1] .+ bWo .+ bRo) # output gate\u001b[39m\n",
       "\u001b[36m  n[t] = tanh(Wn * x[t] .+ Rn * h[t-1] .+ bWn .+ bRn) # new gate\u001b[39m\n",
       "\u001b[36m  c[t] = f[t] .* c[t-1] .+ i[t] .* n[t]               # cell output\u001b[39m\n",
       "\u001b[36m  h[t] = o[t] .* tanh(c[t])\u001b[39m"
      ]
     },
     "execution_count": 696,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@doc Knet.RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Biaff"
      ]
     },
     "execution_count": 648,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Biaff(rhid::Int, mlph::Int, m::Int) =\n",
    "    Biaff(MLP(rhid,mlph, m) , MLP(rhid,mlph, m) , param(m,m;atype=Array{Float64}), param0(m;atype=Array{Float64}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fModel (generic function with 3 methods)"
      ]
     },
     "execution_count": 697,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fModel(tembed::Int, rnnh::Int, mlp1h::Int, mlp1o::Int; o...) = \n",
    "    Chain(RNN(tembed, rnnh; bidirectional=true, rnnType = :lstm, dataType=Float64,o...), Biaff(2*rnnh,mlp1h,mlp1o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain((LSTM(input=50,hidden=100,bidirectional), Biaff(Chain((Dense(P(Array{Float64,2}(100,200)), P(Array{Float64,1}(100)), Knet.relu), Dense(P(Array{Float64,2}(50,100)), P(Array{Float64,1}(50)), Knet.relu))), Chain((Dense(P(Array{Float64,2}(100,200)), P(Array{Float64,1}(100)), Knet.relu), Dense(P(Array{Float64,2}(50,100)), P(Array{Float64,1}(50)), Knet.relu))), P(Array{Float64,2}(50,50)), P(Array{Float64,1}(50)))))"
      ]
     },
     "execution_count": 650,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mymodel1 = fModel(50,100,100,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain((LSTM(input=50,hidden=100,bidirectional,Float64), Biaff(Chain((Dense(P(Array{Float64,2}(100,200)), P(Array{Float64,1}(100)), Knet.relu), Dense(P(Array{Float64,2}(50,100)), P(Array{Float64,1}(50)), Knet.relu))), Chain((Dense(P(Array{Float64,2}(100,200)), P(Array{Float64,1}(100)), Knet.relu), Dense(P(Array{Float64,2}(50,100)), P(Array{Float64,1}(50)), Knet.relu))), P(Array{Float64,2}(50,50)), P(Array{Float64,1}(50)))))"
      ]
     },
     "execution_count": 736,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mymodel2 = fModel(50,100,100,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8001002342890495"
      ]
     },
     "execution_count": 664,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mymodel1(wembedmat[:,dat1x],dat1y .+ 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Base.Generator{Base.Generator{Base.Iterators.Zip{Tuple{Base.Generator{Array{Any,1},getfield(Main, Symbol(\"##128#129\"))},Array{Any,1}}},getfield(Main, Symbol(\"##182#183\"))},getfield(Main, Symbol(\"##192#193\"))}(getfield(Main, Symbol(\"##192#193\"))(), Base.Generator{Base.Iterators.Zip{Tuple{Base.Generator{Array{Any,1},getfield(Main, Symbol(\"##128#129\"))},Array{Any,1}}},getfield(Main, Symbol(\"##182#183\"))}(getfield(Main, Symbol(\"##182#183\"))(), Base.Iterators.Zip{Tuple{Base.Generator{Array{Any,1},getfield(Main, Symbol(\"##128#129\"))},Array{Any,1}}}((Base.Generator{Array{Any,1},getfield(Main, Symbol(\"##128#129\"))}(getfield(Main, Symbol(\"##128#129\"))(), Any[Any[\"Al\", \"-\", \"Zaman\", \":\", \"American\", \"forces\", \"killed\", \"Shaikh\", \"Abdullah\", \"al\"  …  \"the\", \"town\", \"of\", \"Qaim\", \",\", \"near\", \"the\", \"Syrian\", \"border\", \".\"], Any[\"[\", \"This\", \"killing\", \"of\", \"a\", \"respected\", \"cleric\", \"will\", \"be\", \"causing\", \"us\", \"trouble\", \"for\", \"years\", \"to\", \"come\", \".\", \"]\"], Any[\"DPA\", \":\", \"Iraqi\", \"authorities\", \"announced\", \"that\", \"they\", \"had\", \"busted\", \"up\", \"3\", \"terrorist\", \"cells\", \"operating\", \"in\", \"Baghdad\", \".\"], Any[\"Two\", \"of\", \"them\", \"were\", \"being\", \"run\", \"by\", \"2\", \"officials\", \"of\", \"the\", \"Ministry\", \"of\", \"the\", \"Interior\", \"!\"], Any[\"The\", \"MoI\", \"in\", \"Iraq\", \"is\", \"equivalent\", \"to\", \"the\", \"US\", \"FBI\"  …  \"members\", \"of\", \"the\", \"Weathermen\", \"bombers\", \"back\", \"in\", \"the\", \"1960s\", \".\"], Any[\"The\", \"third\", \"was\", \"being\", \"run\", \"by\", \"the\", \"head\", \"of\", \"an\", \"investment\", \"firm\", \".\"], Any[\"You\", \"wonder\", \"if\", \"he\", \"was\", \"manipulating\", \"the\", \"market\", \"with\", \"his\", \"bombing\", \"targets\", \".\"], Any[\"The\", \"cells\", \"were\", \"operating\", \"in\", \"the\", \"Ghazaliyah\", \"and\", \"al\", \"-\", \"Jihad\", \"districts\", \"of\", \"the\", \"capital\", \".\"], Any[\"Although\", \"the\", \"announcement\", \"was\", \"probably\", \"made\", \"to\", \"show\", \"progress\", \"in\"  …  \"Baathists\", \"continue\", \"to\", \"penetrate\", \"the\", \"Iraqi\", \"government\", \"very\", \"hopeful\", \".\"], Any[\"It\", \"reminds\", \"me\", \"too\", \"much\", \"of\", \"the\", \"ARVN\", \"officers\", \"who\", \"were\", \"secretly\", \"working\", \"for\", \"the\", \"other\", \"side\", \"in\", \"Vietnam\", \".\"]  …  Any[\"Over\", \"two\", \"hours\", \"later\", \"(\", \"and\", \"ten\", \"minutes\", \"before\", \"they\", \"closed\", \")\", \"my\", \"car\", \"was\", \"finally\", \"finished\", \".\"], Any[\"A\", \"few\", \"minutes\", \"after\", \"I\", \"left\", \",\", \"I\", \"was\", \"called\"  …  \"which\", \"they\", \"should\", \"have\", \"left\", \"in\", \"the\", \"car\", \")\", \".\"], Any[\"Of\", \"course\", \",\", \"they\", \"would\", \"be\", \"closing\", \"in\", \"5\", \"minutes\"  …  \"to\", \"hurry\", \"up\", \"or\", \"get\", \"it\", \"the\", \"next\", \"day\", \".\"], Any[\"Of\", \"course\", \"I\", \"could\", \"n't\", \"make\", \"it\", \"back\", \"in\", \"time\"  …  \"stay\", \"5\", \"extra\", \"minutes\", \"to\", \"wait\", \"for\", \"me\", \")\", \".\"], Any[\"The\", \"next\", \"day\", \",\", \"no\", \"one\", \"could\", \"find\", \"my\", \"wheel\", \"lock\", \"and\", \"that\", \"particular\", \"technician\", \"was\", \"not\", \"in\", \".\"], Any[\"Of\", \"course\", \",\", \"they\", \"could\", \"n't\", \"call\", \"him\", \"either\", \"to\"  …  \"no\", \"wheel\", \"lock\", \"should\", \"I\", \"get\", \"a\", \"flat\", \")\", \".\"], Any[\"On\", \"Monday\", \"I\", \"called\", \"and\", \"again\", \"it\", \"was\", \"a\", \"big\"  …  \"do\", \"to\", \"find\", \"anyone\", \"who\", \"knew\", \"anything\", \"about\", \"it\", \".\"], Any[\"Supposedly\", \"they\", \"will\", \"be\", \"holding\", \"it\", \"for\", \"me\", \"this\", \"evening\"  …  \"'m\", \"sure\", \"that\", \"will\", \"also\", \"be\", \"a\", \"huge\", \"ordeal\", \".\"], Any[\"The\", \"employees\", \"at\", \"this\", \"Sear's\", \"are\", \"completely\", \"apathetic\", \"and\", \"there\"  …  \"be\", \"any\", \"sort\", \"of\", \"management\", \"that\", \"I\", \"could\", \"see\", \".\"], Any[\"I\", \"will\", \"never\", \"return\", \"there\", \"again\", \"(\", \"and\", \"now\", \"have\"  …  \"of\", \"work\", \"they\", \"actually\", \"performed\", \"on\", \"my\", \"car\", \")\", \".\"]]), Any[Any[0, 1, 1, 1, 6, 7, 1, 7, 8, 8  …  21, 18, 23, 21, 21, 28, 28, 28, 21, 1], Any[10, 3, 10, 7, 7, 7, 3, 10, 10, 0, 10, 10, 14, 10, 16, 14, 10, 10], Any[0, 1, 4, 5, 1, 9, 9, 9, 5, 9, 13, 13, 9, 13, 16, 14, 1], Any[6, 3, 1, 6, 6, 0, 9, 9, 6, 12, 12, 9, 15, 15, 12, 6], Any[2, 6, 4, 2, 6, 0, 10, 10, 10, 6  …  22, 31, 31, 31, 27, 35, 35, 35, 22, 6], Any[2, 5, 5, 5, 0, 8, 8, 5, 12, 12, 12, 8, 5], Any[2, 0, 6, 6, 6, 2, 8, 6, 12, 12, 12, 6, 2], Any[2, 4, 4, 0, 12, 12, 12, 11, 11, 11, 7, 4, 15, 15, 12, 4], Any[6, 3, 6, 6, 6, 21, 8, 6, 8, 11  …  27, 23, 29, 27, 32, 32, 29, 34, 21, 21], Any[2, 0, 2, 5, 2, 9, 9, 9, 2, 13, 13, 13, 9, 17, 17, 17, 13, 19, 13, 2]  …  Any[2, 3, 4, 17, 4, 11, 8, 9, 11, 11, 4, 4, 14, 17, 17, 17, 0, 17], Any[3, 3, 6, 6, 6, 10, 10, 10, 10, 0  …  26, 26, 26, 26, 20, 29, 29, 26, 26, 10], Any[7, 1, 7, 7, 7, 7, 0, 10, 10, 7  …  17, 15, 17, 20, 17, 20, 24, 24, 20, 7], Any[6, 1, 6, 6, 6, 0, 6, 6, 10, 6  …  6, 20, 20, 17, 22, 17, 24, 22, 6, 6], Any[3, 3, 8, 8, 6, 8, 8, 0, 11, 11, 8, 18, 15, 15, 18, 18, 18, 8, 8], Any[7, 1, 7, 7, 7, 7, 0, 7, 7, 11  …  46, 46, 38, 49, 49, 38, 51, 49, 38, 7], Any[2, 4, 4, 0, 13, 13, 13, 13, 13, 13  …  4, 15, 13, 15, 18, 16, 18, 21, 19, 4], Any[5, 5, 5, 5, 0, 5, 8, 5, 10, 5  …  15, 5, 22, 22, 22, 22, 22, 22, 15, 5], Any[2, 8, 5, 5, 2, 8, 8, 0, 13, 13  …  13, 17, 15, 19, 17, 23, 23, 23, 19, 8], Any[4, 4, 4, 0, 4, 4, 4, 10, 10, 4  …  18, 16, 21, 21, 18, 24, 24, 21, 4, 4]]))))"
      ]
     },
     "execution_count": 588,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data5 = ((wembedmat[:,x],y .+ 1) for (x,y) in data4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.3327302102828806"
      ]
     },
     "execution_count": 676,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mymodel2(dtrn[1151]...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29-element Array{Int64,1}:\n",
       "  1\n",
       "  2\n",
       "  2\n",
       "  2\n",
       "  7\n",
       "  8\n",
       "  2\n",
       "  8\n",
       "  9\n",
       "  9\n",
       "  9\n",
       "  9\n",
       "  9\n",
       "  ⋮\n",
       "  8\n",
       " 22\n",
       " 22\n",
       " 19\n",
       " 24\n",
       " 22\n",
       " 22\n",
       " 29\n",
       " 29\n",
       " 29\n",
       " 22\n",
       "  2"
      ]
     },
     "execution_count": 592,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterate(data5)[1][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50×1×30 Array{Float64,3}:\n",
       "[:, :, 1] =\n",
       " -0.7589799761772156  \n",
       " -0.47426000237464905 \n",
       "  0.47369998693466187 \n",
       "  0.7724999785423279  \n",
       " -0.7806400060653687  \n",
       "  0.23232999444007874 \n",
       "  0.0461140014231205  \n",
       "  0.8401399850845337  \n",
       "  0.243709996342659   \n",
       "  0.022978000342845917\n",
       "  0.5396400094032288  \n",
       " -0.36100998520851135 \n",
       "  0.9419800043106079  \n",
       "  ⋮                   \n",
       "  0.035413000732660294\n",
       "  0.5883399844169617  \n",
       "  0.4543899893760681  \n",
       " -0.8425400257110596  \n",
       "  0.10649999976158142 \n",
       " -0.059397000819444656\n",
       "  0.09044899791479111 \n",
       "  0.30581000447273254 \n",
       " -0.6142399907112122  \n",
       "  0.7895399928092957  \n",
       " -0.014116000384092331\n",
       "  0.6448000073432922  \n",
       "\n",
       "[:, :, 2] =\n",
       "  0.542140007019043  \n",
       "  1.0302000045776367 \n",
       "  0.8689600229263306 \n",
       "  0.5001400113105774 \n",
       "  0.9518200159072876 \n",
       " -1.3366999626159668 \n",
       " -0.4010699987411499 \n",
       "  0.3922699987888336 \n",
       "  0.536620020866394  \n",
       "  0.48791998624801636\n",
       " -0.8468700051307678 \n",
       " -0.6293799877166748 \n",
       " -1.3402999639511108 \n",
       "  ⋮                  \n",
       " -0.7340899705886841 \n",
       "  1.3209999799728394 \n",
       "  0.5159100294113159 \n",
       "  0.5730599761009216 \n",
       " -0.8462100028991699 \n",
       " -0.2014700025320053 \n",
       "  1.2488000392913818 \n",
       " -0.7575899958610535 \n",
       " -2.0058999061584473 \n",
       "  0.4759100079536438 \n",
       "  0.18317000567913055\n",
       "  0.43380001187324524\n",
       "\n",
       "[:, :, 3] =\n",
       " -0.16767999529838562\n",
       "  1.2151000499725342 \n",
       "  0.49514999985694885\n",
       "  0.2683599889278412 \n",
       " -0.4584999978542328 \n",
       " -0.2331099957227707 \n",
       " -0.528219997882843  \n",
       " -1.3557000160217285 \n",
       "  0.1609800010919571 \n",
       "  0.376910001039505  \n",
       " -0.9270200133323669 \n",
       " -0.43904000520706177\n",
       " -1.0634000301361084 \n",
       "  ⋮                  \n",
       "  0.6995599865913391 \n",
       "  0.1488499939441681 \n",
       "  0.02945300005376339\n",
       "  1.488800048828125  \n",
       "  0.52360999584198   \n",
       "  0.09935399889945984\n",
       "  1.2515000104904175 \n",
       "  0.09938099980354309\n",
       " -0.07926099747419357\n",
       " -0.30862000584602356\n",
       "  0.30893000960350037\n",
       "  0.11022999882698059\n",
       "\n",
       "...\n",
       "\n",
       "[:, :, 28] =\n",
       "  0.12416999787092209 \n",
       "  1.0658999681472778  \n",
       "  0.33647000789642334 \n",
       " -0.9356300234794617  \n",
       "  0.04676799848675728 \n",
       " -0.3745900094509125  \n",
       " -0.26107001304626465 \n",
       "  0.7559199929237366  \n",
       " -0.8221700191497803  \n",
       " -0.30507999658584595 \n",
       "  0.2481199949979782  \n",
       " -0.9628099799156189  \n",
       " -0.2593500018119812  \n",
       "  ⋮                   \n",
       " -1.1033999919891357  \n",
       "  1.426300048828125   \n",
       "  0.5391299724578857  \n",
       "  0.6975299715995789  \n",
       " -0.023492999374866486\n",
       " -0.8098599910736084  \n",
       "  0.13798999786376953 \n",
       " -0.7403200268745422  \n",
       " -0.9150000214576721  \n",
       "  1.7891000509262085  \n",
       " -0.4135800004005432  \n",
       " -1.4449000358581543  \n",
       "\n",
       "[:, :, 29] =\n",
       "  0.38784000277519226 \n",
       " -0.3619599938392639  \n",
       " -0.04096100106835365 \n",
       "  0.2936899960041046  \n",
       " -0.4400100111961365  \n",
       " -0.426829993724823   \n",
       " -0.32728999853134155 \n",
       "  0.3984200060367584  \n",
       "  0.5217900276184082  \n",
       " -1.8260999917984009  \n",
       " -0.2205599993467331  \n",
       " -1.1518000364303589  \n",
       "  0.047251999378204346\n",
       "  ⋮                   \n",
       " -0.971310019493103   \n",
       "  0.9911999702453613  \n",
       " -0.304639995098114   \n",
       "  0.76569002866745    \n",
       "  1.4150999784469604  \n",
       " -0.8635600209236145  \n",
       " -0.3028700053691864  \n",
       " -0.8349900245666504  \n",
       "  0.39190998673439026 \n",
       "  0.41297000646591187 \n",
       " -0.4170700013637543  \n",
       " -1.1414999961853027  \n",
       "\n",
       "[:, :, 30] =\n",
       "  0.15163999795913696 \n",
       "  0.30177000164985657 \n",
       " -0.16763000190258026 \n",
       "  0.17684000730514526 \n",
       "  0.3171899914741516  \n",
       "  0.33972999453544617 \n",
       " -0.4347800016403198  \n",
       " -0.3108600080013275  \n",
       " -0.44999000430107117 \n",
       " -0.29486000537872314 \n",
       "  0.16607999801635742 \n",
       "  0.11963000148534775 \n",
       " -0.41328001022338867 \n",
       "  ⋮                   \n",
       "  0.41705000400543213 \n",
       "  0.056763000786304474\n",
       " -6.368100002873689e-5\n",
       "  0.06898699700832367 \n",
       "  0.08793900161981583 \n",
       " -0.10284999758005142 \n",
       " -0.13931000232696533 \n",
       "  0.22314000129699707 \n",
       " -0.08080299943685532 \n",
       " -0.35651999711990356 \n",
       "  0.016412999480962753\n",
       "  0.1021599993109703  "
      ]
     },
     "execution_count": 548,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wembedmat[:,dat1x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([399999 319 … 719 3], Any[0, 1, 1, 1, 6, 7, 1, 7, 8, 8  …  21, 18, 23, 21, 21, 28, 28, 28, 21, 1])"
      ]
     },
     "execution_count": 546,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat1 = iterate(data4)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([399999 319 … 719 3], Any[0, 1, 1, 1, 6, 7, 1, 7, 8, 8  …  21, 18, 23, 21, 21, 28, 28, 28, 21, 1])"
      ]
     },
     "execution_count": 547,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat1x,dat1y = dat1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29-element Array{Any,1}:\n",
       "  0\n",
       "  1\n",
       "  1\n",
       "  1\n",
       "  6\n",
       "  7\n",
       "  1\n",
       "  7\n",
       "  8\n",
       "  8\n",
       "  8\n",
       "  8\n",
       "  8\n",
       "  ⋮\n",
       "  7\n",
       " 21\n",
       " 21\n",
       " 18\n",
       " 23\n",
       " 21\n",
       " 21\n",
       " 28\n",
       " 28\n",
       " 28\n",
       " 21\n",
       "  1"
      ]
     },
     "execution_count": 534,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat1y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(input=3,hidden=5,bidirectional)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn1 = RNN(3, 5; bidirectional=true, rnnType = :lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "ename": "MethodError",
     "evalue": "MethodError: no method matching MLP()\nClosest candidates are:\n  MLP(!Matched::Int64, !Matched::Int64, !Matched::Int64) at In[118]:1",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching MLP()\nClosest candidates are:\n  MLP(!Matched::Int64, !Matched::Int64, !Matched::Int64) at In[118]:1",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[176]:1"
     ]
    }
   ],
   "source": [
    "mlp1 = MLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×2×5 Array{Float64,3}:\n",
       "[:, :, 1] =\n",
       " 0.17894     0.196969 \n",
       " 0.0921046  -0.0318884\n",
       " 0.0704419  -0.321619 \n",
       "\n",
       "[:, :, 2] =\n",
       "  0.115746   0.398939\n",
       "  0.331386  -0.34378 \n",
       " -0.230788  -0.247579\n",
       "\n",
       "[:, :, 3] =\n",
       "  0.0822033  -0.20733 \n",
       " -0.201223    0.258837\n",
       "  0.190816    0.115263\n",
       "\n",
       "[:, :, 4] =\n",
       " -0.150135   -0.123681  \n",
       " -0.415456   -0.00392453\n",
       "  0.0200101   0.193123  \n",
       "\n",
       "[:, :, 5] =\n",
       "  0.139295   -0.238648\n",
       " -0.174997   -0.106833\n",
       "  0.0394548   0.35436 "
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = Knet.xavier(3,2,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10×2×5 Array{Float64,3}:\n",
       "[:, :, 1] =\n",
       " -0.0128885    0.00329075 \n",
       "  0.0343607   -0.0249696  \n",
       "  0.00397492  -0.0085281  \n",
       " -0.00670756  -0.0269048  \n",
       "  0.00658488  -0.0289667  \n",
       " -0.0148214    0.000935341\n",
       " -0.0206585    0.00638483 \n",
       "  0.014658    -0.0264543  \n",
       "  0.025681     0.0467773  \n",
       " -0.0156895   -0.0252407  \n",
       "\n",
       "[:, :, 2] =\n",
       " -0.0433838    0.0326755 \n",
       "  0.0460932   -0.0444737 \n",
       " -0.012586    -0.00552732\n",
       " -0.00820949  -0.0684148 \n",
       " -0.032245    -0.0210779 \n",
       "  0.0128878   -0.00412239\n",
       " -0.00738294  -0.00678317\n",
       " -0.00307534  -0.019106  \n",
       "  0.0389119   -0.00107047\n",
       " -0.0194554   -0.0142148 \n",
       "\n",
       "[:, :, 3] =\n",
       " -0.00957498    0.00542634\n",
       "  0.0319245     0.00889233\n",
       "  0.000888995  -0.0145068 \n",
       " -0.0105941    -0.0111612 \n",
       "  0.011582     -0.0128324 \n",
       " -0.0291192     0.0331274 \n",
       " -0.0157241    -0.00833054\n",
       " -0.023667      0.0269522 \n",
       " -0.0583481    -0.0301172 \n",
       "  0.00321347    0.0250973 \n",
       "\n",
       "[:, :, 4] =\n",
       "  0.0285486    0.00450237\n",
       " -0.0386955    0.0150998 \n",
       "  0.00664119  -0.00145946\n",
       " -0.00249189   0.0052149 \n",
       "  0.0123522    0.009272  \n",
       " -0.013672     0.00358319\n",
       "  0.0134957   -0.0135234 \n",
       " -0.0314126    0.0128438 \n",
       " -0.0557299   -0.0584988 \n",
       "  0.0121823    0.0267557 \n",
       "\n",
       "[:, :, 5] =\n",
       "  0.0307097    0.00996806\n",
       " -0.0276854    0.0107857 \n",
       "  0.0114269    0.0143166 \n",
       " -0.0131683    0.0250974 \n",
       "  0.0136925    0.0310784 \n",
       " -0.0211708   -0.00948519\n",
       " -0.00959314  -0.00956016\n",
       " -0.00838772   0.0131531 \n",
       " -0.0144071   -0.059308  \n",
       " -0.00625239   0.0357925 "
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn1(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain((LSTM(input=20,hidden=10,bidirectional),))"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmodel = fModel(20, 5, 10, 10, 10, 10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: KnetArray32 not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: KnetArray32 not defined",
      "",
      "Stacktrace:",
      " [1] getproperty(::Module, ::Symbol) at .\\sysimg.jl:13",
      " [2] top-level scope at In[180]:2"
     ]
    }
   ],
   "source": [
    "input = Knet.xavier(3,1,5)\n",
    "[Knet.KnetArray32(x) for x in input]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×2 Array{Float64,2}:\n",
       " 1.0  2.0"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arrin = atype(zeros(2, 1, 2))\n",
    "arrin[:,1, 1] = [1. 2.]\n",
    "#arrin[:,1, 2] = tirtembed(\"dfhgjfyk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20×1 Array{Float32,2}:\n",
       "  0.8608544  \n",
       "  0.8492885  \n",
       " -0.48142886 \n",
       "  2.665367   \n",
       " -0.050460823\n",
       " -0.72113055 \n",
       "  1.3265398  \n",
       "  0.6482137  \n",
       " -1.4457464  \n",
       " -1.255041   \n",
       "  0.41842613 \n",
       " -0.41712037 \n",
       " -0.025142353\n",
       " -0.18107042 \n",
       " -0.16562504 \n",
       " -0.19828318 \n",
       " -0.25851956 \n",
       "  1.1739769  \n",
       " -1.1310492  \n",
       " -0.013492593"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tirtembed(\"dfhgdfjfyk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20×1×2 Array{Float32,3}:\n",
       "[:, :, 1] =\n",
       "  0.20342048 \n",
       " -0.23905772 \n",
       " -0.096778885\n",
       " -0.13804154 \n",
       "  0.062011335\n",
       " -0.19228645 \n",
       "  0.010738692\n",
       "  0.05998249 \n",
       "  0.14119059 \n",
       "  0.07197984 \n",
       " -0.01608018 \n",
       " -0.15639299 \n",
       " -0.17113964 \n",
       "  0.14826685 \n",
       "  0.10379549 \n",
       " -0.06747454 \n",
       " -0.17891692 \n",
       "  0.22242771 \n",
       " -0.06071747 \n",
       " -0.22232723 \n",
       "\n",
       "[:, :, 2] =\n",
       "  0.20199183  \n",
       " -0.3184071   \n",
       " -0.1432167   \n",
       " -0.23403578  \n",
       "  0.10931119  \n",
       " -0.23149896  \n",
       " -0.004477276 \n",
       "  0.07848018  \n",
       "  0.21460247  \n",
       "  0.10378643  \n",
       "  0.0033652722\n",
       " -0.11604335  \n",
       " -0.12570937  \n",
       "  0.0892244   \n",
       "  0.0703403   \n",
       " -0.03680204  \n",
       " -0.118299015 \n",
       "  0.17206894  \n",
       " -0.033298485 \n",
       " -0.20176002  "
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmodel(reshape([tirtembed(\"dfhgdfjfyk\") tirtembed(\"dfhgdfjfyk\")], 20, 1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20×1×2 Array{Float32,3}:\n",
       "[:, :, 1] =\n",
       "  0.8608544  \n",
       "  0.8492885  \n",
       " -0.48142886 \n",
       "  2.665367   \n",
       " -0.050460823\n",
       " -0.72113055 \n",
       "  1.3265398  \n",
       "  0.6482137  \n",
       " -1.4457464  \n",
       " -1.255041   \n",
       "  0.41842613 \n",
       " -0.41712037 \n",
       " -0.025142353\n",
       " -0.18107042 \n",
       " -0.16562504 \n",
       " -0.19828318 \n",
       " -0.25851956 \n",
       "  1.1739769  \n",
       " -1.1310492  \n",
       " -0.013492593\n",
       "\n",
       "[:, :, 2] =\n",
       " -0.98158425 \n",
       "  0.6077837  \n",
       " -0.2735724  \n",
       " -1.1433774  \n",
       "  0.6099511  \n",
       "  0.074774876\n",
       "  1.4331263  \n",
       "  0.7742791  \n",
       " -0.89767945 \n",
       "  0.61602724 \n",
       "  0.5874392  \n",
       "  0.23272926 \n",
       "  0.37268496 \n",
       "  1.4274784  \n",
       " -1.1315342  \n",
       "  0.8104872  \n",
       " -0.76209766 \n",
       " -2.426628   \n",
       " -0.23896657 \n",
       "  1.1319513  "
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshape([tirtembed(\"dfhgdfjfyk\") tirtembed(\"dfasdgsdffjfyk\")], 20, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "minibatch(x, [y], batchsize; shuffle, partial, xtype, ytype, xsize, ysize)\n",
       "\\end{verbatim}\n",
       "Return an iterator of minibatches [(xi,yi)...] given data tensors x, y and batchsize.  \n",
       "\n",
       "The last dimension of x and y give the number of instances and should be equal. \\texttt{y} is optional, if omitted a sequence of \\texttt{xi} will be generated rather than \\texttt{(xi,yi)} tuples.  Use \\texttt{repeat(d,n)} for multiple epochs, \\texttt{Iterators.take(d,n)} for a partial epoch, and \\texttt{Iterators.cycle(d)} to cycle through the data forever (this can be used with \\texttt{converge}). If you need the iterator to continue from its last position when stopped early (e.g. by a break in a for loop), use \\texttt{Iterators.Stateful(d)} (by default the iterator would restart from the beginning).\n",
       "\n",
       "Keyword arguments:\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{shuffle=false}: Shuffle the instances every epoch.\n",
       "\n",
       "\n",
       "\\item \\texttt{partial=false}: If true include the last partial minibatch < batchsize.\n",
       "\n",
       "\n",
       "\\item \\texttt{xtype=typeof(x)}: Convert xi in minibatches to this type.\n",
       "\n",
       "\n",
       "\\item \\texttt{ytype=typeof(y)}: Convert yi in minibatches to this type.\n",
       "\n",
       "\n",
       "\\item \\texttt{xsize=size(x)}: Convert xi in minibatches to this shape.\n",
       "\n",
       "\n",
       "\\item \\texttt{ysize=size(y)}: Convert yi in minibatches to this shape.\n",
       "\n",
       "\\end{itemize}\n"
      ],
      "text/markdown": [
       "```\n",
       "minibatch(x, [y], batchsize; shuffle, partial, xtype, ytype, xsize, ysize)\n",
       "```\n",
       "\n",
       "Return an iterator of minibatches [(xi,yi)...] given data tensors x, y and batchsize.  \n",
       "\n",
       "The last dimension of x and y give the number of instances and should be equal. `y` is optional, if omitted a sequence of `xi` will be generated rather than `(xi,yi)` tuples.  Use `repeat(d,n)` for multiple epochs, `Iterators.take(d,n)` for a partial epoch, and `Iterators.cycle(d)` to cycle through the data forever (this can be used with `converge`). If you need the iterator to continue from its last position when stopped early (e.g. by a break in a for loop), use `Iterators.Stateful(d)` (by default the iterator would restart from the beginning).\n",
       "\n",
       "Keyword arguments:\n",
       "\n",
       "  * `shuffle=false`: Shuffle the instances every epoch.\n",
       "  * `partial=false`: If true include the last partial minibatch < batchsize.\n",
       "  * `xtype=typeof(x)`: Convert xi in minibatches to this type.\n",
       "  * `ytype=typeof(y)`: Convert yi in minibatches to this type.\n",
       "  * `xsize=size(x)`: Convert xi in minibatches to this shape.\n",
       "  * `ysize=size(y)`: Convert yi in minibatches to this shape.\n"
      ],
      "text/plain": [
       "\u001b[36m  minibatch(x, [y], batchsize; shuffle, partial, xtype, ytype, xsize, ysize)\u001b[39m\n",
       "\n",
       "  Return an iterator of minibatches [(xi,yi)...] given data tensors x,\n",
       "  y and batchsize. \n",
       "\n",
       "  The last dimension of x and y give the number of instances and\n",
       "  should be equal. \u001b[36my\u001b[39m is optional, if omitted a sequence of \u001b[36mxi\u001b[39m will be\n",
       "  generated rather than \u001b[36m(xi,yi)\u001b[39m tuples. Use \u001b[36mrepeat(d,n)\u001b[39m for multiple\n",
       "  epochs, \u001b[36mIterators.take(d,n)\u001b[39m for a partial epoch, and\n",
       "  \u001b[36mIterators.cycle(d)\u001b[39m to cycle through the data forever (this can be\n",
       "  used with \u001b[36mconverge\u001b[39m). If you need the iterator to continue from its\n",
       "  last position when stopped early (e.g. by a break in a for loop),\n",
       "  use \u001b[36mIterators.Stateful(d)\u001b[39m (by default the iterator would restart\n",
       "  from the beginning).\n",
       "\n",
       "  Keyword arguments:\n",
       "\n",
       "    •    \u001b[36mshuffle=false\u001b[39m: Shuffle the instances every epoch.\n",
       "\n",
       "    •    \u001b[36mpartial=false\u001b[39m: If true include the last partial minibatch\n",
       "        < batchsize.\n",
       "\n",
       "    •    \u001b[36mxtype=typeof(x)\u001b[39m: Convert xi in minibatches to this type.\n",
       "\n",
       "    •    \u001b[36mytype=typeof(y)\u001b[39m: Convert yi in minibatches to this type.\n",
       "\n",
       "    •    \u001b[36mxsize=size(x)\u001b[39m: Convert xi in minibatches to this shape.\n",
       "\n",
       "    •    \u001b[36mysize=size(y)\u001b[39m: Convert yi in minibatches to this shape."
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@doc Knet.minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(P(Array{Float32,2}(5,20)), P(Array{Float32,1}(5)))"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linmod = Linear(20,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Param,1}:\n",
       " P(Array{Float32,2}(5,20))\n",
       " P(Array{Float32,1}(5))   "
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linmod |> Knet.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20×1 Array{Float64,2}:\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linput = zeros(20,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "ename": "ErrorException",
     "evalue": "KnetPtr: bad device id -1.",
     "output_type": "error",
     "traceback": [
      "KnetPtr: bad device id -1.",
      "",
      "Stacktrace:",
      " [1] error(::String) at .\\error.jl:33",
      " [2] Knet.KnetPtr(::Int64) at C:\\Users\\dfhdhsd\\.julia\\packages\\Knet\\05UDD\\src\\kptr.jl:94",
      " [3] Type at C:\\Users\\dfhdhsd\\.julia\\packages\\Knet\\05UDD\\src\\karray.jl:117 [inlined]",
      " [4] KnetArray{Float32,2}(::Array{Float64,2}) at C:\\Users\\dfhdhsd\\.julia\\packages\\Knet\\05UDD\\src\\karray.jl:130",
      " [5] top-level scope at In[189]:1"
     ]
    }
   ],
   "source": [
    "linput2 = Knet.KnetArray{Float32,2}(linput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×1 Array{Float32,2}:\n",
       " -0.20123664\n",
       " -0.5370126 \n",
       "  0.7677018 \n",
       "  0.76841515\n",
       " -0.33187535"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linmod(atype(tirtembed(\"dfhgdfjfyk\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Array{Any,1}:\n",
       " (4, 6)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = []\n",
    "push!(d, (4, 6))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.1.0",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
